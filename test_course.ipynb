
{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building an AI Research Paper Reviewer with OpenAI’s o4-mini: Reasoning, Prompt Engineering, and Tool Use\n",
        "\n",
        "Welcome! In this hands-on course, you'll learn how to leverage OpenAI's o4-mini reasoning model to build a practical research paper reviewer in Python. You'll develop skills in prompt engineering for reasoning models, integrate function calling for statistical analysis, and apply these concepts in a real-world capstone project. By the end, you'll understand how to combine LLM reasoning, custom tools, and prompt design to create robust AI-powered applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 1: Introduction to Reasoning Models and the o4-mini API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(This lesson is Video Only. Below is the script and visual suggestions for the video.)*\n",
        "\n",
        "### Video Script: Introduction to Reasoning Models and o4-mini\n",
        "\n",
        "**(0:00-0:10) Introduction**\n",
        "\n",
        "**Host:** Hi everyone, and welcome! In this course, we're going to build something truly exciting: an AI-powered research paper reviewer. We'll be using OpenAI's o4-mini model, a powerful tool for tasks that require deep reasoning. \n",
        "\n",
        "*(Visual: Title card - \"Building an AI Research Paper Reviewer with o4-mini\")*\n",
        "\n",
        "**(0:10-0:45) What are Reasoning Models?**\n",
        "\n",
        "**Host:** So, what exactly are reasoning models? Think of them as the next step up from standard language models. While many LLMs are great at generating text or answering simple questions, reasoning models are designed to tackle more complex problems. They can perform multi-step thinking, analyze intricate information, and even help with tasks like math, coding, and scientific analysis. \n",
        "\n",
        "Imagine you're trying to solve a complex puzzle. You wouldn't just guess randomly, right? You'd break it down, consider different possibilities, and follow a logical path. Reasoning models try to do something similar. \n",
        "\n",
        "*(Visual: Animation of a brain with interconnected gears turning, symbolizing thought processes. Side-by-side comparison: a simple Q&A bot vs. a diagram showing multi-step problem-solving.)*\n",
        "\n",
        "**(0:45-1:30) Introducing o4-mini**\n",
        "\n",
        "**Host:** Now, let's talk about o4-mini. This is one of OpenAI's newer models, specifically optimized for reasoning tasks. What makes it special? \n",
        "\n",
        "First, it packs a punch in performance, especially for its size. It excels in areas like math, coding, and scientific reasoning. \n",
        "Second, it's cost-effective. This makes it great for experimenting, building demos, or integrating into tools where you need strong reasoning without breaking the bank. \n",
        "Third, it's fast! Low-latency outputs mean it's suitable for applications that need quick responses.\n",
        "\n",
        "*(Visual: OpenAI logo, then an o4-mini specific graphic or logo. Bullet points on screen: Top-tier math/coding performance, Cost-effective, Low-latency outputs, Efficient memory footprint.)*\n",
        "\n",
        "**Host:** Of course, like any AI, it has limitations. It's not a true artificial general intelligence, it can still make mistakes or 'hallucinate,' and it operates within a specific context window. But for targeted reasoning tasks, it's incredibly powerful. You'd choose o4-mini when your project demands deep analysis, logical deduction, or complex problem-solving, especially when budget and speed are also important factors.\n",
        "\n",
        "*(Visual: A slide showing a balance scale: one side \"High Reasoning Power\", other side \"Cost & Speed Efficiency\", with o4-mini in the middle.)*\n",
        "\n",
        "**(1:30-2:00) Project Overview: AI Research Paper Reviewer**\n",
        "\n",
        "**Host:** So, what are we building? Our project is an AI research paper reviewer. Here’s the plan:\n",
        "1.  Our tool will take a research paper (as a PDF).\n",
        "2.  It will extract the text content.\n",
        "3.  Then, using o4-mini's reasoning capabilities, it will analyze the paper to highlight potential weak arguments, unsupported claims, or flawed methodology.\n",
        "4.  It will suggest improvements and provide a structured review.\n",
        "5.  And here's a cool part: we'll enhance it with statistical tools, allowing the model to, for example, re-calculate p-values or check effect sizes on demand!\n",
        "\n",
        "*(Visual: A simple flowchart: PDF Icon -> Text Extraction Icon -> o4-mini Brain Icon (analyzing) -> Report Icon. Add smaller icons for statistical tools like a calculator or graph.)*\n",
        "\n",
        "**(2:00-2:15) What's Next?**\n",
        "\n",
        "**Host:** This project will give you hands-on experience with o4-mini, prompt engineering for reasoning, and integrating external tools with LLMs. In the next lesson, we'll get our environment set up and get our API access sorted. Ready to dive in? Let's get started!\n",
        "\n",
        "*(Visual: Screen showing \"Next Up: Setting Up Your Environment\")*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 2: Setting Up Your Environment and Accessing the API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Setting Up Your Environment\n",
        "\n",
        "**(0:00-0:15) Introduction**\n",
        "\n",
        "**Host:** Welcome back! Before we can start building our AI research paper reviewer, we need to set up our development environment. This involves getting an OpenAI API key, installing a few Python libraries, and making sure we handle our API key securely.\n",
        "\n",
        "*(Visual: Title card - \"Lesson 2: Setting Up Your Environment\")*\n",
        "\n",
        "**(0:15-0:45) Obtaining an OpenAI API Key**\n",
        "\n",
        "**Host:** To use o4-mini, or any OpenAI model via their API, you'll need an API key. \n",
        "First, head over to `platform.openai.com`. You'll need to create an account or log in. \n",
        "Once you're in, navigate to the API keys section. It's usually under your account settings or a dedicated 'API Keys' tab. \n",
        "Click on 'Create new secret key'. Give it a descriptive name, like \"o4-mini-reviewer-key\". \n",
        "**Important:** Copy this key immediately and save it somewhere secure, like a password manager. You won't be able to see it again after you close the dialog! \n",
        "Also, ensure you have set up billing information in your OpenAI account, as API usage typically incurs costs.\n",
        "\n",
        "*(Visual: Screencast showing the OpenAI platform: logging in, navigating to API keys, creating a new key (blurring out any actual keys), and the warning to save it.)*\n",
        "\n",
        "**(0:45-1:15) Installing Required Python Libraries**\n",
        "\n",
        "**Host:** Next, we need to install some Python libraries that our project will depend on. We'll be using:\n",
        "-   `openai`: To interact with the OpenAI API.\n",
        "-   `PyMuPDF`: To extract text from PDF files. Its import name is `fitz`.\n",
        "-   `tiktoken`: For tokenizing text, which is important for managing LLM context limits.\n",
        "-   `numpy`: For numerical operations, especially for our statistical helper functions.\n",
        "-   `scipy`: For more advanced statistical functions.\n",
        "\n",
        "You can install these using pip. I'll show you how in the notebook, but the command will look something like `pip install openai PyMuPDF tiktoken numpy scipy`.\n",
        "\n",
        "*(Visual: List of libraries on screen with brief descriptions. A terminal window showing the pip install command.)*\n",
        "\n",
        "**(1:15-2:00) Securely Setting Environment Variables**\n",
        "\n",
        "**Host:** Now, about that API key. You should **never** hardcode your API key directly into your scripts. It's a security risk, especially if you share your code or commit it to a version control system like Git. The best practice is to use environment variables. \n",
        "\n",
        "An environment variable is a variable whose value is set outside the program, typically in your operating system's shell or a special `.env` file. Your Python script can then read this variable at runtime. This keeps your sensitive credentials separate from your codebase.\n",
        "\n",
        "We'll set an environment variable, typically named `OPENAI_API_KEY`, to your actual key. In the notebook, I'll show you how to do this and how our Python code will access it using the `os` library.\n",
        "\n",
        "*(Visual: Diagram: Code points to an \"Environment Variable\" box, which is separate. Text: \"DO NOT hardcode API keys!\". Show an example of setting an environment variable in a terminal (e.g., `export OPENAI_API_KEY='your_key_here'` for Linux/macOS or `set OPENAI_API_KEY=your_key_here` for Windows, though we'll focus on `.env` or Python methods for consistency).)*\n",
        "\n",
        "**(2:00-2:15) Let's Get to the Notebook!**\n",
        "\n",
        "**Host:** Alright, that covers the setup essentials. Now, let's jump into the Jupyter Notebook where we'll walk through these steps and write some code to get everything configured. See you there!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this notebook, we'll prepare our development environment. This includes installing necessary Python packages and configuring our OpenAI API key securely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Installing Dependencies\n",
        "\n",
        "We need several Python libraries for this project:\n",
        "-   **openai**: The official OpenAI Python library to interact with the API.\n",
        "-   **PyMuPDF**: A library for PDF manipulation. We'll use it to extract text from research papers. It's imported in Python as `fitz`.\n",
        "-   **tiktoken**: OpenAI's fast BPE tokenizer. We'll use this to count tokens and chunk text effectively for the language model.\n",
        "-   **numpy**: A fundamental package for numerical computation in Python. Useful for handling data for our statistical functions.\n",
        "-   **scipy**: A library for scientific and technical computing. We'll use it for statistical tests.\n",
        "\n",
        "You can install these by running the following command in your terminal or by executing the cell below (uncomment it first). If you run it in the notebook, you might need to restart the kernel afterward for the packages to be recognized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# !pip install openai PyMuPDF tiktoken numpy scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After installation, let's import them to make sure they are available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import openai\n",
        "import fitz  # PyMuPDF\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "import scipy\n",
        "import os\n",
        "print(\"All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Setting and Accessing the OpenAI API Key\n",
        "\n",
        "Your OpenAI API key is a secret credential. **Do not share it or embed it directly in your code.** The recommended way to handle API keys is through environment variables.\n",
        "\n",
        "**How to set an environment variable:**\n",
        "\n",
        "1.  **Using a `.env` file (Recommended for local development):**\n",
        "    *   Install the `python-dotenv` library: `pip install python-dotenv`\n",
        "    *   Create a file named `.env` in the root directory of your project.\n",
        "    *   Add your API key to the `.env` file like this:\n",
        "        `OPENAI_API_KEY=\"your_actual_api_key_here\"`\n",
        "    *   You can then load this variable in your script.\n",
        "\n",
        "2.  **Setting it in your operating system:**\n",
        "    *   **Linux/macOS:** Open your terminal and type `export OPENAI_API_KEY=\"your_actual_api_key_here\"`. To make this permanent, add this line to your shell's configuration file (e.g., `.bashrc`, `.zshrc`).\n",
        "    *   **Windows:** Open Command Prompt and type `set OPENAI_API_KEY=your_actual_api_key_here`. For PowerShell, use `$env:OPENAI_API_KEY=\"your_actual_api_key_here\"`. To set it permanently, search for \"environment variables\" in the Windows search bar and add it through the System Properties dialog.\n",
        "\n",
        "For this notebook, we'll assume you've set the `OPENAI_API_KEY` environment variable. The `openai` library automatically looks for this variable. If it's not set, you'll need to pass it when initializing the client, but we'll stick to best practices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Optional: If using .env file, load it.\n",
        "# from dotenv import load_dotenv\n",
        "# load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "if api_key:\n",
        "    print(\"OPENAI_API_KEY found!\")\n",
        "    # For security, let's not print the key itself, just a part of it or its presence.\n",
        "    # print(f\"API Key starts with: {api_key[:5]}...\") \n",
        "else:\n",
        "    print(\"Error: OPENAI_API_KEY environment variable not found.\")\n",
        "    print(\"Please set it up before proceeding.\")\n",
        "\n",
        "# Initialize the OpenAI client\n",
        "# The client will automatically use the OPENAI_API_KEY environment variable if set.\n",
        "try:\n",
        "    client = openai.OpenAI()\n",
        "    # You can make a simple test call here if you have credits and want to verify, \n",
        "    # but for setup, just initializing is often enough.\n",
        "    # For example, listing models (this might incur a tiny cost or be free):\n",
        "    # models = client.models.list()\n",
        "    # print(\"Successfully initialized OpenAI client and fetched models.\")\n",
        "    print(\"Successfully initialized OpenAI client.\")\n",
        "except openai.AuthenticationError as e:\n",
        "    print(f\"OpenAI Authentication Error: {e}\")\n",
        "    print(\"Please ensure your API key is correct and your account has credits.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why secure environment variables matter:**\n",
        "-   **Security:** Prevents accidental exposure of your secret keys if you share your code (e.g., on GitHub).\n",
        "-   **Configuration Management:** Allows different configurations for different environments (development, testing, production) without changing code.\n",
        "-   **Collaboration:** Team members can use their own API keys without modifying shared code.\n",
        "\n",
        "**Brief intro to each library’s role in the project:**\n",
        "-   `openai`: This is our gateway to OpenAI's models, including o4-mini. We'll use it to send requests and receive responses.\n",
        "-   `PyMuPDF (fitz)`: Research papers are often in PDF format. This library will help us read the content from these files.\n",
        "-   `tiktoken`: Language models have a limit on how much text they can process at once (context window). Tiktoken helps us count how many \"tokens\" our text will consume, allowing us to break it into manageable chunks.\n",
        "-   `numpy`: When we implement statistical tools (like calculating means, standard deviations), NumPy will provide efficient array operations.\n",
        "-   `scipy`: For more complex statistical calculations like p-values or confidence intervals, SciPy offers robust, pre-built functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Set up your `OPENAI_API_KEY`**: Follow one of the methods described above (preferably using a `.env` file or setting it directly in your OS) to set your OpenAI API key as an environment variable.\n",
        "2.  **Verify**: Run the code cell above that checks for `OPENAI_API_KEY`. Ensure it prints \"OPENAI_API_KEY found!\" and successfully initializes the `OpenAI` client."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(No code cell needed for this practice, as it involves external setup and verification using the existing cells above.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 3: Fundamentals of Prompt Engineering for Reasoning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(This lesson is Video Only. Below is the script and visual suggestions for the video.)*\n",
        "\n",
        "### Video Script: Prompt Engineering for Reasoning Models\n",
        "\n",
        "**(0:00-0:20) Introduction**\n",
        "\n",
        "**Host:** Now that our environment is set up, let's talk about a crucial skill when working with any large language model, especially reasoning models like o4-mini: Prompt Engineering.\n",
        "\n",
        "*(Visual: Title card - \"Lesson 3: Fundamentals of Prompt Engineering for Reasoning Models\")*\n",
        "\n",
        "**(0:20-0:45) What is Prompt Engineering?**\n",
        "\n",
        "**Host:** Simply put, prompt engineering is the art and science of crafting effective inputs – or prompts – to guide an LLM to produce the desired output. Think of yourself as a director and the LLM as an actor. Your prompt is the script and direction you provide. The better your direction, the better the performance!\n",
        "\n",
        "*(Visual: Analogy: A director with a megaphone guiding an actor (LLM). Text: \"Prompt = Your Instruction to the LLM\")*\n",
        "\n",
        "**Host:** For reasoning models, this is even more critical. Because we're asking them to perform complex analysis, make judgments, or follow multi-step logic, our prompts need to be clear, specific, and provide sufficient context.\n",
        "\n",
        "**(0:45-1:45) Prompt Structure for Reasoning Tasks**\n",
        "\n",
        "**Host:** So, what makes a good prompt for a reasoning task? Here are some key elements:\n",
        "\n",
        "1.  **Role Playing:** Assign a role to the LLM. For our project, we'll tell o4-mini: \"You are an expert AI research reviewer.\" This helps set the context and tone for its responses.\n",
        "    *(Visual: Icon of a graduation cap or a magnifying glass. Text: \"Assign a Role: 'You are an expert...' \")*\n",
        "\n",
        "2.  **Clear Instructions & Task Definition:** Be explicit about what you want the model to do. Instead of just \"Review this paper,\" we might say: \"Read the given chunk of a research paper and highlight weak arguments, unsupported claims, or flawed methodology.\"\n",
        "    *(Visual: Checklist icon. Text: \"Clear Instructions: What to do? How to do it?\")*\n",
        "\n",
        "3.  **Context:** Provide all necessary background information. This includes the text to be analyzed, but also any constraints or specific areas to focus on.\n",
        "    *(Visual: Icon of a book or document. Text: \"Provide Context: Relevant information, data, text.\")*\n",
        "\n",
        "4.  **Specify Output Format (Optional but helpful):** Sometimes, you might want the output in a particular structure, like a list, JSON, or a summary with specific sections. Defining this can make the output more consistent and easier to parse.\n",
        "    *(Visual: Icon representing structured data (like JSON braces {}). Text: \"Output Format: Define structure if needed.\")*\n",
        "\n",
        "**(1:45-2:30) Guiding o4-mini for Detailed, Critical Analysis**\n",
        "\n",
        "**Host:** For our research paper reviewer, we want o4-mini to be thorough and critical. So, our prompts will encourage this. We might include phrases like:\n",
        "-   \"Be rigorous and explain your reasoning.\"\n",
        "-   \"Scrutinize statistical claims.\"\n",
        "-   \"Identify any logical fallacies or gaps in argumentation.\"\n",
        "-   \"Suggest specific improvements.\"\n",
        "\n",
        "*(Visual: A magnifying glass closely examining a document. Keywords on screen: \"Rigorous\", \"Explain Reasoning\", \"Scrutinize\", \"Identify Flaws\", \"Suggest Improvements\".)*\n",
        "\n",
        "**Host:** We can also tell the model about any tools it can use, like our statistical helpers, which we'll build later. For example: \"You can request tools to recalculate p-values or compute confidence intervals.\"\n",
        "\n",
        "**(2:30-3:15) Common Pitfalls and Best Practices**\n",
        "\n",
        "**Host:** Prompt engineering is often an iterative process. Here are some common pitfalls to avoid:\n",
        "-   **Ambiguity:** If your prompt is vague, the LLM might misinterpret it.\n",
        "-   **Leading Questions:** Avoid phrasing that biases the model towards a particular answer, unless that's your specific intent for testing.\n",
        "-   **Insufficient Context:** If the model doesn't have enough information, it can't reason effectively.\n",
        "-   **Overly Complex Prompts:** While you need to be specific, a prompt that's too long or convoluted can confuse the model.\n",
        "\n",
        "*(Visual: A split screen: Left side shows a confusing, messy prompt with a question mark. Right side shows a clear, concise prompt with a checkmark.)*\n",
        "\n",
        "**Host:** And some best practices:\n",
        "-   **Be Specific:** The more precise your instructions, the better.\n",
        "-   **Iterate and Refine:** Start with a simple prompt and gradually add complexity. Test and see what works.\n",
        "-   **Use Examples (Few-Shot Prompting):** Sometimes, providing a couple of examples of the desired input/output format can significantly improve results. (Though for our project, we'll focus more on zero-shot with clear instructions and tool use).\n",
        "-   **Consider Model Parameters:** Things like `temperature` (randomness) or `reasoning_effort` (for o4-mini) can influence output, but the prompt itself is foundational.\n",
        "\n",
        "*(Visual: A cyclical diagram: \"Draft Prompt\" -> \"Test\" -> \"Analyze Output\" -> \"Refine Prompt\" -> loop back.)*\n",
        "\n",
        "**(3:15-3:30) Conclusion**\n",
        "\n",
        "**Host:** Mastering prompt engineering is key to unlocking the full potential of reasoning models like o4-mini. As we build our reviewer, we'll be putting these principles into practice. In the next lesson, we'll start building the helper functions that our o4-mini model will be able to call to perform statistical checks! This is where things get really interactive. See you then!\n",
        "\n",
        "*(Visual: Screen showing \"Next Up: Integrating Function Calling - Statistical Tool Helpers\")*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 4: Integrating Function Calling—Building Statistical Tool Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Integrating Function Calling\n",
        "\n",
        "**(0:00-0:20) Introduction**\n",
        "\n",
        "**Host:** Welcome back! In the previous lesson, we discussed how to craft effective prompts. Now, we're going to give our o4-mini model some superpowers: the ability to call external functions. This is a game-changer for making LLMs more useful in real-world applications.\n",
        "\n",
        "*(Visual: Title card - \"Lesson 4: Integrating Function Calling - Building Statistical Tool Helpers\")*\n",
        "\n",
        "**(0:20-0:50) Why Function Calling Enhances LLM Reasoning**\n",
        "\n",
        "**Host:** LLMs are great at processing and generating text, but they don't inherently have access to real-time information, specific databases, or the ability to perform complex calculations beyond their training. Function calling bridges this gap. \n",
        "\n",
        "It allows the LLM, when it deems necessary based on your prompt, to say, \"Hey, I need to run a specific calculation or get some external data. Can you execute this function for me with these arguments?\" Your application then runs the function and sends the result back to the LLM, which incorporates it into its reasoning process.\n",
        "\n",
        "*(Visual: Diagram: LLM brain icon -> decides to call a function -> sends request to \"Your Code\" (which has tools) -> Your Code executes function -> sends result back to LLM -> LLM uses result.)*\n",
        "\n",
        "**Host:** For our research paper reviewer, this means o4-mini can do more than just read text; it can actively verify statistical claims by requesting calculations.\n",
        "\n",
        "**(0:50-1:40) Overview of Statistical Helpers We'll Build**\n",
        "\n",
        "**Host:** We're going to equip our reviewer with a few key statistical tools. These will be Python functions that the LLM can call:\n",
        "\n",
        "1.  **Recalculate p-value:** If a paper claims a certain p-value for a comparison between two groups, our LLM can ask this tool to recalculate it based on provided sample data. This helps verify statistical significance.\n",
        "    *(Visual: Icon representing a p-value or a statistical test.)*\n",
        "\n",
        "2.  **Compute Cohen's d:** P-values tell us about statistical significance, but not necessarily practical significance. Cohen's d is a measure of effect size. Our LLM can use this tool to assess the magnitude of an observed difference.\n",
        "    *(Visual: Icon representing effect size, like a bar chart showing a small vs. large difference.)*\n",
        "\n",
        "3.  **Compute Confidence Interval:** This tool will calculate the confidence interval for a sample group's mean, helping the LLM understand the reliability and precision of estimates presented in the paper.\n",
        "    *(Visual: Icon of a bell curve with a confidence interval highlighted.)*\n",
        "\n",
        "4.  **Describe Group:** A simpler function to get basic descriptive statistics for a data sample, like the mean, standard deviation, and sample size. This provides foundational context for other statistical evaluations.\n",
        "    *(Visual: Icon representing summary statistics (mean, std dev, n).)*\n",
        "\n",
        "**(1:40-2:10) How the LLM Interacts with These Tools**\n",
        "\n",
        "**Host:** So how does this work in practice? \n",
        "First, when we make an API call to o4-mini, we'll describe these available tools to it – their names, what they do, and what parameters they expect. \n",
        "Then, as o4-mini reviews a chunk of the paper, if it encounters a statistical claim it wants to verify (and we've prompted it to do so!), it won't just make something up. Instead, its response will include a special `function_call` object. This object will specify which of our functions it wants to call and the arguments (like the actual data points from the paper) it wants to use.\n",
        "\n",
        "Our Python code will detect this `function_call`, execute the corresponding Python function (e.g., `recalculate_p_value`) with the provided arguments, and then send the result of that function back to o4-mini in a subsequent API call. The model then uses this new information to continue its review, now armed with a verified calculation.\n",
        "\n",
        "*(Visual: Simplified sequence diagram: User Prompt + Text Chunk -> o4-mini -> (Output includes `function_call` for `compute_cohens_d`) -> Your Python App -> Executes `compute_cohens_d(data1, data2)` -> (Result: `d=0.5`) -> Sends result to o4-mini -> o4-mini generates final analysis incorporating Cohen's d.)*\n",
        "\n",
        "**(2:10-2:25) Let's Build Them!**\n",
        "\n",
        "**Host:** This is a powerful way to make LLMs more factual and capable. In the notebook, we'll implement these Python functions and then, in a later lesson, we'll see how to register them with the o4-mini API and handle the call-and-response flow. Let's get coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Building Statistical Tool Helpers\n",
        "\n",
        "In this notebook, we'll implement the Python functions that will serve as tools for our LLM. These functions will perform various statistical calculations. Later, we'll make these functions callable by the o4-mini model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Required Imports\n",
        "We'll primarily use `numpy` for numerical operations and `scipy.stats` for statistical functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, sem, t\n",
        "import json # For pretty printing dicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Recalculate p-value\n",
        "\n",
        "This function performs Welch’s t-test between two independent sample groups. Welch's t-test is used when the two groups might have unequal variances. It tests the null hypothesis that the means of the two groups are equal. A small p-value suggests that the observed difference in means is statistically significant (i.e., unlikely to have occurred by chance)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def recalculate_p_value(group1, group2):\n",
        "    \"\"\"Calculate p-value between two independent sample groups using Welch's t-test.\"\"\"\n",
        "    # Ensure inputs are numpy arrays for scipy functions\n",
        "    group1_arr = np.array(group1)\n",
        "    group2_arr = np.array(group2)\n",
        "    \n",
        "    # Perform Welch's t-test (assumes unequal variances by default with equal_var=False)\n",
        "    t_stat, p_value = ttest_ind(group1_arr, group2_arr, equal_var=False)\n",
        "    return {\"p_value\": round(p_value, 4)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "sample_group1 = [2.5, 3.1, 2.8, 3.5, 2.9]\n",
        "sample_group2 = [3.8, 3.2, 4.1, 3.9, 3.5]\n",
        "p_value_result = recalculate_p_value(sample_group1, sample_group2)\n",
        "print(f\"Recalculate p-value result: {json.dumps(p_value_result)}\")\n",
        "\n",
        "# Example where difference is likely not significant\n",
        "sample_group3 = [2.5, 3.1, 2.8, 3.5, 2.9]\n",
        "sample_group4 = [2.6, 3.0, 2.9, 3.4, 3.0]\n",
        "p_value_result_ns = recalculate_p_value(sample_group3, sample_group4)\n",
        "print(f\"Recalculate p-value result (non-significant): {json.dumps(p_value_result_ns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Compute Cohen’s d\n",
        "\n",
        "Cohen’s d is a standardized measure of effect size, representing the difference between two means in terms of standard deviations. It helps quantify the magnitude of the difference, which is important because a statistically significant result (small p-value) doesn't always mean a practically important or large effect, especially with large sample sizes.\n",
        "\n",
        "The formula for Cohen's d for two independent groups is:\n",
        "$$ d = \\frac{{\\text{mean}_1 - \\text{mean}_2}}{{\\text{pooled standard deviation}}} $$\n",
        "Where pooled standard deviation is often calculated as:\n",
        "(This version is used when sample sizes are equal or nearly equal, and is a common simplification. More complex formulas exist for unequal sample sizes or variances)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def compute_cohens_d(group1, group2):\n",
        "    \"\"\"Compute Cohen's d for effect size between two independent groups.\"\"\"\n",
        "    group1_arr = np.array(group1)\n",
        "    group2_arr = np.array(group2)\n",
        "\n",
        "    mean1, mean2 = np.mean(group1_arr), np.mean(group2_arr)\n",
        "    # ddof=1 for sample standard deviation\n",
        "    std1, std2 = np.std(group1_arr, ddof=1), np.std(group2_arr, ddof=1)\n",
        "    \n",
        "    # Calculate pooled standard deviation (simplified version)\n",
        "    # Ensure no division by zero if std dev is 0 (e.g., all values in a group are the same)\n",
        "    if std1 == 0 and std2 == 0:\n",
        "        # If both means are also same, d is 0. Otherwise, it's undefined or infinite.\n",
        "        # For simplicity, if means differ and stds are 0, this indicates an extreme separation.\n",
        "        # However, this scenario is rare with real data.\n",
        "        # Let's return NaN or handle as an error, or a very large number if means differ.\n",
        "        if mean1 == mean2:\n",
        "            return {\"cohens_d\": 0.0}\n",
        "        else:\n",
        "            # This case is problematic; typically indicates data issue or perfect separation.\n",
        "            # Returning NaN might be appropriate, or a large placeholder.\n",
        "            # For now, let's return NaN as it signals an issue with inputs for standard Cohen's d.\n",
        "            return {\"cohens_d\": float('nan')}\n",
        "    \n",
        "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
        "    \n",
        "    if pooled_std == 0: # Avoid division by zero if pooled_std is zero (e.g. single identical value in both groups)\n",
        "        if mean1 == mean2:\n",
        "            return {\"cohens_d\": 0.0}\n",
        "        else:\n",
        "            return {\"cohens_d\": float('inf') if mean1 > mean2 else float('-inf')} # Or NaN\n",
        "\n",
        "    d = (mean1 - mean2) / pooled_std\n",
        "    return {\"cohens_d\": round(d, 4)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "cohens_d_result = compute_cohens_d(sample_group1, sample_group2)\n",
        "print(f\"Cohen's d result: {json.dumps(cohens_d_result)}\")\n",
        "\n",
        "cohens_d_result_ns = compute_cohens_d(sample_group3, sample_group4)\n",
        "print(f\"Cohen's d result (small effect): {json.dumps(cohens_d_result_ns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Compute Confidence Interval\n",
        "\n",
        "This function computes the confidence interval for the mean of a single sample group. A confidence interval provides a range of values within which the true population mean is likely to fall, with a certain level of confidence (e.g., 95%). It helps in understanding the precision of the sample mean as an estimate of the population mean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def compute_confidence_interval(data, confidence=0.95):\n",
        "    \"\"\"Compute confidence interval for a single sample group's mean.\"\"\"\n",
        "    data_arr = np.array(data)\n",
        "    n = len(data_arr)\n",
        "    if n < 2: # Need at least 2 data points to compute standard error and CI\n",
        "        return {\n",
        "            \"mean\": round(np.mean(data_arr), 4) if n == 1 else float('nan'),\n",
        "            \"confidence_interval\": [float('nan'), float('nan')],\n",
        "            \"confidence\": confidence,\n",
        "            \"error\": \"Sample size too small for CI calculation (n < 2)\"\n",
        "        }\n",
        "\n",
        "    mean = np.mean(data_arr)\n",
        "    # Standard Error of the Mean (SEM)\n",
        "    standard_error = sem(data_arr)\n",
        "    \n",
        "    if standard_error == 0: # If all values are the same, SEM is 0, interval is just the mean\n",
        "        margin_of_error = 0\n",
        "    else:\n",
        "        # Degrees of freedom\n",
        "        dof = n - 1\n",
        "        # t-critical value from t-distribution's percent point function (ppf)\n",
        "        t_critical = t.ppf((1 + confidence) / 2., dof)\n",
        "        margin_of_error = standard_error * t_critical\n",
        "\n",
        "    lower_bound = mean - margin_of_error\n",
        "    upper_bound = mean + margin_of_error\n",
        "    \n",
        "    return {\n",
        "        \"mean\": round(mean, 4),\n",
        "        \"confidence_interval\": [round(lower_bound, 4), round(upper_bound, 4)],\n",
        "        \"confidence_level\": confidence\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "sample_data = [10, 12, 11.5, 13, 10.5, 11, 12.5, 14, 10.8, 11.2]\n",
        "ci_result = compute_confidence_interval(sample_data, confidence=0.95)\n",
        "print(f\"Confidence Interval (95%): {json.dumps(ci_result, indent=2)}\")\n",
        "\n",
        "ci_result_99 = compute_confidence_interval(sample_data, confidence=0.99)\n",
        "print(f\"Confidence Interval (99%): {json.dumps(ci_result_99, indent=2)}\")\n",
        "\n",
        "single_point_data = [5]\n",
        "ci_single = compute_confidence_interval(single_point_data)\n",
        "print(f\"Confidence Interval (single point): {json.dumps(ci_single, indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Describe Group\n",
        "\n",
        "This is a utility function to provide basic descriptive statistics for a sample group: mean, standard deviation (sample), and sample size (n). This information is often fundamental for interpreting research results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def describe_group(data):\n",
        "    \"\"\"Summarize sample mean, standard deviation, and count for a group.\"\"\"\n",
        "    data_arr = np.array(data)\n",
        "    n = len(data_arr)\n",
        "    \n",
        "    if n == 0:\n",
        "        return {\"mean\": float('nan'), \"std_dev\": float('nan'), \"n\": 0, \"error\": \"Empty data array\"}\n",
        "    \n",
        "    mean = np.mean(data_arr)\n",
        "    # ddof=1 for sample standard deviation; if n=1, std_dev is undefined (NaN by numpy)\n",
        "    std_dev = np.std(data_arr, ddof=1) if n > 1 else 0.0 # or float('nan') if preferred for n=1\n",
        "    \n",
        "    return {\n",
        "        \"mean\": round(mean, 4),\n",
        "        \"std_dev\": round(std_dev, 4) if not np.isnan(std_dev) else float('nan'),\n",
        "        \"n\": n\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "description_result = describe_group(sample_data)\n",
        "print(f\"Group Description: {json.dumps(description_result, indent=2)}\")\n",
        "\n",
        "empty_data = []\n",
        "description_empty = describe_group(empty_data)\n",
        "print(f\"Group Description (empty): {json.dumps(description_empty, indent=2)}\")\n",
        "\n",
        "single_value_data = [100]\n",
        "description_single = describe_group(single_value_data)\n",
        "print(f\"Group Description (single value): {json.dumps(description_single, indent=2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These helper functions provide a solid foundation for the statistical analysis capabilities of our research paper reviewer. In later lessons, we'll see how to make these accessible to the o4-mini model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Implement a `calculate_variance(data)` function:**\n",
        "    *   This function should take a list or NumPy array `data` as input.\n",
        "    *   It should calculate and return the sample variance (where the denominator is N-1).\n",
        "    *   The result should be a dictionary like `{\"variance\": value}`.\n",
        "    *   Remember to handle cases like an empty list or a list with a single element (variance is undefined or 0 respectively).\n",
        "    *   Test your function with a few examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def calculate_variance(data):\n",
        "    \"\"\"Calculate the sample variance for a list or NumPy array of data.\"\"\"\n",
        "    data_arr = np.array(data)\n",
        "    n = len(data_arr)\n",
        "    \n",
        "    if n < 2:\n",
        "        # Variance is undefined for n < 2 (or 0 for n=1 if defined that way, np.var returns 0 for ddof=0)\n",
        "        # For sample variance (ddof=1), it's typically undefined or NaN.\n",
        "        return {\"variance\": float('nan'), \"error\": \"Sample size < 2, variance undefined\"}\n",
        "    \n",
        "    # np.var with ddof=1 calculates sample variance\n",
        "    variance_val = np.var(data_arr, ddof=1)\n",
        "    return {\"variance\": round(variance_val, 4)}\n",
        "\n",
        "# Test cases for calculate_variance\n",
        "test_data_variance = [1, 2, 3, 4, 5]\n",
        "variance_result = calculate_variance(test_data_variance)\n",
        "print(f\"Variance for {test_data_variance}: {json.dumps(variance_result)}\") # Expected: (1^2+0^2+(-1)^2+(-2)^2) / (5-1) = (1+0+1+4)/4 = 2.5. No, ( (1-3)^2 + (2-3)^2 + (3-3)^2 + (4-3)^2 + (5-3)^2 ) / (5-1) = (4+1+0+1+4)/4 = 10/4 = 2.5\n",
        "\n",
        "test_data_single_variance = [10]\n",
        "variance_single_result = calculate_variance(test_data_single_variance)\n",
        "print(f\"Variance for {test_data_single_variance}: {json.dumps(variance_single_result)}\")\n",
        "\n",
        "test_data_empty_variance = []\n",
        "variance_empty_result = calculate_variance(test_data_empty_variance)\n",
        "print(f\"Variance for {test_data_empty_variance}: {json.dumps(variance_empty_result)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 5: Extracting and Preparing Research Paper Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Extracting and Preparing Content\n",
        "\n",
        "**(0:00-0:20) Introduction**\n",
        "\n",
        "**Host:** Hi again! We've built our statistical helper functions. Now, we need to get the actual content from research papers for our o4-mini model to analyze. Most research papers are in PDF format, and LLMs have limits on how much text they can process at once. So, this lesson is all about extraction and preparation.\n",
        "\n",
        "*(Visual: Title card - \"Lesson 5: Extracting and Preparing Research Paper Content\")*\n",
        "\n",
        "**(0:20-0:50) Using PyMuPDF to Extract Text from PDFs**\n",
        "\n",
        "**Host:** PDFs can be tricky. They contain text, images, complex layouts, tables, and sometimes scanned pages. For our project, we'll focus on extracting the textual content. We'll use a Python library called `PyMuPDF`, which is imported as `fitz`. \n",
        "\n",
        "`PyMuPDF` allows us to open a PDF file, iterate through its pages, and extract the raw text from each page. We can then combine this text into a single string representing the entire paper's content.\n",
        "\n",
        "*(Visual: Animation: PDF document icon -> PyMuPDF/fitz logo -> extracted text icon. Show a snippet of Python code for opening a PDF and getting text from a page.)*\n",
        "\n",
        "**(0:50-1:45) Tokenization and Chunking for LLM Context Limits**\n",
        "\n",
        "**Host:** Now, here's a critical step. Large Language Models like o4-mini don't read text character by character or word by word in the way humans do. They process text in units called 'tokens.' A token can be a whole word, part of a word, or even just a punctuation mark. \n",
        "\n",
        "*(Visual: Example: \"Hello world!\" -> Tokens: [\"Hello\", \" world\", \"!\"] (approximation).)*\n",
        "\n",
        "**Host:** Each LLM has a 'context window' – a maximum number of tokens it can consider at one time. For o4-mini, this might be, for example, 128,000 tokens with the `cl100k_base` encoding, but the effective input for a single reasoning step might be smaller, like the 12,000 tokens mentioned in the reference tutorial for chunking. Research papers can be very long, easily exceeding this limit. If we send too much text, the model will either error out or truncate the input, losing valuable information.\n",
        "\n",
        "*(Visual: A horizontal bar representing the LLM's context window. Show a long document scroll being too big for the bar, then being cut into smaller pieces that fit.)*\n",
        "\n",
        "**Host:** To handle this, we first tokenize the text using OpenAI's `tiktoken` library. This library helps us count tokens accurately for specific models. Then, we 'chunk' the text – we break the long sequence of tokens into smaller, manageable segments or 'chunks,' each fitting within the model's processing limit. We'll aim for a maximum token count per chunk, like 12,000 tokens as used in the tutorial's example.\n",
        "\n",
        "*(Visual: Flowchart: Full Text -> `tiktoken` (Tokenization) -> List of Tokens -> Chunking Logic -> List of Text Chunks.)*\n",
        "\n",
        "**(1:45-2:10) Preparing Coherent Input for the Model**\n",
        "\n",
        "**Host:** When chunking, it's important to make sure each chunk is still somewhat coherent. We simply split by a fixed number of tokens. While more advanced methods like semantic chunking exist (breaking text at natural paragraph or section boundaries), for this project, fixed-size token chunking is a good starting point. The key is that each chunk is a self-contained piece of text that o4-mini can analyze. We decode these token chunks back into text before sending them to the model.\n",
        "\n",
        "This process ensures we can feed an entire research paper to our model, piece by piece, without overwhelming it and without losing context by just cutting off the end of the paper.\n",
        "\n",
        "*(Visual: Show a long document being split into several numbered chunks. Emphasize that each chunk is processed individually.)*\n",
        "\n",
        "**(2:10-2:20) Let's Code It!**\n",
        "\n",
        "**Host:** In the notebook, we'll write Python functions to implement PDF text extraction and this token-based chunking strategy. Let's get to it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Extracting and Preparing Content\n",
        "\n",
        "This notebook covers two main tasks:\n",
        "1.  Extracting raw text from PDF files using `PyMuPDF` (imported as `fitz`).\n",
        "2.  Tokenizing and chunking the extracted text using `tiktoken` to prepare it for the LLM, respecting its context window limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Required Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import fitz  # PyMuPDF\n",
        "import tiktoken\n",
        "import os # For path operations if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Extracting Text from PDF\n",
        "\n",
        "We'll define a function `extract_text_from_pdf` that takes a PDF file path, opens the PDF, iterates through each page, extracts text, and concatenates it into a single string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extracts all text content from a PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)  # Open the PDF\n",
        "    except RuntimeError as e:\n",
        "        return f\"Error opening PDF: {e}\"\n",
        "    \n",
        "    full_text = \"\"\n",
        "    for page_num in range(len(doc)):\n",
        "        page = doc.load_page(page_num)  # Load one page\n",
        "        full_text += page.get_text() + \"\\n\"  # Extract text and add a newline for separation\n",
        "    \n",
        "    doc.close()\n",
        "    return full_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage (Placeholder):**\n",
        "\n",
        "To run the example below, you'll need a PDF file. The tutorial mentions a `Fake_paper.pdf`. If you have it, place it in the same directory as this notebook and uncomment the lines. Otherwise, this cell will show how to use it conceptually. For a quick test, you can create a dummy PDF or use any PDF you have available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Create a dummy PDF for testing if you don't have one\n",
        "try:\n",
        "    doc = fitz.open() # new empty PDF\n",
        "    page = doc.new_page()\n",
        "    page.insert_text(fitz.Point(50, 72), \"This is a test PDF document for demonstration purposes.\")\n",
        "    page.insert_text(fitz.Point(50, 92), \"It contains a few lines of text to check extraction.\")\n",
        "    dummy_pdf_path = \"dummy_paper.pdf\"\n",
        "    doc.save(dummy_pdf_path)\n",
        "    doc.close()\n",
        "    print(f\"Created '{dummy_pdf_path}' for testing.\")\n",
        "\n",
        "    # Test extraction\n",
        "    # Replace 'dummy_paper.pdf' with the path to your PDF if you have one e.g. \"Fake_paper.pdf\"\n",
        "    # pdf_file_path = \"Fake_paper.pdf\" # Or use dummy_pdf_path\n",
        "    extracted_text = extract_text_from_pdf(dummy_pdf_path)\n",
        "\n",
        "    if not extracted_text.startswith(\"Error\"):\n",
        "        print(f\"Successfully extracted text (first 200 chars):\\n{extracted_text[:200]}...\")\n",
        "    else:\n",
        "        print(extracted_text)\n",
        "        \n",
        "    # Clean up the dummy file\n",
        "    if os.path.exists(dummy_pdf_path):\n",
        "        os.remove(dummy_pdf_path)\n",
        "        print(f\"Removed '{dummy_pdf_path}'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during dummy PDF creation or extraction: {e}\")\n",
        "    print(\"Please ensure PyMuPDF is installed correctly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Chunking Text using Tiktoken\n",
        "\n",
        "**Why Chunking Matters for LLMs:**\n",
        "LLMs have a finite context window (maximum number of tokens they can process at once). Long documents like research papers usually exceed this limit. Chunking breaks the document into smaller pieces that fit within this window, allowing the LLM to process the entire document sequentially, chunk by chunk.\n",
        "\n",
        "**Ensuring No Loss of Context or Text:**\n",
        "Our chunking strategy will be based on token count. We'll encode the entire text into tokens, then split this token list into sub-lists of a maximum size. Each sub-list of tokens is then decoded back into text to form a chunk. This ensures that we process the whole document without abrupt cutoffs in the middle of sentences if possible (though simple fixed-size token chunking might still split sentences; more advanced methods could use sentence boundaries).\n",
        "\n",
        "We'll use the `cl100k_base` encoding, which is compatible with models like GPT-4, GPT-4o, and o4-mini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def chunk_text(text, max_tokens=12000, model_encoding=\"cl100k_base\"):\n",
        "    \"\"\"Chunks text into segments with a maximum number of tokens.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(model_encoding)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting encoding: {e}. Using cl100k_base as fallback.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\") # Fallback\n",
        "        \n",
        "    tokens = encoding.encode(text)\n",
        "    \n",
        "    chunks = []\n",
        "    for i in range(0, len(tokens), max_tokens):\n",
        "        chunk_tokens = tokens[i:i + max_tokens]\n",
        "        chunk_text = encoding.decode(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        \n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Example Usage:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "sample_long_text = \"This is a sample text. \" * 1000  # Create a reasonably long string\n",
        " # o4-mini's context window can be large, but the tutorial uses 12000 tokens for chunking for the reasoning API.\n",
        " # For this example, let's use a much smaller max_tokens to demonstrate chunking clearly.\n",
        "text_chunks = chunk_text(sample_long_text, max_tokens=50) \n",
        "\n",
        "print(f\"Original text length (chars): {len(sample_long_text)}\")\n",
        "print(f\"Number of chunks created: {len(text_chunks)}\")\n",
        "\n",
        "if text_chunks:\n",
        "    print(f\"\\nFirst chunk (first 100 chars):\\n{text_chunks[0][:100]}...\")\n",
        "    # Let's check token count of the first chunk\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens_in_first_chunk = encoding.encode(text_chunks[0])\n",
        "    print(f\"Tokens in first chunk: {len(tokens_in_first_chunk)}\")\n",
        "\n",
        "    if len(text_chunks) > 1:\n",
        "        print(f\"\\nSecond chunk (first 100 chars):\\n{text_chunks[1][:100]}...\")\n",
        "        tokens_in_second_chunk = encoding.encode(text_chunks[1])\n",
        "        print(f\"Tokens in second chunk: {len(tokens_in_second_chunk)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With these functions, we can now take a PDF, extract its content, and prepare it in a format suitable for processing by o4-mini, one chunk at a time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Modify `chunk_text` to include an overlap between chunks:**\n",
        "    *   Add a new parameter `overlap_tokens` to the `chunk_text` function (e.g., default to 50 or 100 tokens).\n",
        "    *   When creating subsequent chunks (all chunks after the first one), make them start `overlap_tokens` earlier in the original token list. This means the end of one chunk will overlap with the beginning of the next.\n",
        "    *   For example, if `max_tokens` is 1000 and `overlap_tokens` is 50:\n",
        "        *   Chunk 1: tokens 0 to 999\n",
        "        *   Chunk 2: tokens 950 to 1949 (starts 50 tokens before 1000, and still takes `max_tokens`)\n",
        "        *   Chunk 3: tokens 1900 to 2899\n",
        "    *   **Why might this be useful?** Overlapping chunks can help maintain context across chunk boundaries. Information near the end of one chunk and the beginning of the next is seen by the model in two slightly different contexts, which can help prevent loss of continuity in the analysis.\n",
        "    *   Test your modified function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def chunk_text_with_overlap(text, max_tokens=12000, overlap_tokens=100, model_encoding=\"cl100k_base\"):\n",
        "    \"\"\"Chunks text into segments with overlap and a maximum number of tokens.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.get_encoding(model_encoding)\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting encoding: {e}. Using cl100k_base as fallback.\")\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "        \n",
        "    tokens = encoding.encode(text)\n",
        "    if not tokens:\n",
        "        return []\n",
        "\n",
        "    chunks = []\n",
        "    step_size = max_tokens - overlap_tokens\n",
        "    if step_size <= 0:\n",
        "        # Avoid infinite loop or negative step if overlap is too large\n",
        "        print(\"Warning: overlap_tokens >= max_tokens. Using step_size = max_tokens / 2 for safety.\")\n",
        "        step_size = max_tokens // 2 if max_tokens > 1 else 1\n",
        "        if step_size == 0: step_size = 1 # Ensure step_size is at least 1\n",
        "\n",
        "    for i in range(0, len(tokens), step_size):\n",
        "        chunk_tokens = tokens[i : i + max_tokens]\n",
        "        if not chunk_tokens: # Should not happen if len(tokens) > 0\n",
        "            continue\n",
        "        chunk_text = encoding.decode(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        # If the last chunk created goes beyond the total tokens, we're done.\n",
        "        # This check is implicitly handled by the loop's end condition and slicing.\n",
        "        # The last chunk might be smaller than max_tokens.\n",
        "        if i + max_tokens >= len(tokens):\n",
        "            break\n",
        "            \n",
        "    return chunks\n",
        "\n",
        "# Test the modified function\n",
        "sample_long_text_overlap = \"This is sentence one. This is sentence two. This is sentence three. This is sentence four. This is sentence five. This is sentence six. This is sentence seven. This is sentence eight. This is sentence nine. This is sentence ten. This is sentence eleven. This is sentence twelve. \" * 10\n",
        "\n",
        "# Use smaller token counts for easier inspection of overlap\n",
        "max_t = 30\n",
        "overlap_t = 5\n",
        "\n",
        "print(f\"\\nTesting chunk_text_with_overlap with max_tokens={max_t}, overlap_tokens={overlap_t}\")\n",
        "overlapped_chunks = chunk_text_with_overlap(sample_long_text_overlap, max_tokens=max_t, overlap_tokens=overlap_t)\n",
        "\n",
        "print(f\"Number of overlapped chunks: {len(overlapped_chunks)}\")\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "for i, chunk in enumerate(overlapped_chunks):\n",
        "    tokens_in_chunk = encoding.encode(chunk)\n",
        "    print(f\"Chunk {i+1} (len: {len(tokens_in_chunk)} tokens): '{chunk[:50]}...'\" )\n",
        "\n",
        "# Verify overlap (conceptual - requires careful inspection of output)\n",
        "if len(overlapped_chunks) > 1:\n",
        "    print(\"\\nChecking overlap between chunk 1 and 2 (conceptually):\")\n",
        "    # Last few words of chunk 1\n",
        "    # First few words of chunk 2\n",
        "    # They should share some common words if overlap is working\n",
        "    # For token-based overlap, exact word match might vary but sequence should overlap.\n",
        "    \n",
        "    # To truly verify, one would decode a small part of the end of tokens of chunk 1\n",
        "    # and a small part of the start of tokens of chunk 2.\n",
        "    tokens_c1 = encoding.encode(overlapped_chunks[0])\n",
        "    tokens_c2 = encoding.encode(overlapped_chunks[1])\n",
        "    \n",
        "    # The last `overlap_tokens` of the *intended* full first chunk (before it might be truncated if it's the last chunk)\n",
        "    # should align with the first `overlap_tokens` of the second chunk.\n",
        "    # More precisely, tokens_c2 should start with tokens_c1[max_t - overlap_t : max_t]\n",
        "    # This is true if chunk1 was not the very last chunk and had full max_tokens length.\n",
        "    \n",
        "    # Let's print end of chunk 1 and start of chunk 2\n",
        "    print(f\"End of Chunk 1: ...{overlapped_chunks[0][-30:]}\")\n",
        "    print(f\"Start of Chunk 2: {overlapped_chunks[1][:30]}...\")\n",
        "\n",
        "    # A more robust check:\n",
        "    # Original tokens list:\n",
        "    original_tokens = encoding.encode(sample_long_text_overlap)\n",
        "    # Tokens for chunk 1 (as generated by the function)\n",
        "    c1_gen_tokens = encoding.encode(overlapped_chunks[0])\n",
        "    # Tokens for chunk 2 (as generated by the function)\n",
        "    c2_gen_tokens = encoding.encode(overlapped_chunks[1])\n",
        "\n",
        "    # Expected start of chunk 2 tokens, based on overlap from chunk 1's source tokens in original_tokens\n",
        "    # Chunk 1 would be original_tokens[0:max_t]\n",
        "    # Chunk 2 should start from original_tokens[max_t - overlap_t : max_t - overlap_t + max_t]\n",
        "    expected_start_index_for_c2 = max_t - overlap_t\n",
        "    expected_c2_tokens_from_original = original_tokens[expected_start_index_for_c2 : expected_start_index_for_c2 + max_t]\n",
        "\n",
        "    if list(c2_gen_tokens) == list(expected_c2_tokens_from_original)[:len(c2_gen_tokens)]:\n",
        "        print(\"\\nToken overlap verification successful (comparing generated chunk 2 with expected tokens based on overlap).\")\n",
        "    else:\n",
        "        print(\"\\nToken overlap verification failed or needs closer inspection.\")\n",
        "        # print(f\"Generated C2 tokens: {list(c2_gen_tokens)[:10]}\")\n",
        "        # print(f\"Expected C2 tokens: {list(expected_c2_tokens_from_original)[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 6: Mapping and Registering Tools for LLM Access"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Mapping and Registering Tools\n",
        "\n",
        "**(0:00-0:20) Introduction**\n",
        "\n",
        "**Host:** Hello and welcome back! In Lesson 4, we built a set of Python functions for statistical analysis. Now, how do we tell our o4-mini model that these tools exist and how to use them? That's where tool mapping and tool registration (or schema definition) come in.\n",
        "\n",
        "*(Visual: Title card - \"Lesson 6: Mapping and Registering Tools for LLM Access\")*\n",
        "\n",
        "**(0:20-0:45) What is Tool Mapping and Why Does It Matter?**\n",
        "\n",
        "**Host:** When the LLM decides to call a function, it will refer to it by a specific name. Our Python code needs to know which actual Python function corresponds to that name. This is 'tool mapping.' \n",
        "\n",
        "Typically, we create a simple dictionary in our code where keys are the function names the LLM will use, and values are the actual Python function objects. For example, the LLM might say \"call `recalculate_p_value`,\" and our map will point this to our `recalculate_p_value` Python function.\n",
        "\n",
        "*(Visual: Diagram: LLM says \"Call 'compute_effect_size'\" -> Arrow points to a dictionary: `{\"compute_effect_size\": our_python_cohens_d_function}` -> Arrow to the Python function code.)*\n",
        "\n",
        "**Host:** This mapping acts as a bridge, ensuring that when the LLM requests a tool, the correct piece of our code gets executed.\n",
        "\n",
        "**(0:45-1:45) Defining Tool Schemas for OpenAI Function Calling**\n",
        "\n",
        "**Host:** Just knowing the name isn't enough for the LLM. It also needs to understand *what the tool does* and *what information (parameters) it needs* to do its job. This is where tool schemas come in. For the OpenAI API, we define these schemas in a specific JSON format.\n",
        "\n",
        "Each tool definition typically includes:\n",
        "1.  `type`: This is usually \"function\".\n",
        "2.  `name`: The name of the function, which the LLM will use (this should match the key in our tool map).\n",
        "3.  `description`: A clear, concise explanation of what the function does. This is crucial! The LLM uses this description to decide *when* it's appropriate to call this function.\n",
        "4.  `parameters`: An object describing the inputs the function expects. This itself has properties:\n",
        "    *   `type`: Usually \"object\".\n",
        "    *   `properties`: An object where each key is a parameter name (e.g., \"group1\", \"data\", \"confidence\"). For each parameter, we define:\n",
        "        *   `type`: The data type (e.g., \"array\", \"number\", \"string\").\n",
        "        *   `description`: A description of that parameter.\n",
        "        *   `items` (if type is \"array\"): Describes the type of items in the array (e.g., `{\"type\": \"number\"}`).\n",
        "    *   `required`: A list of parameter names that are mandatory for the function call.\n",
        "\n",
        "*(Visual: A snippet of the JSON schema for one tool, highlighting `name`, `description`, and `parameters`. Zoom into the `properties` section to show parameter definitions.)*\n",
        "\n",
        "**Host:** Crafting good descriptions and accurate parameter schemas is vital. It helps the LLM make intelligent decisions about which tool to use, when, and how to format the arguments correctly.\n",
        "\n",
        "**(1:45-2:10) Connecting LLM Calls to Local Code**\n",
        "\n",
        "**Host:** So, the workflow is: \n",
        "1. We define these tool schemas and send them to the LLM as part of our initial request, along with our prompt and the text chunk to analyze.\n",
        "2. The LLM analyzes the text and, based on our prompt and its understanding of the tools (from their schemas), might decide to call one.\n",
        "3. If it does, its response will include a `function_call` object with the `name` of the function and the `arguments` (as a JSON string).\n",
        "4. Our code parses this, uses our `tool_function_map` to find the actual Python function, executes it with the provided arguments, and then sends the result back to the LLM.\n",
        "\n",
        "*(Visual: Recap flowchart: 1. Define Schemas -> 2. Send to LLM -> 3. LLM requests tool -> 4. Your code uses map, executes, returns result.)*\n",
        "\n",
        "**(2:10-2:20) Let's Define These in the Notebook!**\n",
        "\n",
        "**Host:** In the upcoming notebook segment, we'll create the `tool_function_map` dictionary and write out the JSON-like schemas for each of our statistical helper functions. This will set the stage for the core reasoning loop we'll build in the next lesson."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Mapping and Registering Tools\n",
        "\n",
        "In this notebook, we'll define two crucial components for enabling function calling with the o4-mini model:\n",
        "1.  **`tool_function_map`**: A Python dictionary that maps the function names (as the LLM will know them) to our actual Python functions from Lesson 4.\n",
        "2.  **`tools` list**: A list of dictionaries, where each dictionary is a schema describing a function to the OpenAI API. This schema tells the LLM what the function does and what parameters it expects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Importing Helper Functions\n",
        "\n",
        "First, let's make sure our statistical helper functions from Lesson 4 are accessible. If you're running this in a continuous environment, they might still be in memory. For a standalone script or a fresh notebook session, you'd typically import them from a separate Python file (e.g., `statistics_helper.py`).\n",
        "\n",
        "For this notebook, we'll redefine them here for simplicity, or you can copy-paste them from your Lesson 4 solution. **It's good practice to eventually move these into a separate `.py` file and import them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Assuming functions from Lesson 4 are defined in the current scope or imported.\n",
        "# For clarity, let's redefine them or imagine they are imported from statistics_helper.py\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, sem, t\n",
        "import json\n",
        "\n",
        "def recalculate_p_value(group1, group2):\n",
        "    group1_arr = np.array(group1)\n",
        "    group2_arr = np.array(group2)\n",
        "    t_stat, p_value = ttest_ind(group1_arr, group2_arr, equal_var=False)\n",
        "    return {\"p_value\": round(p_value, 4)}\n",
        "\n",
        "def compute_cohens_d(group1, group2):\n",
        "    group1_arr = np.array(group1)\n",
        "    group2_arr = np.array(group2)\n",
        "    mean1, mean2 = np.mean(group1_arr), np.mean(group2_arr)\n",
        "    std1, std2 = np.std(group1_arr, ddof=1), np.std(group2_arr, ddof=1)\n",
        "    if std1 == 0 and std2 == 0:\n",
        "        return {\"cohens_d\": 0.0 if mean1 == mean2 else float('nan')}\n",
        "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
        "    if pooled_std == 0:\n",
        "        return {\"cohens_d\": 0.0 if mean1 == mean2 else (float('inf') if mean1 > mean2 else float('-inf'))}\n",
        "    d = (mean1 - mean2) / pooled_std\n",
        "    return {\"cohens_d\": round(d, 4)}\n",
        "\n",
        "def compute_confidence_interval(data, confidence=0.95):\n",
        "    data_arr = np.array(data)\n",
        "    n = len(data_arr)\n",
        "    if n < 2: \n",
        "        return {\"mean\": round(np.mean(data_arr), 4) if n == 1 else float('nan'), \"confidence_interval\": [float('nan'), float('nan')], \"confidence_level\": confidence, \"error\": \"Sample size too small\"}\n",
        "    mean = np.mean(data_arr)\n",
        "    standard_error = sem(data_arr)\n",
        "    if standard_error == 0: margin_of_error = 0\n",
        "    else: margin_of_error = standard_error * t.ppf((1 + confidence) / 2., n-1)\n",
        "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence_level\": confidence}\n",
        "\n",
        "def describe_group(data):\n",
        "    data_arr = np.array(data)\n",
        "    n = len(data_arr)\n",
        "    if n == 0: return {\"mean\": float('nan'), \"std_dev\": float('nan'), \"n\": 0, \"error\": \"Empty data\"}\n",
        "    mean = np.mean(data_arr)\n",
        "    std_dev = np.std(data_arr, ddof=1) if n > 1 else 0.0\n",
        "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4) if not np.isnan(std_dev) else float('nan'), \"n\": n}\n",
        "\n",
        "# And the practice exercise function from Lesson 4, if you did it:\n",
        "def calculate_variance(data):\n",
        "    data_arr = np.array(data)\n",
        "    n = len(data_arr)\n",
        "    if n < 2: return {\"variance\": float('nan'), \"error\": \"Sample size < 2\"}\n",
        "    variance_val = np.var(data_arr, ddof=1)\n",
        "    return {\"variance\": round(variance_val, 4)}\n",
        "\n",
        "print(\"Helper functions ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Tool Function Map (`tool_function_map`)\n",
        "\n",
        "This dictionary maps the string name that the LLM will use for a function to the actual Python function object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "tool_function_map = {\n",
        "    \"recalculate_p_value\": recalculate_p_value,\n",
        "    \"compute_cohens_d\": compute_cohens_d,\n",
        "    \"compute_confidence_interval\": compute_confidence_interval,\n",
        "    \"describe_group\": describe_group,\n",
        "    # Add your custom functions here if you created them, e.g.:\n",
        "    # \"calculate_variance\": calculate_variance \n",
        "}\n",
        "\n",
        "print(\"tool_function_map created:\")\n",
        "for name, func in tool_function_map.items():\n",
        "    print(f\"  '{name}': {func.__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Tool Schema Definitions (`tools` list)\n",
        "\n",
        "This is a list of dictionaries, where each dictionary defines a tool according to the OpenAI API's expected schema. The LLM uses these definitions to understand what each tool does and what arguments it needs.\n",
        "\n",
        "**Key components of a tool schema:**\n",
        "-   `type`: Always \"function\" for our use case.\n",
        "-   `function`:\n",
        "    -   `name`: The name the LLM will use to call the function (must match a key in `tool_function_map`).\n",
        "    -   `description`: A clear explanation of what the function does. This is very important for the LLM to decide when to use the tool.\n",
        "    -   `parameters`: An object describing the function's parameters.\n",
        "        -   `type`: Should be \"object\".\n",
        "        -   `properties`: An object where each key is a parameter name.\n",
        "            -   `type`: The JSON schema type (e.g., \"array\", \"number\", \"string\", \"boolean\").\n",
        "            -   `description`: Description of the parameter.\n",
        "            -   `items` (if `type` is \"array\"): An object describing the type of items in the array (e.g., `{\"type\": \"number\"}`).\n",
        "        -   `required`: A list of parameter names that are mandatory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"recalculate_p_value\",\n",
        "            \"description\": \"Calculate p-value (Welch's t-test) between two independent sample groups (lists of numbers). Use this to verify claims of statistical significance.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"group1\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"description\": \"First sample group data (list of numbers).\"\n",
        "                    },\n",
        "                    \"group2\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"description\": \"Second sample group data (list of numbers).\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"group1\", \"group2\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"compute_cohens_d\",\n",
        "            \"description\": \"Compute effect size (Cohen's d) between two independent sample groups (lists of numbers). Use this to understand the magnitude of a difference, not just its statistical significance.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"group1\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"description\": \"First sample group data (list of numbers).\"\n",
        "                    },\n",
        "                    \"group2\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"description\": \"Second sample group data (list of numbers).\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"group1\", \"group2\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"compute_confidence_interval\",\n",
        "            \"description\": \"Compute confidence interval for the mean of a single sample group (list of numbers). Use this to assess the precision of an estimated mean.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"data\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"description\": \"Sample group data (list of numbers).\"\n",
        "                    },\n",
        "                    \"confidence\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"Confidence level for the interval (e.g., 0.95 for 95%). Default is 0.95.\",\n",
        "                        \"default\": 0.95\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"data\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"describe_group\",\n",
        "            \"description\": \"Summarize a sample group (list of numbers) by calculating its mean, standard deviation (sample), and count (n). Use this for basic data description.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"data\": {\n",
        "                        \"type\": \"array\",\n",
        "                        \"items\": {\"type\": \"number\"},\n",
        "                        \"description\": \"Sample group data (list of numbers).\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"data\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    # Add schemas for your custom functions here\n",
        "]\n",
        "\n",
        "print(f\"Defined {len(tools)} tools.\")\n",
        "print(\"First tool schema:\")\n",
        "print(json.dumps(tools[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Input/Output Validation and Error Handling Basics:**\n",
        "\n",
        "While the OpenAI API uses these schemas to guide the LLM in formatting its function call requests, it's still crucial that your actual Python functions (the ones in `tool_function_map`) perform their own robust input validation and error handling.\n",
        "-   **Type Checking/Conversion:** Ensure inputs are of the expected type (e.g., lists of numbers). Convert if necessary and safe.\n",
        "-   **Handling Edge Cases:** What if lists are empty? What if standard deviation is zero? Our helper functions already include some of this.\n",
        "-   **Returning Errors:** If a function cannot perform its calculation due to bad input, it should return a clear error message or an appropriate value (like NaN), which can then be sent back to the LLM. The LLM might then be able to comment on the data quality issue.\n",
        "\n",
        "The schemas help the LLM try to call your functions correctly, but your functions are the last line of defense for correctness and robustness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Define the tool schema for `calculate_variance`** (the function you created in Lesson 4's practice exercise, or defined earlier in this notebook).\n",
        "2.  Add this schema to the `tools` list.\n",
        "3.  Add the `calculate_variance` function to the `tool_function_map` (if you haven't already)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# 1. Define the schema for calculate_variance\n",
        "variance_tool_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"calculate_variance\",\n",
        "        \"description\": \"Calculate the sample variance for a list of numbers. Sample variance uses N-1 in the denominator.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"data\": {\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\"type\": \"number\"},\n",
        "                    \"description\": \"A list of numbers for which to calculate the sample variance.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"data\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# 2. Add the schema to the tools list\n",
        "# Make sure not to add it if it's already there from a previous run\n",
        "if not any(tool['function']['name'] == 'calculate_variance' for tool in tools):\n",
        "    tools.append(variance_tool_schema)\n",
        "    print(\"'calculate_variance' schema added to tools list.\")\n",
        "else:\n",
        "    print(\"'calculate_variance' schema already in tools list.\")\n",
        "\n",
        "# 3. Add the function to tool_function_map\n",
        "if 'calculate_variance' not in tool_function_map:\n",
        "    tool_function_map['calculate_variance'] = calculate_variance\n",
        "    print(\"'calculate_variance' function added to tool_function_map.\")\n",
        "else:\n",
        "    print(\"'calculate_variance' function already in tool_function_map.\")\n",
        "\n",
        "# Verify\n",
        "print(f\"\\nTotal tools defined: {len(tools)}\")\n",
        "print(f\"'calculate_variance' in tool_function_map: {'calculate_variance' in tool_function_map}\")\n",
        "print(f\"Schema for 'calculate_variance':\")\n",
        "for tool in tools:\n",
        "    if tool['function']['name'] == 'calculate_variance':\n",
        "        print(json.dumps(tool, indent=2))\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 7: Reasoning Workflow—Reviewing Paper Chunks with o4-mini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Reasoning Workflow with o4-mini\n",
        "\n",
        "**(0:00-0:25) Introduction**\n",
        "\n",
        "**Host:** We've set up our environment, learned about prompt engineering, built statistical tools, prepared our PDF content, and registered our tools with schemas. Now it's time for the main event: implementing the reasoning workflow where o4-mini reviews chunks of a research paper and uses our tools!\n",
        "\n",
        "*(Visual: Title card - \"Lesson 7: Reasoning Workflow - Reviewing Paper Chunks with o4-mini\")*\n",
        "\n",
        "**(0:25-1:00) The Model's System Prompt: The Critical Reviewer**\n",
        "\n",
        "**Host:** The first key ingredient in our interaction with o4-mini is the **system prompt**. This is where we set the stage, define the AI's role, and give it its primary instructions. For our project, the system prompt will instruct o4-mini to act as an expert AI research reviewer. \n",
        "\n",
        "It will be told to: \n",
        "- Read the provided text chunk from a research paper.\n",
        "- Highlight weak arguments, unsupported claims, or flawed methodology.\n",
        "- Crucially, it will be informed about the available statistical tools (like recalculating p-values, computing Cohen's d, etc.) and encouraged to use them to be rigorous.\n",
        "- And finally, to explain its reasoning, suggest improvements, and offer a verdict on the chunk.\n",
        "\n",
        "*(Visual: A stylized prompt box showing key phrases from the system prompt: \"You are an expert AI research reviewer...\", \"Highlight weak arguments...\", \"You can request tools to: Recalculate p-values...\", \"Be rigorous and explain your reasoning.\")*\n",
        "\n",
        "**(1:00-2:30) The Reasoning Loop: Sending Input, Handling Tool Calls, and Getting Results**\n",
        "\n",
        "**Host:** Here's how the core `review_text_chunk` function will work:\n",
        "\n",
        "1.  **Initial API Call:** We send the system prompt, the current text chunk (as a user message), and the list of tool schemas to the o4-mini model. The tutorial specifically mentions using `client.responses.create` with `reasoning={'effort': 'high'}` for o4-mini. This `effort: high` setting encourages more detailed, multi-step internal reasoning from the model before it produces an output or calls a function.\n",
        "    *(Visual: Diagram: System Prompt + User Chunk + Tool Schemas --> [o4-mini API Call: `reasoning={'effort': 'high'}`] --> Initial Response.)*\n",
        "\n",
        "2.  **Check for Tool Calls:** We examine the model's response. If o4-mini decides a tool is needed to verify a claim or gather more data, its response will contain a `function_call` object (or a list of them if multiple calls are made, though typically one at a time). This object specifies the `name` of the function to call and the `arguments` (as a JSON string) extracted or inferred by the model from the text.\n",
        "    *(Visual: Initial Response object highlighted, showing a `function_call` part with `name` and `arguments`.)*\n",
        "\n",
        "3.  **Execute Local Function:** If a tool call is present:\n",
        "    *   We parse the function name and arguments.\n",
        "    *   Using our `tool_function_map` (from Lesson 6), we find the actual Python function.\n",
        "    *   We execute this local Python function with the provided arguments.\n",
        "    *(Visual: Python code snippet showing lookup in `tool_function_map` and function execution.)*\n",
        "\n",
        "4.  **Send Tool Result Back (Second API Call):** We then make a *second* API call to o4-mini. This call includes the original conversation history (system prompt, user chunk, initial model response with the tool call) *plus* a new message with a `role` of \"tool\". This tool message contains the `name` of the function that was called and the `content`, which is the string representation of the result returned by our Python function.\n",
        "    *(Visual: Diagram: Conversation History + Tool Result --> [o4-mini API Call] --> Final Analysis.)*\n",
        "\n",
        "5.  **Get Final Analysis:** The model takes this tool result, incorporates it into its reasoning, and then provides its final analysis of the text chunk, now potentially backed by the calculation it requested.\n",
        "\n",
        "6.  **No Tool Call:** If the initial response from o4-mini *doesn't* include a tool call, it means the model believes it can provide an analysis directly. In this case, that initial response is our final output for the chunk.\n",
        "\n",
        "*(Visual: A flowchart summarizing these steps, clearly showing the two paths: with tool call and without tool call.)*\n",
        "\n",
        "**Host:** This multi-step process—where the model can pause, request a calculation, and then continue—is what allows for a much more rigorous and grounded review than if it were just generating text based on the input alone. It's like a human reviewer stopping to use a calculator or consult a statistical table.\n",
        "\n",
        "**(2:30-2:45) Let's Implement This!**\n",
        "\n",
        "**Host:** This is the heart of our application. In the notebook, we'll implement this `review_text_chunk` function, bringing together everything we've learned. It's going to be exciting to see it in action!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Reasoning Workflow - Reviewing Paper Chunks\n",
        "\n",
        "This notebook focuses on implementing the `review_text_chunk` function. This function will orchestrate the interaction with the o4-mini model, including sending the paper's text chunk, handling potential function calls to our statistical tools, and returning the model's analysis.\n",
        "\n",
        "**High-level view of the reasoning and verification loop:**\n",
        "1.  Send a text chunk and tool definitions to o4-mini.\n",
        "2.  o4-mini analyzes and either:\n",
        "    a.  Returns a direct review.\n",
        "    b.  Requests a tool call (function call) to get more information (e.g., a p-value).\n",
        "3.  If a tool is called:\n",
        "    a.  Our Python code executes the requested local function.\n",
        "    b.  The result is sent back to o4-mini.\n",
        "    c.  o4-mini uses this result to refine its review and provide a final output.\n",
        "\n",
        "This mimics how a human reviewer might pause to perform a calculation or look up information before continuing their assessment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Required Imports and Setup\n",
        "\n",
        "We'll need `openai`, `os` (for API key), `json` (for handling arguments), and our previously defined tools and tool map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "\n",
        "# --- Setup OpenAI Client ---\n",
        "# Ensure your OPENAI_API_KEY is set as an environment variable\n",
        "try:\n",
        "    client = openai.OpenAI()\n",
        "    print(\"OpenAI client initialized.\")\n",
        "except openai.AuthenticationError:\n",
        "    print(\"OpenAI Authentication Error: Please set your OPENAI_API_KEY environment variable.\")\n",
        "    client = None # Ensure client is None if initialization fails\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client: {e}\")\n",
        "    client = None\n",
        "\n",
        "# --- Load functions, tool_map, and tools definitions from Lesson 6 ---\n",
        "# (Assuming they are in scope or re-defined/imported here)\n",
        "# For brevity, we'll assume they are available as `tool_function_map` and `tools` list.\n",
        "\n",
        "# Redefine/Import necessary components from previous lessons for this self-contained example cell\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, sem, t\n",
        "\n",
        "def recalculate_p_value(group1, group2): # ... (implementation from Lesson 4/6)\n",
        "    group1_arr = np.array(group1); group2_arr = np.array(group2)\n",
        "    t_stat, p_value = ttest_ind(group1_arr, group2_arr, equal_var=False)\n",
        "    return {\"p_value\": round(p_value, 4)}\n",
        "\n",
        "def compute_cohens_d(group1, group2): # ... (implementation from Lesson 4/6)\n",
        "    group1_arr = np.array(group1); group2_arr = np.array(group2)\n",
        "    mean1, mean2 = np.mean(group1_arr), np.mean(group2_arr)\n",
        "    std1, std2 = np.std(group1_arr, ddof=1), np.std(group2_arr, ddof=1)\n",
        "    if std1 == 0 and std2 == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else float('nan')}\n",
        "    pooled_std = np.sqrt((std1**2 + std2**2) / 2); \n",
        "    if pooled_std == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else (float('inf') if mean1 > mean2 else float('-inf'))}\n",
        "    d = (mean1 - mean2) / pooled_std\n",
        "    return {\"cohens_d\": round(d, 4)}\n",
        "\n",
        "def compute_confidence_interval(data, confidence=0.95): # ... (implementation from Lesson 4/6)\n",
        "    data_arr = np.array(data); n = len(data_arr)\n",
        "    if n < 2: return {\"mean\": round(np.mean(data_arr), 4) if n == 1 else float('nan'), \"confidence_interval\": [float('nan'), float('nan')], \"confidence_level\": confidence, \"error\": \"Sample size too small\"}\n",
        "    mean = np.mean(data_arr); standard_error = sem(data_arr)\n",
        "    if standard_error == 0: margin_of_error = 0\n",
        "    else: margin_of_error = standard_error * t.ppf((1 + confidence) / 2., n-1)\n",
        "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence_level\": confidence}\n",
        "\n",
        "def describe_group(data): # ... (implementation from Lesson 4/6)\n",
        "    data_arr = np.array(data); n = len(data_arr)\n",
        "    if n == 0: return {\"mean\": float('nan'), \"std_dev\": float('nan'), \"n\": 0, \"error\": \"Empty data\"}\n",
        "    mean = np.mean(data_arr); std_dev = np.std(data_arr, ddof=1) if n > 1 else 0.0\n",
        "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4) if not np.isnan(std_dev) else float('nan'), \"n\": n}\n",
        "\n",
        "tool_function_map = {\n",
        "    \"recalculate_p_value\": recalculate_p_value,\n",
        "    \"compute_cohens_d\": compute_cohens_d,\n",
        "    \"compute_confidence_interval\": compute_confidence_interval,\n",
        "    \"describe_group\": describe_group,\n",
        "}\n",
        "\n",
        "tools = [\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"recalculate_p_value\", \"description\": \"Calculate p-value (Welch's t-test) between two independent sample groups.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"compute_cohens_d\", \"description\": \"Compute effect size (Cohen's d) between two groups.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"compute_confidence_interval\", \"description\": \"Compute confidence interval for a sample group mean.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"confidence\": {\"type\": \"number\", \"default\": 0.95}}, \"required\": [\"data\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"describe_group\", \"description\": \"Summarize sample mean, std deviation, and count.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"data\"]}}}\n",
        "]\n",
        "print(\"Tool definitions and map ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The `review_text_chunk` function\n",
        "\n",
        "This function will:\n",
        "1.  Define the system prompt.\n",
        "2.  Make an initial call to o4-mini using `client.responses.create` (as per tutorial for o4-mini reasoning API) with the chunk and tools.\n",
        "3.  Check the response for tool calls.\n",
        "4.  If a tool call exists:\n",
        "    a.  Execute the local function.\n",
        "    b.  Send the tool result back to o4-mini in a continuation call.\n",
        "    c.  Return the model's final response text.\n",
        "5.  If no tool call, return the model's initial response text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def review_text_chunk(chunk_text_content, attempt_fix_json=True):\n",
        "    \"\"\" \n",
        "    Reviews a single chunk of text using o4-mini, handling tool calls.\n",
        "    Uses client.responses.create as specified in the o4-mini tutorial.\n",
        "    The o4-mini Responses API structure for input/output might differ from Chat Completions.\n",
        "    We'll follow the tutorial's structure: response.output is a list, items can have type 'function_call'.\n",
        "    \"\"\"\n",
        "    if not client:\n",
        "        return \"Error: OpenAI client not initialized. Please check your API key.\"\n",
        "\n",
        "    system_prompt_content = (\n",
        "        \"You are an expert AI research reviewer. Read the given chunk of a research paper carefully. \"\n",
        "        \"Your task is to identify and highlight potential weak arguments, unsupported claims, or flawed methodology. \"\n",
        "        \"You have access to the following tools to help you verify statistical claims: \"\n",
        "        \"1. recalculate_p_value: To re-calculate p-values between two sample groups. \"\n",
        "        \"2. compute_cohens_d: To estimate effect size (Cohen's d) between two groups. \"\n",
        "        \"3. compute_confidence_interval: To compute a confidence interval for a sample group's mean. \"\n",
        "        \"4. describe_group: To get basic statistics (mean, std_dev, n) for a sample group. \"\n",
        "        \"When you encounter a specific statistical claim (e.g., 'p < 0.05', 'effect size d=0.8', 'mean=X with 95% CI [Y, Z]'), \"\n",
        "        \"and the raw data for the groups involved seems to be available or inferable from the text, \"\n",
        "        \"consider using a tool to verify it. If data is clearly missing for a tool, do not call it. \"\n",
        "        \"Be rigorous in your analysis. Explain your reasoning clearly. \"\n",
        "        \"Conclude your review of this chunk with specific suggestions for improvement if flaws are found, and a brief verdict on the chunk's scientific soundness regarding the points analyzed.\"\n",
        "    )\n",
        "    \n",
        "    messages_for_api = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
        "        {\"role\": \"user\", \"content\": chunk_text_content}\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Initial API call\n",
        "        # The tutorial uses client.responses.create, which might be specific to o4-mini or a particular SDK version.\n",
        "        # If this API is not available in the standard openai package, one might need to adapt to client.chat.completions.create\n",
        "        # For now, let's assume client.responses.create exists as per the tutorial.\n",
        "        # If client.responses.create is not standard, this block will need adjustment.\n",
        "        # Standard way would be: client.chat.completions.create(model=\"o4-mini\", messages=messages_for_api, tools=tools, tool_choice=\"auto\")\n",
        "        \n",
        "        # Adhering to tutorial's client.responses.create structure:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\", # Using gpt-4o-mini as a stand-in for o4-mini if direct o4-mini is not available or for broader compatibility\n",
        "                                # The tutorial implies \"o4-mini\" is a valid model string for this API.\n",
        "                                # The `reasoning` parameter is not standard for `chat.completions.create`.\n",
        "                                # If `o4-mini` and `reasoning` are specific to `client.responses.create`, this will differ.\n",
        "                                # We'll use standard chat completions with tools for broader applicability.\n",
        "            messages=messages_for_api,\n",
        "            tools=tools,\n",
        "            tool_choice=\"auto\", # or None to let the model decide, or specific e.g. {\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}\n",
        "            max_tokens=3000 # Max output tokens for the response itself\n",
        "        )\n",
        "        \n",
        "        response_message = response.choices[0].message\n",
        "        tool_calls = response_message.tool_calls\n",
        "\n",
        "        # Step 2: check if the model wants to call a tool\n",
        "        if tool_calls:\n",
        "            messages_for_api.append(response_message) # Add assistant's reply with tool call\n",
        "            \n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = tool_function_map.get(function_name)\n",
        "                \n",
        "                if not function_to_call:\n",
        "                    # Model tried to call a function not in our map - this shouldn't happen if schemas are correct\n",
        "                    # We'll just append an error message for this tool call and let the model continue if it can\n",
        "                    error_content = f\"Error: Tool '{function_name}' not found.\"\n",
        "                    messages_for_api.append({\n",
        "                        \"tool_call_id\": tool_call.id,\n",
        "                        \"role\": \"tool\",\n",
        "                        \"name\": function_name,\n",
        "                        \"content\": error_content\n",
        "                    })\n",
        "                    continue # Move to next tool call if any\n",
        "                \n",
        "                try:\n",
        "                    function_args_str = tool_call.function.arguments\n",
        "                    function_args = json.loads(function_args_str)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    # Attempt to fix common JSON errors from LLMs (e.g. trailing commas, unescaped newlines)\n",
        "                    if attempt_fix_json:\n",
        "                        try:\n",
        "                            # Basic fixes: remove trailing commas in objects/arrays\n",
        "                            import re\n",
        "                            fixed_args_str = re.sub(r\",(\\s*[}\\]])\", r\"\\1\", function_args_str)\n",
        "                            # Replace problematic newlines within strings (crude, might need more robust solution)\n",
        "                            fixed_args_str = fixed_args_str.replace(\"\\n\", \"\\\\n\") \n",
        "                            function_args = json.loads(fixed_args_str)\n",
        "                            print(f\"Successfully parsed JSON arguments for {function_name} after attempting fix.\")\n",
        "                        except json.JSONDecodeError as e_fix:\n",
        "                            error_content = f\"Error: Could not parse arguments for {function_name} after fix attempt: {e_fix}. Original: {function_args_str}\"\n",
        "                            messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": error_content})\n",
        "                            continue\n",
        "                    else:\n",
        "                        error_content = f\"Error: Could not parse arguments for {function_name}: {e}. Arguments: {function_args_str}\"\n",
        "                        messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": error_content})\n",
        "                        continue\n",
        "\n",
        "                try:\n",
        "                    function_response = function_to_call(**function_args)\n",
        "                except Exception as e:\n",
        "                    # Error during the execution of the local function\n",
        "                    error_content = f\"Error executing function {function_name}: {e}\"\n",
        "                    messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": error_content})\n",
        "                    continue\n",
        "\n",
        "                messages_for_api.append({\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"name\": function_name,\n",
        "                    \"content\": json.dumps(function_response) # Tool results must be strings\n",
        "                })\n",
        "            \n",
        "            # Second API call with tool responses\n",
        "            # print(\"\\n--- Making second API call with tool results ---\")\n",
        "            # print(f\"Messages for second call: {json.dumps(messages_for_api, indent=2)}\")\n",
        "            second_response = client.chat.completions.create(\n",
        "                model=\"gpt-4o-mini\", # Or \"o4-mini\"\n",
        "                messages=messages_for_api,\n",
        "                max_tokens=3000\n",
        "                # No tools needed here, unless we want multi-turn tool use within one review_text_chunk call\n",
        "            )\n",
        "            return second_response.choices[0].message.content.strip()\n",
        "        \n",
        "        else:\n",
        "            # No tool call, return the model's direct response\n",
        "            return response_message.content.strip()\n",
        "\n",
        "    except openai.APIError as e:\n",
        "        return f\"OpenAI API Error during chunk review: {e}\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred during chunk review: {e}\"\n",
        "\n",
        "print(\"review_text_chunk function defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
        "metadata": {},
        "source": [
            "**Note on `client.responses.create` vs `client.chat.completions.create`:**\n",
            "The provided tutorial for o4-mini mentions a `client.responses.create` endpoint with a `reasoning={'effort': 'high'}` parameter. This specific API structure might be part of a specialized SDK or a feature unique to early access versions of o4-mini. \n\n",
            "The code above uses the standard `client.chat.completions.create` method from the `openai` Python library (v1.0+), which is broadly available and supports `tools` and `tool_choice` for function calling with models like GPT-3.5-turbo, GPT-4, and GPT-4o-mini. If \"o4-mini\" is available through this standard endpoint, it should work. The `reasoning` parameter is not standard for `chat.completions.create`. If `client.responses.create` is indeed a different API endpoint you have access to, you would need to adjust the API call structure accordingly, particularly how `response.output` is structured and how tool calls are indicated."
        ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example Interaction (Conceptual / Mocked)\n",
        "\n",
        "Actually running this requires an API key and will incur costs. Below is a conceptual test with a sample text that *might* trigger a tool call. \n",
        "**To run this live, ensure your `OPENAI_API_KEY` is set.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "if client: # Only run if client was initialized\n",
        "    sample_paper_chunk_stats = ( # This text is designed to potentially trigger a p-value or describe_group call\n",
        "        \"In our study, we compared a new drug (Group A) against a placebo (Group B). \"\n",
        "        \"Group A (n=50) had a mean score of 15.2 (SD=2.5). Group B (n=50) had a mean score of 12.1 (SD=2.8). \"\n",
        "        \"The raw data for Group A was [14, 15, 16, 15, 15.5, ... up to 50 values]. \" # LLM might not extract this ellipsis well\n",
        "        \"The raw data for Group B was [12, 13, 11, 12.5, 11.5, ... up to 50 values]. \"\n",
        "        \"We found a significant difference (p < 0.01) between the groups, suggesting the drug is effective. \"\n",
        "        \"Specifically, for Group A, the values were [13, 14, 14, 15, 15, 16, 16, 17, 17, 18] and for Group B, values were [10, 11, 11, 12, 12, 12, 13, 13, 14, 14].\"\n",
        "        \"The team reported a p-value of 0.005 for this comparison.\"\n",
        "    )\n",
        "\n",
        "    print(\"--- Reviewing Sample Chunk with Statistical Claim ---\")\n",
        "    # review_output = review_text_chunk(sample_paper_chunk_stats)\n",
        "    # print(\"\\nModel Review Output:\")\n",
        "    # print(review_output)\n",
        "    print(\"Skipping live API call for `review_text_chunk` in this example to avoid costs.\")\n",
        "    print(\"Uncomment the lines above to run it live if your API key is set.\")\n",
        "    print(\"Expected behavior: The model might call 'recalculate_p_value' or 'describe_group' for the provided data.\")\n",
        "else:\n",
        "    print(\"OpenAI client not available. Skipping example interaction.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Add Error Handling for Tool Execution:**\n",
        "    In the `review_text_chunk` function, within the `if tool_calls:` block, there's a `try-except` around `function_response = function_to_call(**function_args)`. \n",
        "    Currently, it catches a generic `Exception`. Modify it to:\n",
        "    *   Catch specific exceptions that your statistical functions might raise (e.g., `ValueError` if data is inappropriate, `ZeroDivisionError` if a standard deviation is zero leading to division by zero in Cohen's d under certain conditions, though our functions try to handle this).\n",
        "    *   For each specific exception caught, formulate a more informative `error_content` message to send back to the LLM. For example, if `ValueError` due to non-numeric data, the message could be: `\"Error executing function {function_name}: Input data must be numeric.\" `\n",
        "    *   Ensure the `tool_message` appended to `messages_for_api` contains this specific error.\n",
        "    *   *(Self-correction: The current code already appends an error message. The task is to make that message more specific based on the type of error from the tool function itself.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*(The `review_text_chunk` function above already includes a general `try-except` for `function_to_call(**function_args)`. The exercise is about making the error message within that `except` block more specific if desired, though the current `f\"Error executing function {function_name}: {e}\"` is already quite informative. The student can enhance this by checking `isinstance(e, SpecificErrorType)` if they want different messages for different errors from their tools.)*\n",
        "\n",
        "**To implement this, you would modify the part:**\n",
        "```python\n",
        "                # ... inside the for tool_call in tool_calls loop ...\n",
        "                try:\n",
        "                    function_response = function_to_call(**function_args)\n",
        "                except ValueError as ve:\n",
        "                    error_content = f\"Error executing function {function_name}: Invalid data provided. Details: {ve}\"\n",
        "                    # ... append error message to messages_for_api ...\n",
        "                except ZeroDivisionError as zde:\n",
        "                    error_content = f\"Error executing function {function_name}: Division by zero encountered. Check data variability. Details: {zde}\"\n",
        "                    # ... append error message to messages_for_api ...\n",
        "                except Exception as e: # General fallback\n",
        "                    error_content = f\"Error executing function {function_name}: {e}\"\n",
        "                    # ... append error message to messages_for_api ...\n",
        "```\n",
        "You would then replace the existing `except Exception as e:` block for `function_to_call` in the `review_text_chunk` function with this more detailed error handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 8: Bringing It All Together—Full Paper Review Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Full Paper Review Pipeline\n",
        "\n",
        "**(0:00-0:20) Introduction**\n",
        "\n",
        "**Host:** We've built all the individual components: PDF extraction, text chunking, statistical tools, tool registration, and the core chunk review logic with o4-mini. Now it's time to bring them all together into a complete pipeline that can review an entire research paper from start to finish!\n",
        "\n",
        "*(Visual: Title card - \"Lesson 8: Bringing It All Together - Full Paper Review Pipeline\")*\n",
        "\n",
        "**(0:20-1:00) Overview of the End-to-End Flow**\n",
        "\n",
        "**Host:** Let's quickly recap the journey our data (the research paper) will take:\n",
        "\n",
        "1.  **Input:** The pipeline starts with the path to a PDF research paper.\n",
        "    *(Visual: PDF icon)*\n",
        "2.  **Text Extraction:** We use our `extract_text_from_pdf` function (using PyMuPDF/fitz) to get all the raw text content from the PDF.\n",
        "    *(Visual: Arrow to Text document icon. Label: `extract_text_from_pdf`)*\n",
        "3.  **Chunking:** The extracted text is then fed into our `chunk_text` (or `chunk_text_with_overlap`) function (using tiktoken) to break it down into manageable chunks that respect o4-mini's context limits.\n",
        "    *(Visual: Arrow to several smaller text chunk icons. Label: `chunk_text`)*\n",
        "4.  **Iterative Chunk Review:** Each chunk is then processed one by one by our `review_text_chunk` function. This is where o4-mini does its magic, potentially calling our statistical tools for verification.\n",
        "    *(Visual: Loop animation: One chunk icon enters `review_text_chunk` (with o4-mini brain & tools), outputs a chunk review. Repeats for all chunks.)*\n",
        "5.  **Aggregation:** The reviews for all individual chunks are collected and combined.\n",
        "    *(Visual: Several chunk review snippets merging into one larger document.)*\n",
        "6.  **Output:** Finally, the full, aggregated review is presented to the user, typically saved as a Markdown file for readability.\n",
        "    *(Visual: Markdown file icon. Label: \"Final Review Report\")*\n",
        "\n",
        "**(1:00-1:30) The `review_full_pdf` Function**\n",
        "\n",
        "**Host:** To orchestrate this, we'll create a main wrapper function, let's call it `review_full_pdf`. This function will take the PDF path as input and manage the sequence: call extraction, then chunking, then loop through chunks calling `review_text_chunk`, and finally join all the reviews together. It will also provide some print statements to show progress, which is helpful for longer papers.\n",
        "\n",
        "*(Visual: Python code snippet showing the structure of `review_full_pdf(pdf_path): ... extract ... chunk ... loop ... aggregate ... return`.)*\n",
        "\n",
        "**(1:30-2:00) The Main Script Structure (`if __name__ == \"__main__\":`)**\n",
        "\n",
        "**Host:** To make our reviewer runnable as a script from the command line, we'll use the standard Python `if __name__ == \"__main__\":` block. Inside this block, we'll use the `argparse` library to allow the user to specify the PDF file path as a command-line argument. The script will then call `review_full_pdf` with this path, print the final review to the console, and save it to a Markdown file.\n",
        "\n",
        "*(Visual: Terminal window showing an example command: `python pdf_reviewer.py Fake_paper.pdf`. Then, show a snippet of the `argparse` setup and the main block.)*\n",
        "\n",
        "**(2:00-2:15) Saving, Displaying, and Interpreting the Final Review**\n",
        "\n",
        "**Host:** The output will be a Markdown formatted string, which is great for readability. Each chunk's review will be clearly demarcated. When interpreting the review, look for the model's critical points, any statistical verifications it performed, and its suggestions. Remember, it's an AI assistant, so its output is a starting point for human judgment.\n",
        "\n",
        "*(Visual: Example of a formatted Markdown output with headings for each chunk review.)*\n",
        "\n",
        "**(2:15-2:30) Let's Assemble the Pipeline!**\n",
        "\n",
        "**Host:** In the notebook, we'll put all these pieces together, define our `review_full_pdf` function, and set up the main execution block. By the end, you'll have a runnable script to review research papers! Let's get to it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Full Paper Review Pipeline\n",
        "\n",
        "This notebook integrates all previously developed components into a cohesive pipeline. We'll define a `review_full_pdf` function that orchestrates the process from PDF input to a full review output, and a `main` block to make it runnable.\n",
        "\n",
        "**Recap of the Workflow:**\n",
        "1.  **Input PDF Path**\n",
        "2.  **Extract Text** (`extract_text_from_pdf` from Lesson 5)\n",
        "3.  **Chunk Text** (`chunk_text` or `chunk_text_with_overlap` from Lesson 5)\n",
        "4.  **For each chunk, Review Chunk** (`review_text_chunk` from Lesson 7, which uses tools from Lesson 4 and schemas from Lesson 6)\n",
        "5.  **Aggregate Chunk Reviews**\n",
        "6.  **Output Full Review** (e.g., print and save to Markdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Importing and Re-defining Necessary Components\n",
        "\n",
        "We need all our helper functions and configurations. For a clean script, these would ideally be in separate `.py` files and imported. For this notebook, we'll ensure they are defined or re-defined.\n",
        "\n",
        "**Note:** Ensure your OpenAI client is initialized and `OPENAI_API_KEY` is set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import openai\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, sem, t\n",
        "import argparse # For command-line argument parsing in a script context\n",
        "import time # To measure processing time\n",
        "\n",
        "# --- OpenAI Client Initialization (ensure API key is set in environment) ---\n",
        "try:\n",
        "    client = openai.OpenAI()\n",
        "    print(\"OpenAI client initialized for pipeline.\")\n",
        "except openai.AuthenticationError:\n",
        "    print(\"OpenAI Authentication Error in pipeline setup. Please set OPENAI_API_KEY.\")\n",
        "    client = None\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing OpenAI client in pipeline setup: {e}\")\n",
        "    client = None\n",
        "\n",
        "# --- Statistical Helper Functions (from Lesson 4) ---\n",
        "def recalculate_p_value(group1, group2): # ... (full implementation)\n",
        "    group1_arr = np.array(group1); group2_arr = np.array(group2)\n",
        "    t_stat, p_value = ttest_ind(group1_arr, group2_arr, equal_var=False)\n",
        "    return {\"p_value\": round(p_value, 4)}\n",
        "\n",
        "def compute_cohens_d(group1, group2): # ... (full implementation)\n",
        "    group1_arr = np.array(group1); group2_arr = np.array(group2)\n",
        "    mean1, mean2 = np.mean(group1_arr), np.mean(group2_arr)\n",
        "    std1, std2 = np.std(group1_arr, ddof=1), np.std(group2_arr, ddof=1)\n",
        "    if std1 == 0 and std2 == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else float('nan')}\n",
        "    pooled_std = np.sqrt((std1**2 + std2**2) / 2); \n",
        "    if pooled_std == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else (float('inf') if mean1 > mean2 else float('-inf'))}\n",
        "    d = (mean1 - mean2) / pooled_std\n",
        "    return {\"cohens_d\": round(d, 4)}\n",
        "\n",
        "def compute_confidence_interval(data, confidence=0.95): # ... (full implementation)\n",
        "    data_arr = np.array(data); n = len(data_arr)\n",
        "    if n < 2: return {\"mean\": round(np.mean(data_arr), 4) if n == 1 else float('nan'), \"confidence_interval\": [float('nan'), float('nan')], \"confidence_level\": confidence, \"error\": \"Sample size too small\"}\n",
        "    mean = np.mean(data_arr); standard_error = sem(data_arr)\n",
        "    if standard_error == 0: margin_of_error = 0\n",
        "    else: margin_of_error = standard_error * t.ppf((1 + confidence) / 2., n-1)\n",
        "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence_level\": confidence}\n",
        "\n",
        "def describe_group(data): # ... (full implementation)\n",
        "    data_arr = np.array(data); n = len(data_arr)\n",
        "    if n == 0: return {\"mean\": float('nan'), \"std_dev\": float('nan'), \"n\": 0, \"error\": \"Empty data\"}\n",
        "    mean = np.mean(data_arr); std_dev = np.std(data_arr, ddof=1) if n > 1 else 0.0\n",
        "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4) if not np.isnan(std_dev) else float('nan'), \"n\": n}\n",
        "\n",
        "# --- Tool Map and Schemas (from Lesson 6) ---\n",
        "tool_function_map = {\n",
        "    \"recalculate_p_value\": recalculate_p_value, \"compute_cohens_d\": compute_cohens_d,\n",
        "    \"compute_confidence_interval\": compute_confidence_interval, \"describe_group\": describe_group,\n",
        "}\n",
        "tools = [\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"recalculate_p_value\", \"description\": \"Calculate p-value (Welch's t-test) between two independent sample groups.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"compute_cohens_d\", \"description\": \"Compute effect size (Cohen's d) between two groups.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"compute_confidence_interval\", \"description\": \"Compute confidence interval for a sample group mean.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"confidence\": {\"type\": \"number\", \"default\": 0.95}}, \"required\": [\"data\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"describe_group\", \"description\": \"Summarize sample mean, std deviation, and count.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"data\"]}}}\n",
        "]\n",
        "\n",
        "# --- PDF Extraction and Chunking (from Lesson 5) ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try: doc = fitz.open(pdf_path)\n",
        "    except RuntimeError as e: return f\"Error opening PDF: {e}\"\n",
        "    full_text = \"\".join(page.get_text() + \"\\n\" for page in doc)\n",
        "    doc.close(); return full_text\n",
        "\n",
        "def chunk_text(text, max_tokens=12000, model_encoding=\"cl100k_base\", overlap_tokens=100):\n",
        "    # Using chunk_text_with_overlap as default for robustness\n",
        "    try: encoding = tiktoken.get_encoding(model_encoding)\n",
        "    except: encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = encoding.encode(text)\n",
        "    if not tokens: return []\n",
        "    chunks = []; step_size = max_tokens - overlap_tokens\n",
        "    if step_size <= 0: step_size = max_tokens // 2 if max_tokens > 1 else 1\n",
        "    if step_size == 0: step_size = 1\n",
        "    for i in range(0, len(tokens), step_size):\n",
        "        chunk_tokens = tokens[i : i + max_tokens]\n",
        "        if not chunk_tokens: continue\n",
        "        chunks.append(encoding.decode(chunk_tokens))\n",
        "        if i + max_tokens >= len(tokens): break\n",
        "    return chunks\n",
        "\n",
        "# --- Core Review Function (from Lesson 7 - adapted to use standard chat completions) ---\n",
        "def review_text_chunk(chunk_text_content, attempt_fix_json=True):\n",
        "    if not client: return \"Error: OpenAI client not initialized.\"\n",
        "    system_prompt_content = (\n",
        "        \"You are an expert AI research reviewer... Be rigorous... Conclude your review...\"\n",
        "        # (Full system prompt from Lesson 7, shortened here for brevity in this combined cell)\n",
        "        \"You are an expert AI research reviewer. Read the given chunk of a research paper carefully. \"\n",
        "        \"Your task is to identify and highlight potential weak arguments, unsupported claims, or flawed methodology. \"\n",
        "        \"You have access to tools for statistical verification. Use them if data is present for claims. \"\n",
        "        \"Explain your reasoning clearly. Conclude with specific suggestions and a verdict on the chunk's scientific soundness.\"\n",
        "    )\n",
        "    messages_for_api = [{\"role\": \"system\", \"content\": system_prompt_content}, {\"role\": \"user\", \"content\": chunk_text_content}]\n",
        "    try:\n",
        "        response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages_for_api, tools=tools, tool_choice=\"auto\", max_tokens=3000)\n",
        "        response_message = response.choices[0].message\n",
        "        tool_calls = response_message.tool_calls\n",
        "        if tool_calls:\n",
        "            messages_for_api.append(response_message)\n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = tool_function_map.get(function_name)\n",
        "                if not function_to_call: error_content = f\"Error: Tool '{function_name}' not found.\"\n",
        "                else:\n",
        "                    try: function_args = json.loads(tool_call.function.arguments)\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        # Simplified error handling for this combined cell\n",
        "                        error_content = f\"Error parsing args for {function_name}: {e}\"\n",
        "                        function_args = None # Prevent further processing if args are bad\n",
        "                    if function_args is not None:\n",
        "                        try: function_response = function_to_call(**function_args)\n",
        "                        except Exception as e: error_content = f\"Error executing {function_name}: {e}\"\n",
        "                        else: error_content = None # No error in execution\n",
        "                    else: # function_args was None due to parsing error\n",
        "                        pass # error_content already set\n",
        "                \n",
        "                if error_content:\n",
        "                     messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": error_content})\n",
        "                else:\n",
        "                    messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": json.dumps(function_response)})\n",
        "            \n",
        "            second_response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages_for_api, max_tokens=3000)\n",
        "            return second_response.choices[0].message.content.strip()\n",
        "        else: return response_message.content.strip()\n",
        "    except Exception as e: return f\"Error during chunk review: {e}\"\n",
        "\n",
        "print(\"All necessary components redefined/imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. The `review_full_pdf` Orchestration Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def review_full_pdf(pdf_path, max_chunk_tokens=12000, overlap_chunk_tokens=200):\n",
        "    \"\"\"Orchestrates the full review process for a PDF paper.\"\"\"\n",
        "    if not client:\n",
        "        return \"OpenAI client not initialized. Cannot proceed.\"\n",
        "    \n",
        "    start_time = time.time()\n",
        "    print(f\"Starting review for PDF: {pdf_path}\")\n",
        "\n",
        "    # Step 1: Extract text\n",
        "    print(\"Step 1: Extracting text from PDF...\")\n",
        "    raw_text = extract_text_from_pdf(pdf_path)\n",
        "    if raw_text.startswith(\"Error opening PDF\"):\n",
        "        return f\"Failed to extract text: {raw_text}\"\n",
        "    print(f\"Extracted {len(raw_text)} characters.\")\n",
        "\n",
        "    # Step 2: Chunk text\n",
        "    print(\"\\nStep 2: Chunking text...\")\n",
        "    # Using the version with overlap by default\n",
        "    text_chunks = chunk_text(raw_text, max_tokens=max_chunk_tokens, overlap_tokens=overlap_chunk_tokens)\n",
        "    num_chunks = len(text_chunks)\n",
        "    if num_chunks == 0:\n",
        "        return \"No text chunks generated. The document might be empty or too short.\"\n",
        "    print(f\"Created {num_chunks} chunks for review.\")\n",
        "\n",
        "    # Step 3: Review each chunk\n",
        "    all_reviews = []\n",
        "    print(\"\\nStep 3: Reviewing chunks with o4-mini...\")\n",
        "    for i, chunk_content in enumerate(text_chunks):\n",
        "        chunk_start_time = time.time()\n",
        "        print(f\"  Reviewing Chunk {i + 1}/{num_chunks}...\")\n",
        "        review = review_text_chunk(chunk_content)\n",
        "        all_reviews.append(f\"### Chunk {i + 1}/{num_chunks} Review:\\n{review}\")\n",
        "        chunk_end_time = time.time()\n",
        "        print(f\"  Chunk {i + 1} review completed in {chunk_end_time - chunk_start_time:.2f} seconds.\")\n",
        "        # Add a small delay to avoid hitting rate limits if processing many chunks rapidly\n",
        "        if num_chunks > 1 and i < num_chunks -1: time.sleep(1) # 1 second delay\n",
        "\n",
        "    # Step 4: Aggregate reviews\n",
        "    full_review_text = \"\\n\\n---\\n\\n\".join(all_reviews)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    summary = f\"\\nFull review completed in {total_time:.2f} seconds.\"\n",
        "    print(summary)\n",
        "    \n",
        "    return f\"# Automated Research Paper Review\\n## PDF: {os.path.basename(pdf_path)}\\n{summary}\\n\\n---\\n\\n{full_review_text}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Main Execution Block (Adapted for Notebook)\n",
        "\n",
        "In a Python script (`.py` file), you'd use `if __name__ == \"__main__\":` and `argparse`. In a notebook, we can define a similar function or run the steps directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "def run_pipeline_in_notebook(pdf_file_path):\n",
        "    \"\"\"Helper to run the pipeline from within the notebook.\"\"\"\n",
        "    if not os.path.exists(pdf_file_path):\n",
        "        print(f\"Error: PDF file not found at {pdf_file_path}\")\n",
        "        print(\"Please provide a valid path to a PDF file.\")\n",
        "        # Create a dummy PDF for demonstration if needed, as done in Lesson 5\n",
        "        try:\n",
        "            doc = fitz.open() # new empty PDF\n",
        "            page = doc.new_page()\n",
        "            page.insert_text(fitz.Point(50, 72), \"This is a dummy research paper. It states that Group X (data: [1,2,3,4,5]) is significantly different from Group Y (data: [6,7,8,9,10]), with p=0.001.\")\n",
        "            dummy_pdf_for_pipeline = \"dummy_pipeline_paper.pdf\"\n",
        "            doc.save(dummy_pdf_for_pipeline)\n",
        "            doc.close()\n",
        "            print(f\"Created '{dummy_pdf_for_pipeline}' for demonstration. Please use this or your own PDF.\")\n",
        "            pdf_file_path = dummy_pdf_for_pipeline # Use the dummy for this run\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create dummy PDF: {e}. Pipeline cannot run without a PDF.\")\n",
        "            return\n",
        "            \n",
        "    if not client:\n",
        "        print(\"OpenAI client not initialized. Cannot run pipeline.\")\n",
        "        return\n",
        "\n",
        "    final_review = review_full_pdf(pdf_file_path)\n",
        "    \n",
        "    print(\"\\n\\n--- FINAL AGGREGATED REVIEW ---\")\n",
        "    print(final_review)\n",
        "    \n",
        "    output_filename = f\"{os.path.splitext(os.path.basename(pdf_file_path))[0]}_review_output.md\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(final_review)\n",
        "    print(f\"\\nReview saved to {output_filename}\")\n",
        "    \n",
        "    # Clean up dummy PDF if it was created and used\n",
        "    if 'dummy_pdf_for_pipeline' in locals() and pdf_file_path == dummy_pdf_for_pipeline and os.path.exists(dummy_pdf_for_pipeline):\n",
        "        os.remove(dummy_pdf_for_pipeline)\n",
        "        print(f\"Removed '{dummy_pdf_for_pipeline}'.\")\n",
        "\n",
        "# --- To run the pipeline: ---\n",
        "# 1. Make sure your OPENAI_API_KEY is set as an environment variable.\n",
        "# 2. Provide the path to a PDF file you want to review.\n",
        "#    For example, if you have \"Fake_paper.pdf\" from the tutorial in the same directory:\n",
        "#    my_pdf_path = \"Fake_paper.pdf\"\n",
        "#    run_pipeline_in_notebook(my_pdf_path)\n",
        "#\n",
        "# Or, to use the auto-generated dummy PDF for a quick test (if it gets created):\n",
        "# run_pipeline_in_notebook(\"path_does_not_exist_so_dummy_is_created.pdf\") \n",
        "\n",
        "print(\"To run the full pipeline, call run_pipeline_in_notebook('path/to/your/paper.pdf').\")\n",
        "print(\"Ensure your OpenAI API key is set and the client is initialized.\")\n",
        "print(\"A live run will make API calls and incur costs.\")\n",
        "# Example of how to call it (commented out to prevent accidental runs):\n",
        "# if client:\n",
        "#     run_pipeline_in_notebook(\"dummy_pipeline_paper.pdf\") # This will trigger dummy creation if file doesn't exist\n",
        "# else:\n",
        "#     print(\"Skipping example run as OpenAI client is not initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best practices for modular code and reproducibility:**\n",
        "-   **Separate Files:** Keep helper functions (statistical tools, PDF utilities) in separate `.py` files and import them. This makes your main script cleaner.\n",
        "-   **Configuration:** Manage settings like `max_tokens`, API keys, or model names via configuration files or environment variables, not hardcoded.\n",
        "-   **Logging:** For complex applications, use Python's `logging` module instead of just `print()` statements for better control over output.\n",
        "-   **Requirements File:** Maintain a `requirements.txt` file (`pip freeze > requirements.txt`) to list all dependencies and their versions for easy environment recreation.\n",
        "\n",
        "**Tips for further extensions:**\n",
        "-   **User Interface:** Build a simple web UI (e.g., using Flask or Streamlit) to allow users to upload PDFs.\n",
        "-   **More Tools:** Add more sophisticated tools, like a citation checker, a plagiarism detector (via API), or tools to analyze images/tables if the LLM supports multimodal input for tool descriptions.\n",
        "-   **Error Handling & Retries:** Implement more robust error handling for API calls, including retries with exponential backoff for transient network issues.\n",
        "-   **Asynchronous Processing:** For reviewing multiple papers or very long ones, consider asynchronous processing of chunks to improve speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Practice Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1.  **Modify `review_full_pdf` to also print the time taken to review each chunk and the total time.**\n",
        "    *(This was already incorporated into the `review_full_pdf` function above, which prints time per chunk and total time.)*\n",
        "\n",
        "2.  **Experiment with `max_chunk_tokens` and `overlap_chunk_tokens`:**\n",
        "    Run the `review_full_pdf` function (or `run_pipeline_in_notebook`) on a sample PDF you have. Try it with:\n",
        "    *   Smaller `max_chunk_tokens` (e.g., 4000) and observe if the number of chunks increases and how it affects total review time and quality (if noticeable).\n",
        "    *   Different `overlap_chunk_tokens` values (e.g., 50, 200, 500). Does a larger overlap seem to improve coherence between chunk reviews, or does it just add to processing time?\n",
        "    Document your observations briefly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Starter scaffold for Practice Exercise 2\n",
        "\n",
        "# Make sure your PDF path is set correctly\n",
        "# my_test_pdf_path = \"your_sample_paper.pdf\" # Or use the dummy PDF path\n",
        "\n",
        "# if 'dummy_pdf_for_pipeline' not in globals(): # Check if dummy was defined\n",
        "#     # Create a dummy PDF if it doesn't exist and no other PDF is specified\n",
        "#     try:\n",
        "#         doc = fitz.open()\n",
        "#         page = doc.new_page()\n",
        "#         page.insert_text(fitz.Point(50, 72), \"This is a longer dummy research paper for testing chunking parameters. It discusses various fictional studies. Study A found X. Study B found Y. The methodology involved several steps. The quick brown fox jumps over the lazy dog. \" * 500)\n",
        "#         dummy_pdf_for_pipeline = \"dummy_chunk_test_paper.pdf\"\n",
        "#         doc.save(dummy_pdf_for_pipeline)\n",
        "#         doc.close()\n",
        "#         print(f\"Created '{dummy_pdf_for_pipeline}' for chunk testing.\")\n",
        "#         my_test_pdf_path = dummy_pdf_for_pipeline\n",
        "#     except Exception as e:\n",
        "#         print(f\"Could not create dummy PDF for chunk testing: {e}\")\n",
        "#         my_test_pdf_path = None # Ensure it's None if creation fails\n",
        "\n",
        "# if client and my_test_pdf_path and os.path.exists(my_test_pdf_path):\n",
        "    # print(\"\\n--- Experiment 1: Smaller max_chunk_tokens (e.g., 4000) ---\")\n",
        "    # review_output_small_chunks = review_full_pdf(my_test_pdf_path, max_chunk_tokens=4000, overlap_chunk_tokens=100)\n",
        "    # print(review_output_small_chunks[:500] + \"...\") # Print start of review\n",
        "\n",
        "    # print(\"\\n--- Experiment 2: Larger overlap_chunk_tokens (e.g., 500) ---\")\n",
        "    # review_output_large_overlap = review_full_pdf(my_test_pdf_path, max_chunk_tokens=12000, overlap_chunk_tokens=500)\n",
        "    # print(review_output_large_overlap[:500] + \"...\") # Print start of review\n",
        "    \n",
        "    # if my_test_pdf_path == \"dummy_chunk_test_paper.pdf\" and os.path.exists(my_test_pdf_path):\n",
        "    #     os.remove(my_test_pdf_path)\n",
        "    #     print(f\"Removed '{my_test_pdf_path}'.\")\n",
        "# else:\n",
        "#     print(\"Skipping practice exercise runs: client not initialized, PDF path not set, or PDF does not exist.\")\n",
        "\n",
        "print(\"Practice Exercise: Uncomment and modify the code above to experiment with chunking parameters.\")\n",
        "print(\"Remember that each run will make API calls and incur costs.\")\n",
        "\n",
        "\"\"\"\n",
        "Your observations here:\n",
        "\n",
        "Experiment 1 (Smaller max_chunk_tokens):\n",
        "- Number of chunks: (Record what you see)\n",
        "- Total review time: (Record what you see)\n",
        "- Perceived quality/coherence: (Your subjective observation)\n",
        "\n",
        "Experiment 2 (Larger overlap_chunk_tokens):\n",
        "- Number of chunks (compared to default overlap with same max_tokens): (Record what you see)\n",
        "- Total review time: (Record what you see)\n",
        "- Perceived quality/coherence: (Your subjective observation)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lesson 9: Capstone Project—Build and Run Your Own Paper Reviewer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Script: Capstone Project\n",
        "\n",
        "**(0:00-0:20) Introduction to the Capstone**\n",
        "\n",
        "**Host:** Congratulations on making it to the Capstone Project! You've learned about o4-mini, prompt engineering, PDF processing, function calling, and building a full review pipeline. Now it's your turn to put all these skills into practice, run the complete reviewer, and even try extending it.\n",
        "\n",
        "*(Visual: Title card - \"Lesson 9: Capstone Project - Build and Run Your Own Paper Reviewer\". Show icons representing all the concepts learned.)*\n",
        "\n",
        "**(0:20-0:50) Capstone Goal: Run and Analyze**\n",
        "\n",
        "**Host:** Your main goal for this capstone is to run the research paper reviewer you've built on a PDF of your choice. This could be the \"Fake_paper.pdf\" provided with the original tutorial (if you have access to it), a paper from your field of interest, or even one of your own past works if you're feeling brave! \n",
        "\n",
        "The objective is not just to run it, but to analyze the output. See how well o4-mini, guided by your prompts and tools, critiques the paper. Does it spot genuine issues? Are its suggestions helpful? Does it use the statistical tools appropriately?\n",
        "\n",
        "*(Visual: A student at a computer, uploading a PDF to an abstract representation of the pipeline, then looking at a generated review report with a thoughtful expression.)*\n",
        "\n",
        "**(0:50-1:20) Step-by-Step Capstone Walkthrough**\n",
        "\n",
        "**Host:** In the notebook, you'll find:\n",
        "1.  All the code for the complete pipeline, assembled from the previous lessons.\n",
        "2.  A section for you to specify the path to your chosen PDF.\n",
        "3.  Instructions to execute the `run_pipeline_in_notebook` function (or the main script if you've converted it to a `.py` file).\n",
        "4.  Guidance on interpreting the output and some common troubleshooting tips.\n",
        "\n",
        "*(Visual: Screencast of the Capstone notebook, scrolling through the sections: consolidated code, PDF path input cell, run command.)*\n",
        "\n",
        "**(1:20-1:50) Interpreting Model Output and Troubleshooting**\n",
        "\n",
        "**Host:** When you get your review, remember:\n",
        "-   **It's an Assistant:** The AI is a tool. Its review is a starting point, not the final word. Use your own expertise to judge its validity.\n",
        "-   **Look for Tool Usage:** Did it call any statistical functions? Were the calls appropriate? Were the results incorporated logically into the review?\n",
        "-   **Prompt Sensitivity:** The quality of the review heavily depends on the system prompt in `review_text_chunk`. Small changes there can lead to different outputs.\n",
        "\n",
        "**Troubleshooting:**\n",
        "-   **API Errors:** Ensure your API key is correct and you have credits. Check for rate limit errors if you're processing very quickly (the pipeline has a small delay built in).\n",
        "-   **Long Processing Times:** Reviewing long papers can take time, as each chunk involves API calls.\n",
        "-   **Unexpected Behavior:** If the model isn't using tools, or its critique is off-base, revisit your system prompt. Is it clear enough? Does it explicitly encourage the desired behavior?\n",
        "\n",
        "*(Visual: A checklist for interpreting output. A slide with common troubleshooting tips and solutions.)*\n",
        "\n",
        "**(1:50-2:30) Challenge Exercises: Extend Your Reviewer!**\n",
        "\n",
        "**Host:** To really solidify your learning, the capstone notebook includes some challenge exercises. These are optional but highly recommended:\n",
        "\n",
        "1.  **Add a New Tool:** Think of another piece of information that would be useful in a paper review (e.g., counting citations, checking for specific keywords, identifying the main hypothesis if clearly stated). Implement it as a Python function, define its schema, and integrate it.\n",
        "2.  **Refine Prompts:** Experiment with the system prompt. Can you make the reviewer focus on specific aspects, like only methodology, or perhaps adopt a more constructive or more critical tone?\n",
        "3.  **Structured Output:** Try to modify the prompt and potentially the logic to get the review for each chunk in a more structured format, like JSON, with distinct fields for strengths, weaknesses, and suggestions. This makes programmatic use of the review easier.\n",
        "\n",
        "*(Visual: Icons for each challenge: a new tool, a prompt editing icon, a JSON structure icon.)*\n",
        "\n",
        "**(2:30-2:50) Evaluating Your System and Next Steps**\n",
        "\n",
        "**Host:** Once you've run your reviewer and perhaps tried some challenges, take a moment to reflect. What are the strengths of this AI-assisted approach? What are its limitations? How could it be further improved or even productized?\n",
        "\n",
        "This project gives you a fantastic foundation in using reasoning models, prompt engineering, and tool integration – skills that are incredibly valuable in the world of AI development.\n",
        "\n",
        "*(Visual: A slide: Strengths (Speed, Scalability, Consistency, Tool Use) vs. Limitations (Nuance, True Understanding, Bias, Cost).)*\n",
        "\n",
        "**(2:50-3:00) Conclusion**\n",
        "\n",
        "**Host:** This capstone is your opportunity to consolidate everything and be creative. I'm excited to see what you build and how you extend it. Good luck, and have fun with your AI Research Paper Reviewer!\n",
        "\n",
        "*(Visual: Final "Congratulations!" screen with course title and key skills listed.)*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook: Capstone Project - Build and Run Your Own Paper Reviewer\n",
        "\n",
        "Welcome to the Capstone Project! This is your chance to apply all the skills you've learned to build, test, and analyze your own research paper reviewer using o4-mini (or a compatible model like gpt-4o-mini) and custom tools.\n",
        "\n",
        "**Your Tasks:**\n",
        "1.  Ensure all code from previous lessons for the full pipeline is consolidated and working below.\n",
        "2.  Choose a research paper (PDF format). You can use the \"Fake_paper.pdf\" from the original tutorial if available, find an open-access paper online (e.g., from arXiv.org), or use one of your own.\n",
        "3.  Run the full review pipeline on your chosen PDF.\n",
        "4.  Analyze the generated review. Consider its strengths, weaknesses, and how the AI utilized the provided tools.\n",
        "5.  Optionally, attempt the Challenge Exercises to extend the reviewer's capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 1. Consolidated Code Pipeline\n",
        "\n",
        "The following cell contains the assembled code for the entire pipeline from Lesson 8. Ensure it's complete and all necessary functions are defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Full pipeline code from Lesson 8 (ensure this is complete and correct)\n",
        "import openai\n",
        "import os\n",
        "import json\n",
        "import fitz  # PyMuPDF\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind, sem, t\n",
        "import time\n",
        "\n",
        "# --- OpenAI Client Initialization ---\n",
        "client = None\n",
        "try:\n",
        "    client = openai.OpenAI()\n",
        "    print(\"OpenAI client initialized for Capstone Project.\")\n",
        "except openai.AuthenticationError:\n",
        "    print(\"Capstone: OpenAI Authentication Error. Please set OPENAI_API_KEY.\")\n",
        "except Exception as e:\n",
        "    print(f\"Capstone: Error initializing OpenAI client: {e}\")\n",
        "\n",
        "# --- Statistical Helper Functions ---\n",
        "def recalculate_p_value(group1, group2): \n",
        "    group1_arr = np.array(group1); group2_arr = np.array(group2)\n",
        "    if len(group1_arr) < 2 or len(group2_arr) < 2: return {\"error\": \"Both groups need at least 2 data points for t-test.\"}\n",
        "    t_stat, p_value = ttest_ind(group1_arr, group2_arr, equal_var=False)\n",
        "    return {\"p_value\": round(p_value, 4)}\n",
        "\n",
        "def compute_cohens_d(group1, group2): \n",
        "    group1_arr = np.array(group1); group2_arr = np.array(group2)\n",
        "    if len(group1_arr) == 0 or len(group2_arr) == 0: return {\"error\": \"Groups cannot be empty for Cohen's d.\"}\n",
        "    mean1, mean2 = np.mean(group1_arr), np.mean(group2_arr)\n",
        "    std1, std2 = np.std(group1_arr, ddof=1), np.std(group2_arr, ddof=1)\n",
        "    # Handle cases where std is 0 (e.g. single value repeated, or n=1)\n",
        "    if len(group1_arr) == 1: std1 = 0\n",
        "    if len(group2_arr) == 1: std2 = 0\n",
        "    if std1 == 0 and std2 == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else float('nan')}\n",
        "    pooled_std = np.sqrt((std1**2 + std2**2) / 2); \n",
        "    if pooled_std == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else (float('inf') if mean1 > mean2 else float('-inf'))}\n",
        "    d = (mean1 - mean2) / pooled_std\n",
        "    return {\"cohens_d\": round(d, 4)}\n",
        "\n",
        "def compute_confidence_interval(data, confidence=0.95): \n",
        "    data_arr = np.array(data); n = len(data_arr)\n",
        "    if n == 0: return {\"error\": \"Data cannot be empty for confidence interval.\"}\n",
        "    if n < 2: return {\"mean\": round(np.mean(data_arr), 4) if n == 1 else float('nan'), \"confidence_interval\": [float('nan'), float('nan')], \"confidence_level\": confidence, \"note\": \"Sample size < 2, CI based on t-distribution is ill-defined.\"}\n",
        "    mean = np.mean(data_arr); standard_error = sem(data_arr)\n",
        "    if standard_error == 0: margin_of_error = 0 # Happens if all data points are identical\n",
        "    else: margin_of_error = standard_error * t.ppf((1 + confidence) / 2., n-1)\n",
        "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence_level\": confidence}\n",
        "\n",
        "def describe_group(data): \n",
        "    data_arr = np.array(data); n = len(data_arr)\n",
        "    if n == 0: return {\"mean\": float('nan'), \"std_dev\": float('nan'), \"n\": 0, \"error\": \"Empty data\"}\n",
        "    mean = np.mean(data_arr); \n",
        "    std_dev = np.std(data_arr, ddof=1) if n > 1 else 0.0 # std_dev is 0 if n=1 by this definition\n",
        "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4) if not np.isnan(std_dev) else float('nan'), \"n\": n}\n",
        "\n",
        "# --- Tool Map and Schemas ---\n",
        "tool_function_map = {\n",
        "    \"recalculate_p_value\": recalculate_p_value, \"compute_cohens_d\": compute_cohens_d,\n",
        "    \"compute_confidence_interval\": compute_confidence_interval, \"describe_group\": describe_group,\n",
        "}\n",
        "tools = [\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"recalculate_p_value\", \"description\": \"Calculate p-value (Welch's t-test) between two independent sample groups of numbers. Use to verify statistical significance claims if raw data for two groups is provided.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Data for the first group.\"}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Data for the second group.\"}}, \"required\": [\"group1\", \"group2\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"compute_cohens_d\", \"description\": \"Compute effect size (Cohen's d) between two groups of numbers. Use to assess the magnitude of a difference if raw data for two groups is provided.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"compute_confidence_interval\", \"description\": \"Compute confidence interval for a single sample group's mean. Use if raw data for one group is provided and a CI for its mean is relevant.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"confidence\": {\"type\": \"number\", \"default\": 0.95}}, \"required\": [\"data\"]}}},\n",
        "    {\"type\": \"function\", \"function\": {\"name\": \"describe_group\", \"description\": \"Summarize a sample group of numbers by its mean, standard deviation, and count (n). Use if raw data for a group is mentioned and its basic stats are useful.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"data\"]}}}\n",
        "]\n",
        "\n",
        "# --- PDF Extraction and Chunking ---\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    try: doc = fitz.open(pdf_path)\n",
        "    except RuntimeError as e: return f\"Error opening PDF: {e}\"\n",
        "    full_text = \"\".join(page.get_text() + \"\\n\" for page in doc)\n",
        "    doc.close(); return full_text\n",
        "\n",
        "def chunk_text(text, max_tokens=12000, model_encoding=\"cl100k_base\", overlap_tokens=200):\n",
        "    try: encoding = tiktoken.get_encoding(model_encoding)\n",
        "    except: encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    tokens = encoding.encode(text)\n",
        "    if not tokens: return []\n",
        "    chunks = []; step_size = max_tokens - overlap_tokens\n",
        "    if step_size <= 0: step_size = max_tokens // 2 if max_tokens > 1 else 1\n",
        "    if step_size == 0: step_size = 1\n",
        "    for i in range(0, len(tokens), step_size):\n",
        "        chunk_tokens = tokens[i : i + max_tokens]\n",
        "        if not chunk_tokens: continue\n",
        "        chunks.append(encoding.decode(chunk_tokens))\n",
        "        if i + max_tokens >= len(tokens): break\n",
        "    return chunks\n",
        "\n",
        "# --- Core Review Function ---\n",
        "def review_text_chunk(chunk_text_content, attempt_fix_json=True):\n",
        "    if not client: return \"Error: OpenAI client not initialized.\"\n",
        "    system_prompt_content = (\n",
        "        \"You are an expert AI research reviewer. Your task is to critically analyze the provided chunk of a research paper. \"\n",
        "        \"Identify weak arguments, unsupported claims, logical fallacies, or methodological flaws. \"\n",
        "        \"If specific numerical data for groups (e.g., lists of values, means, SDs, Ns) are present in the text related to a statistical claim, \"\n",
        "        \"use the available tools (recalculate_p_value, compute_cohens_d, compute_confidence_interval, describe_group) to verify these claims. \"\n",
        "        \"Only call a tool if sufficient data appears to be directly extractable from the text for its parameters. Do not invent data. \"\n",
        "        \"If you use a tool, clearly state the tool's findings in your review. \"\n",
        "        \"Explain your reasoning for any critiques. Conclude with specific suggestions for improvement for this chunk, and a brief overall verdict on its scientific soundness based on your analysis.\"\n",
        "    )\n",
        "    messages_for_api = [{\"role\": \"system\", \"content\": system_prompt_content}, {\"role\": \"user\", \"content\": chunk_text_content}]\n",
        "    try:\n",
        "        response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages_for_api, tools=tools, tool_choice=\"auto\", max_tokens=3000)\n",
        "        response_message = response.choices[0].message\n",
        "        tool_calls = response_message.tool_calls\n",
        "        if tool_calls:\n",
        "            messages_for_api.append(response_message)\n",
        "            for tool_call in tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_to_call = tool_function_map.get(function_name)\n",
        "                error_content_for_tool = None # Specific error for this tool call\n",
        "                if not function_to_call: error_content_for_tool = f\"Error: Tool '{function_name}' not found.\"\n",
        "                else:\n",
        "                    try: \n",
        "                        function_args_str = tool_call.function.arguments\n",
        "                        function_args = json.loads(function_args_str)\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        error_content_for_tool = f\"Error parsing JSON arguments for {function_name}: {e}. Args: '{function_args_str}'\"\n",
        "                        function_args = None \n",
        "                    if function_args is not None:\n",
        "                        try: function_response = function_to_call(**function_args)\n",
        "                        except Exception as e: error_content_for_tool = f\"Error executing function {function_name}: {e}\"\n",
        "                \n",
        "                if error_content_for_tool:\n",
        "                     messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": error_content_for_tool})\n",
        "                else:\n",
        "                    messages_for_api.append({\"tool_call_id\": tool_call.id, \"role\": \"tool\", \"name\": function_name, \"content\": json.dumps(function_response)})\n",
        "            \n",
        "            second_response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=messages_for_api, max_tokens=3000)\n",
        "            return second_response.choices[0].message.content.strip()\n",
        "        else: return response_message.content.strip()\n",
        "    except Exception as e: return f\"Error during chunk review: {e} (Initial/Final API Call)\"\n",
        "\n",
        "# --- Full PDF Review Orchestration ---\n",
        "def review_full_pdf(pdf_path, max_chunk_tokens=12000, overlap_chunk_tokens=200):\n",
        "    if not client: return \"OpenAI client not initialized.\"\n",
        "    start_time = time.time(); print(f\"Starting review for PDF: {pdf_path}\")\n",
        "    raw_text = extract_text_from_pdf(pdf_path)\n",
        "    if raw_text.startswith(\"Error opening PDF\"): return f\"Failed to extract text: {raw_text}\"\n",
        "    print(f\"Extracted {len(raw_text)} chars.\")\n",
        "    text_chunks = chunk_text(raw_text, max_tokens=max_chunk_tokens, overlap_tokens=overlap_chunk_tokens)\n",
        "    num_chunks = len(text_chunks)\n",
        "    if num_chunks == 0: return \"No text chunks generated.\"\n",
        "    print(f\"Created {num_chunks} chunks.\")\n",
        "    all_reviews = []\n",
        "    for i, chunk_content in enumerate(text_chunks):\n",
        "        chunk_s_time = time.time(); print(f\"  Reviewing Chunk {i + 1}/{num_chunks}...\")\n",
        "        review = review_text_chunk(chunk_content)\n",
        "        all_reviews.append(f\"### Chunk {i + 1}/{num_chunks} Review:\\n{review}\")\n",
        "        print(f\"  Chunk {i + 1} review completed in {time.time() - chunk_s_time:.2f}s.\")\n",
        "        if num_chunks > 1 and i < num_chunks -1: time.sleep(1)\n",
        "    full_review_text = \"\\n\\n---\\n\\n\".join(all_reviews)\n",
        "    total_time = time.time() - start_time\n",
        "    summary = f\"\\nFull review completed in {total_time:.2f}s.\"\n",
        "    print(summary)\n",
        "    return f\"# Automated Research Paper Review\\n## PDF: {os.path.basename(pdf_path)}{summary}\\n\\n---\\n\\n{full_review_text}\"\n",
        "\n",
        "# --- Notebook Pipeline Runner ---\n",
        "def run_pipeline_in_notebook(pdf_file_path):\n",
        "    if not os.path.exists(pdf_file_path):\n",
        "        print(f\"Error: PDF file not found at {pdf_file_path}. Will attempt to create and use a dummy PDF.\")\n",
        "        try:\n",
        "            doc = fitz.open(); page = doc.new_page()\n",
        "            page.insert_text(fitz.Point(50, 72), \"This is a dummy paper. Study shows Group A [10,11,12] vs Group B [13,14,15] has p=0.018. This needs verification.\")\n",
        "            dummy_pdf_capstone = \"dummy_capstone_paper.pdf\"\n",
        "            doc.save(dummy_pdf_capstone); doc.close()\n",
        "            print(f\"Created '{dummy_pdf_capstone}'. Using this for the run.\")\n",
        "            pdf_file_path = dummy_pdf_capstone\n",
        "        except Exception as e:\n",
        "            print(f\"Could not create dummy PDF: {e}. Pipeline cannot run.\"); return\n",
        "            \n",
        "    if not client: print(\"OpenAI client not initialized. Cannot run pipeline.\"); return\n",
        "    final_review = review_full_pdf(pdf_file_path)\n",
        "    print(\"\\n\\n--- CAPSTONE: FINAL AGGREGATED REVIEW ---\"); print(final_review)\n",
        "    output_filename = f\"{os.path.splitext(os.path.basename(pdf_file_path))[0]}_capstone_review.md\"\n",
        "    with open(output_filename, \"w\", encoding=\"utf-8\") as f: f.write(final_review)\n",
        "    print(f\"\\nReview saved to {output_filename}\")\n",
        "    if 'dummy_pdf_capstone' in locals() and pdf_file_path == dummy_pdf_capstone and os.path.exists(dummy_pdf_capstone):\n",
        "        os.remove(dummy_pdf_capstone); print(f\"Removed '{dummy_pdf_capstone}'.\")\n",
        "\n",
        "print(\"Capstone Project setup complete. Call run_pipeline_in_notebook('path/to/your/paper.pdf') to start.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2. Specify Your PDF and Run the Reviewer\n",
        "\n",
        "In the cell below, replace `\"YOUR_PDF_FILE_PATH_HERE.pdf\"` with the actual path to the PDF you want to review. If the path is incorrect or the file doesn't exist, the script will attempt to create and use a small dummy PDF named `dummy_capstone_paper.pdf` for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Specify the path to your PDF file here\n",
        "pdf_to_review_path = \"YOUR_PDF_FILE_PATH_HERE.pdf\" # e.g., \"Fake_paper.pdf\" or \"my_research_paper.pdf\"\n",
        "\n",
        "if client: # Only proceed if OpenAI client is initialized\n",
        "    print(f\"Attempting to run pipeline for: {pdf_to_review_path}\")\n",
        "    # run_pipeline_in_notebook(pdf_to_review_path)\n",
        "    print(\"Execution of run_pipeline_in_notebook is commented out to prevent accidental runs.\")\n",
        "    print(\"To run: 1. Set pdf_to_review_path correctly. 2. Uncomment the line above. 3. Ensure API key is active.\")\n",
        "else:\n",
        "    print(\"OpenAI client not initialized. Please set your API key and restart the notebook kernel if necessary.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3. Analyze the Output\n",
        "\n",
        "Once the pipeline completes, a Markdown file with the review will be saved in the same directory as this notebook.\n",
        "\n",
        "Open the Markdown file and read through the review. Consider the following:\n",
        "-   **Clarity and Relevance:** Is the AI's critique clear and relevant to the content of the paper?\n",
        "-   **Accuracy:** Did it correctly identify any actual weaknesses or strengths? (Requires your own understanding of the paper).\n",
        "-   **Tool Usage:**\n",
        "    -   Did the AI call any of the statistical tools? Which ones?\n",
        "    -   Were the arguments it passed to the tools sensible (e.g., did it extract reasonable data from the text)?\n",
        "    -   How did it incorporate the tool's output into its final review of that chunk?\n",
        "-   **Suggestions:** Are the suggestions for improvement actionable and specific?\n",
        "-   **Overall Tone:** Is the tone appropriate for a research paper review?\n",
        "-   **Limitations:** What aspects of a human review does it miss? Where does it excel?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Challenge Exercises\n",
        "\n",
        "Attempt these to deepen your understanding and skills."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Challenge 1: Add a New Tool - Citation Counter\n",
        "\n",
        "1.  **Implement the Function:**\n",
        "    Write a Python function `count_citations(text_chunk)` that uses regular expressions (the `re` module) to find and count citation patterns (e.g., `[1]`, `[1, 2]`, `(Author, 2023)`, `Author et al., 2023`). This will be a heuristic and might not be perfectly accurate.\n",
        "    The function should return a dictionary like `{\"citation_count\": number, \"patterns_found\": [list_of_found_patterns]}`.\n",
        "2.  **Define its Schema:**\n",
        "    Create a JSON schema for this tool, describing its name (`count_citations`), purpose, and parameters (it takes `text_chunk` as input, though the LLM will likely call it on the current chunk it's processing, so the parameter might just be implicit or a dummy if the LLM is told to use it on the current text).\n",
        "    For simplicity, let's assume the LLM will be prompted to use it on the current `chunk_text_content` it's analyzing, so the tool function itself might not need the text as an argument if the main `review_text_chunk` passes the current chunk to it directly if called. However, standard tool definition requires parameters. So, let's define it to take `text_to_scan`.\n",
        "3.  **Integrate:**\n",
        "    Add the function to `tool_function_map` and its schema to the `tools` list.\n",
        "4.  **Update System Prompt:**\n",
        "    Modify the system prompt in `review_text_chunk` to inform the LLM about this new tool and encourage its use (e.g., \"You can also use `count_citations` to get an idea of the number of references made in this chunk.\").\n",
        "5.  **Test:**\n",
        "    Run your reviewer on a paper and see if/how it uses the citation counter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "import re\n",
        "\n",
        "def count_citations(text_to_scan):\n",
        "    \"\"\"Counts common citation patterns in a given text chunk.\"\"\"\n",
        "    # Pattern for [1], [12], [1, 2], [1, 2, 3], etc.\n",
        "    numerical_citations = re.findall(r'\\[\\d+(,\\s*\\d+)*\\]', text_to_scan)\n",
        "    # Pattern for (Author, YYYY) or (Author et al., YYYY)\n",
        "    author_year_citations = re.findall(r'\\([A-Za-z][A-Za-z\\s\\.&]+(et al\\.)?,\\s*\\d{4}\\)', text_to_scan)\n",
        "    \n",
        "    all_found_patterns = numerical_citations + author_year_citations\n",
        "    citation_count = len(all_found_patterns)\n",
        "    \n",
        "    return {\"citation_count\": citation_count, \"patterns_found\": all_found_patterns[:20]} # Return first 20 found to keep it concise\n",
        "\n",
        "# Test the citation counter\n",
        "sample_text_with_citations = \"This is supported by [1] and also (Smith, 2020). Some other work includes [2, 3] and (Jones et al., 2021).\"\n",
        "citation_info = count_citations(sample_text_with_citations)\n",
        "print(f\"Citation count test: {citation_info}\")\n",
        "\n",
        "# Schema for count_citations\n",
        "citation_tool_schema = {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"count_citations\",\n",
        "        \"description\": \"Counts and lists common citation patterns (e.g., [1], (Author, YYYY)) found in a given text. Use this to get a rough estimate of references in a section.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"text_to_scan\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The segment of text in which to count citations.\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"text_to_scan\"]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# To integrate (you'd modify the main tool_function_map and tools list in the consolidated code):\n",
        "# 1. Add to tool_function_map:\n",
        "#    tool_function_map['count_citations'] = count_citations\n",
        "# 2. Add to tools list:\n",
        "#    tools.append(citation_tool_schema)\n",
        "# 3. Update system_prompt in review_text_chunk to mention this tool.\n",
        "\n",
        "print(\"Citation counter function and schema defined. Manual integration steps provided above.\")\n",
        "print(\"To fully test, you'd need to re-initialize the 'tools' and 'tool_function_map' in the consolidated code section and re-run.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Challenge 2: Refine Prompts for Specific Focus\n",
        "\n",
        "1.  **Create a Copy:** Make a copy of the `review_text_chunk` function (e.g., `review_text_chunk_methodology_focus`).\n",
        "2.  **Modify System Prompt:** In this new function, alter the system prompt to instruct the AI to *primarily* focus on the paper's methodology. For example: \"You are an expert AI research reviewer, specializing in methodological rigor. For the given chunk, focus intensely on the description of methods, experimental design, statistical approaches used, and sample characteristics. Highlight any ambiguities, potential biases, or limitations in the methodology.\"\n",
        "3.  **Test:** Run this specialized reviewer on a few chunks that you know contain methodological descriptions and compare its output to the general reviewer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Placeholder for Challenge 2 - Refine Prompts\n",
        "# Students would copy the review_text_chunk function and modify its system_prompt.\n",
        "# Then, they would call this modified function within review_full_pdf or directly.\n",
        "print(\"Challenge 2: See instructions above to implement a methodology-focused reviewer.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Challenge 3: Structured Output (JSON)\n",
        "\n",
        "1.  **Modify System Prompt:** Update the system prompt in `review_text_chunk` to request the output in a specific JSON format. For example: \"Provide your review as a JSON object with the following keys: 'strengths' (a list of strings), 'weaknesses' (a list of strings), 'suggestions' (a list of strings), 'tool_usage_summary' (a string describing any tools used and their findings), and 'overall_verdict' (a string).\"\n",
        "2.  **Parse Output:** Modify the `review_text_chunk` function to attempt to `json.loads()` the model's final content. Handle potential `JSONDecodeError` if the model doesn't perfectly adhere to the format.\n",
        "3.  **Update Aggregation:** The `review_full_pdf` function would then need to handle these JSON objects, perhaps by formatting them nicely into the Markdown report or collecting lists of all weaknesses, etc.\n",
        "4.  **Test:** This is more advanced and may require more prompt iteration to get reliable JSON output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": null,
      "source": [
        "# Placeholder for Challenge 3 - Structured Output\n",
        "# Students would modify review_text_chunk's system_prompt and add JSON parsing for the output.\n",
        "print(\"Challenge 3: See instructions above to implement JSON structured output.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 5. Evaluating Strengths/Limits and Next Steps\n",
        "\n",
        "-   **Strengths:** What did your AI reviewer do well? (e.g., speed, consistency, catching obvious errors, using tools for verification).\n",
        "-   **Limitations:** Where did it fall short? (e.g., nuanced understanding, subtle errors, over-reliance on text patterns, creativity in suggestions, cost of API calls for very long papers).\n",
        "-   **Next Steps:** If this were a real product, what would be the next 1-3 features or improvements you'd prioritize? (e.g., better UI, more sophisticated tools, cost optimization, fine-tuning a model, human-in-the-loop review process)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "Congratulations on completing the course and capstone project! You've gained valuable experience in building sophisticated AI applications with reasoning models and tool use."
      ]
    }
  ]
}
