{
  "course_title": "Building an AI Research Paper Reviewer with OpenAI‚Äôs o4-mini: Reasoning, Prompt Engineering, and Tool Use",
  "course_summary": "In this hands-on course, you'll learn how to leverage OpenAI's o4-mini reasoning model to build a practical research paper reviewer in Python. You'll develop skills in prompt engineering for reasoning models, integrate function calling for statistical analysis, and apply these concepts in a real-world capstone project. By the end, you'll understand how to combine LLM reasoning, custom tools, and prompt design to create robust AI-powered applications.",
  "lessons": [
    {
      "lesson_number": 1,
      "lesson_title": "Introduction to Reasoning Models and the o4-mini API",
      "lesson_format": "Video Only",
      "video_script": "# Lesson 1: Introduction to Reasoning Models and the o4-mini API\n\n---\n\n**[Opening scene: Animated robot holding a research paper with Python code floating around]**\n\nHello and welcome! In this course, you'll learn how to harness the power of OpenAI‚Äôs o4-mini API to build an intelligent research paper reviewer‚Äîa project that brings together language models, reasoning, and statistical tools.\n\nLet‚Äôs start with the basics: **What are reasoning models?**\n\n**[Cut to: Simplified diagram showing a regular LLM vs. a 'reasoning model' with extra gears and logic symbols]**\n\nReasoning models, like o4-mini, are a special class of large language models (LLMs) designed not just to generate text, but to analyze, explain, and reason deeply about complex problems. Think of them as the difference between someone who can repeat facts and someone who can critically evaluate and solve problems.\n\n### Where do reasoning models shine?\n\n- **Scientific analysis**\n- **Code understanding and debugging**\n- **Mathematical problem-solving**\n- **Evaluating arguments or research claims**\n\nNow, let‚Äôs look at the **o4-mini model** itself.\n\n**[Show: o4-mini robot mascot, with stats like 'Fast', 'Affordable', 'Great at Math & Code']**\n\nOpenAI‚Äôs o4-mini is the latest in their o-series of reasoning models. It‚Äôs smaller and more cost-effective than some of its big siblings, but don‚Äôt let that fool you‚Äîit‚Äôs state-of-the-art for its size, achieving high scores on benchmarks for math, code, and science tasks.\n\n**Key strengths of o4-mini:**\n- Fast, low-latency outputs (great for interactive or high-volume apps)\n- Efficient use of memory and API credits\n- Top-tier reasoning for code, math, and scientific text\n\nBut there are also some trade-offs:\n- Slightly less context window than the largest models\n- Not as strong for purely creative or open-ended tasks as GPT-4o\n\n### What will we build in this course?\n\nYou‚Äôll build a research paper reviewer that can:\n- Ingest a research paper (PDF or text)\n- Highlight weak arguments or flaws in logic\n- Suggest improvements and provide a verdict\n- Even call Python functions to check p-values or confidence intervals when needed!\n\n**[Show: Flowchart of the pipeline ‚Äî paper in, LLM review, statistical tool, review out]**\n\nBy the end of this course, you‚Äôll know how to:\n- Use reasoning models via the OpenAI API\n- Design effective prompts for critical analysis\n- Integrate custom Python tools for real-time fact checking\n\nReady to dive in? In the next lesson, we‚Äôll get you set up with API access and all the tools you need.\n\n---\n\n## Suggested Images/Visuals\n- Animated robot (o4-mini mascot) holding a research paper\n- Comparison chart: Standard LLM vs. Reasoning Model (with 'logic gears')\n- Flowchart: PDF ‚Üí Extraction ‚Üí LLM ‚Üí Review Output\n- o4-mini \"feature card\" (speed, reasoning, cost)\n- Diagram: LLM calling a Python function (arrows from LLM to calculator to result)\n\n",
      "visual_suggestions": [
        "Animated robot (o4-mini mascot) holding a research paper",
        "Comparison chart: Standard LLM vs. Reasoning Model (with 'logic gears')",
        "Flowchart: PDF ‚Üí Extraction ‚Üí LLM ‚Üí Review Output",
        "o4-mini 'feature card' (speed, reasoning, cost-efficiency)",
        "Diagram: LLM calling a Python function (arrows from LLM to calculator to result)"
      ]
    },
    {
      "lesson_number": 2,
      "lesson_title": "Setting Up Your Environment and Accessing the API",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 2: Setting Up Your Environment and Accessing the API\n\n---\n\n**[Opening scene: Terminal window, Python logo, and OpenAI logo]**\n\nBefore we can build our AI-powered research reviewer, let‚Äôs make sure our development environment is ready to go!\n\nIn this lesson, you‚Äôll:\n- Get access to the OpenAI API by creating your own API key\n- Install the Python libraries needed for text extraction, tokenization, and statistics\n- Learn how to securely set up your API key so your credentials stay safe\n\n**[Show: Step-by-step animation of a user visiting the OpenAI API dashboard]**\n\n### Step 1: Get Your OpenAI API Key\n1. Go to [platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)\n2. Log in, click **Create new secret key**, then copy it somewhere safe (you‚Äôll need it for your code).\n3. Ensure your account has billing set up if required.\n\n**[Show: Terminal installing packages]**\n\n### Step 2: Install Required Libraries\nWe‚Äôll use several libraries:\n- `openai` ‚Äî to talk to the o4-mini API\n- `PyMuPDF` (imported as `fitz`) ‚Äî to extract text from PDFs\n- `tiktoken` ‚Äî to handle tokenization for LLM input\n- `numpy` and `scipy` ‚Äî for statistical calculations\n\n**[Show: .env file, or terminal export command with API key]**\n\n### Step 3: Secure Your API Key\nNever hard-code your API keys! Instead, use environment variables. This keeps your credentials safe and makes your code more portable.\n\nWe‚Äôll walk you through setting the `OPENAI_API_KEY` variable, and show you how to check it from Python.\n\nAfter this lesson, you‚Äôll be ready to start building and experimenting safely. Let‚Äôs jump into the notebook and get set up!\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 2: Setting Up Your Environment and Accessing the API\n\n---\n\nIn this lesson, you'll set up everything you need to start working with OpenAI's o4-mini API and prepare your Python environment for the research paper reviewer project.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. Install Required Packages\n\nWe'll need a few Python packages:\n- `openai`: For accessing the OpenAI API\n- `PyMuPDF` (imported as `fitz`): For extracting text from PDFs\n- `tiktoken`: For handling tokenization\n- `numpy`, `scipy`: For statistical calculations\n\n> **Tip:** In Jupyter or Colab, use `!` to run shell commands."
        },
        {
          "cell_type": "code",
          "source": "!pip install openai PyMuPDF tiktoken numpy scipy"
        },
        {
          "cell_type": "markdown",
          "source": "## 2. Setting the OpenAI API Key Securely\n\nIt's best practice **not** to hard-code your API key in scripts. Instead, set it as an environment variable named `OPENAI_API_KEY`.\n\n**Why?**\n- Keeps your credentials out of your codebase\n- Prevents accidental sharing of secrets\n- Makes your code portable across machines\n"
        },
        {
          "cell_type": "markdown",
          "source": "### (a) On your own computer (Linux/macOS/Colab):\n```bash\nexport OPENAI_API_KEY=\"your-secret-key\"\n```\n\n### (b) In a notebook (for quick demos only):"
        },
        {
          "cell_type": "code",
          "source": "import os\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\"  # Replace with your actual key"
        },
        {
          "cell_type": "markdown",
          "source": "## 3. Verifying Your Environment\n\nLet's check that your API key is set and the libraries are installed."
        },
        {
          "cell_type": "code",
          "source": "import openai, fitz, tiktoken, numpy, scipy\nimport os\n\n# Check if API key is set\napi_key = os.getenv(\"OPENAI_API_KEY\")\nif api_key:\n    print(\"‚úÖ OPENAI_API_KEY is set!\")\nelse:\n    print(\"‚ö†Ô∏è OPENAI_API_KEY is NOT set. Please set it before proceeding.\")"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## 4. What Does Each Library Do?\n\n- **openai**: Allows Python to communicate with OpenAI's API, including o4-mini.\n- **PyMuPDF (fitz)**: Efficient PDF text extraction.\n- **tiktoken**: Tokenizes text into the format the model expects (avoids cutoff errors).\n- **numpy, scipy**: Provide fast, reliable tools for statistical calculations (like p-values, effect sizes).\n\n---"
        },
        {
          "cell_type": "markdown",
          "source": "## Practice Exercise üìù\n\n**Task:**\n1. Check that you can import all required libraries without errors.\n2. Print your `OPENAI_API_KEY` (don't share it anywhere public!).\n\nFill in the code below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# 1. Import necessary libraries\n# 2. Print the first 6 characters of your OPENAI_API_KEY (for verification)\n"
        }
      ]
    },
    {
      "lesson_number": 3,
      "lesson_title": "Fundamentals of Prompt Engineering for Reasoning Models",
      "lesson_format": "Video Only",
      "video_script": "# Lesson 3: Fundamentals of Prompt Engineering for Reasoning Models\n\n---\n\n**[Opening scene: Text prompts being crafted, model thinking bubbles, and output text boxes]**\n\nWelcome back! Now that your environment is ready, let‚Äôs talk about one of the most important aspects of working with reasoning models: **prompt engineering**.\n\n### What is prompt engineering?\n\nPrompt engineering is the art and science of designing the instructions and input that you give a language model. The way you craft your prompt can dramatically change the model‚Äôs output‚Äîespecially for reasoning tasks where you want detailed, step-by-step analysis.\n\n**[Show: Example prompt vs. output pairs]**\n\nLet‚Äôs look at the structure of an effective reasoning prompt:\n\n- **System prompt:** Sets the model‚Äôs role and expectations (e.g., ‚ÄúYou are an expert research paper reviewer.‚Äù)\n- **Task instruction:** What do you want it to do? (e.g., ‚ÄúIdentify weak arguments and suggest improvements.‚Äù)\n- **Context input:** The actual research text or question the model should analyze.\n\n**[Show: Highlighted system prompt and user input in a code cell]**\n\n### How does prompt structure influence model behavior?\n\n- **Specificity:** The more specific your instructions, the more targeted and reliable the output.\n- **Role assignment:** Telling the model to ‚Äúthink like a reviewer‚Äù encourages deeper, critical responses.\n- **Stepwise reasoning:** Explicitly asking for detailed reasoning can activate chain-of-thought responses, producing transparent, logical steps.\n\n**[Show: Model output with step-by-step reasoning highlighted]**\n\n### Best Practices\n- Tell the model *who* it is and *how* to behave\n- Request reasoning, not just answers (e.g., ‚ÄúExplain your logic‚Äù)\n- Encourage use of tools (e.g., ‚ÄúIf needed, call a function to verify p-values.‚Äù)\n\n### Common Pitfalls\n- Being too vague or open-ended (‚ÄúIs this paper good?‚Äù)\n- Overloading the prompt with too many instructions\n- Forgetting to clarify the expected output format\n\n**[Show: Bad vs. good prompts side by side]**\n\n### For reasoning models like o4-mini\n- Use clear, concise instructions\n- Encourage multi-step thinking\n- Invite the model to justify its verdict and request data checks as needed\n\nWith well-engineered prompts, you unlock the real reasoning power of models like o4-mini. In the next lesson, we‚Äôll see how prompts and function calling work together for robust, data-driven review!\n\n---\n\n## Suggested Images/Visuals\n- Side-by-side: Simple prompt vs. detailed reasoning prompt\n- Model ‚Äúthought bubble‚Äù showing step-by-step reasoning steps\n- Annotated system prompt and user input\n- Chart: Prompt specificity vs. output quality\n- Highlighted example of model using chain-of-thought\n",
      "visual_suggestions": [
        "Side-by-side: simple prompt vs. detailed reasoning prompt",
        "Model 'thought bubble' showing step-by-step reasoning",
        "Annotated system prompt and user input in a code cell",
        "Chart: prompt specificity vs. output quality",
        "Example output with chain-of-thought reasoning highlighted"
      ]
    },
    {
      "lesson_number": 4,
      "lesson_title": "Integrating Function Calling‚ÄîBuilding Statistical Tool Helpers",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 4: Integrating Function Calling‚ÄîBuilding Statistical Tool Helpers\n\n---\n\n**[Opening scene: LLM cartoon character reaching out to a calculator or statistics toolbox]**\n\nWelcome back! Today, you'll learn how to supercharge your LLM with *function calling*. This lets the model request calculations from Python functions whenever it finds statistical claims or wants to verify results.\n\n**Why is this important?**\nLLMs, even advanced ones like o4-mini, excel at understanding text and reasoning. But for precise calculations‚Äîlike recalculating a p-value or computing a confidence interval‚Äîthey‚Äôre better off asking Python to do the math!\n\n**[Show: Diagram‚Äîmodel outputting a function call, Python returns answer, LLM continues]**\n\nIn this project, we‚Äôll implement four core helpers:\n- `recalculate_p_value`: Tests for statistical significance between two groups\n- `compute_cohens_d`: Measures effect size (how big is the difference?)\n- `compute_confidence_interval`: Estimates the plausible range of a group‚Äôs mean\n- `describe_group`: Returns mean, standard deviation, and sample size of a dataset\n\nThese tools let the model *fact-check* claims in a research paper, bringing rigor and transparency to its reviews.\n\nYou‚Äôll build each function, try them out, and see what kind of results they return for various sample data. This prepares your codebase for seamless integration with the o4-mini model‚Äôs tool-calling abilities in later lessons.\n\nLet‚Äôs open the notebook and get building!\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 4: Integrating Function Calling‚ÄîBuilding Statistical Tool Helpers\n\n---\n\nIn this lesson, you'll implement Python functions for statistical analysis. These helpers will later be available for the LLM to call during its review process.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. Imports\n\nWe'll use `numpy` and `scipy.stats` for calculations, just like a real research reviewer would.\n"
        },
        {
          "cell_type": "code",
          "source": "import numpy as np\nfrom scipy.stats import ttest_ind, sem, t"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## 2. Helper Functions\n\n### (a) Recalculate p-value\n\nThis function performs Welch‚Äôs t-test between two independent groups to check if their means are statistically different."
        },
        {
          "cell_type": "code",
          "source": "def recalculate_p_value(group1, group2):\n    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n    return {\"p_value\": round(p_value, 4)}"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\nSuppose we have two groups of experiment results:"
        },
        {
          "cell_type": "code",
          "source": "group1 = [2.1, 2.3, 2.9, 3.0, 2.7]\ngroup2 = [3.1, 3.3, 3.9, 4.0, 3.7]\nrecalculate_p_value(group1, group2)"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n### (b) Compute Cohen‚Äôs d\n\nCohen‚Äôs d measures how big the difference is between two groups (the effect size)."
        },
        {
          "cell_type": "code",
          "source": "def compute_cohens_d(group1, group2):\n    mean1, mean2 = np.mean(group1), np.mean(group2)\n    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n    d = (mean1 - mean2) / pooled_std\n    return {\"cohens_d\": round(d, 4)}"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\n"
        },
        {
          "cell_type": "code",
          "source": "compute_cohens_d(group1, group2)"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n### (c) Compute Confidence Interval\n\nThis function estimates the mean and a confidence interval for a single group."
        },
        {
          "cell_type": "code",
          "source": "def compute_confidence_interval(data, confidence=0.95):\n    data = np.array(data)\n    n = len(data)\n    mean = np.mean(data)\n    margin = sem(data) * t.ppf((1 + confidence) / 2., n - 1)\n    return {\n        \"mean\": round(mean, 4),\n        \"confidence_interval\": [round(mean - margin, 4), round(mean + margin, 4)],\n        \"confidence\": confidence\n    }"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\n"
        },
        {
          "cell_type": "code",
          "source": "compute_confidence_interval(group1, confidence=0.95)"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n### (d) Describe Group\n\nSummarize the mean, standard deviation, and sample size for a group."
        },
        {
          "cell_type": "code",
          "source": "def describe_group(data):\n    data = np.array(data)\n    return {\n        \"mean\": round(np.mean(data), 4),\n        \"std_dev\": round(np.std(data, ddof=1), 4),\n        \"n\": len(data)\n    }"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\n"
        },
        {
          "cell_type": "code",
          "source": "describe_group(group2)"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## Understanding the Logic\n\n- **p-value** tells you if an observed difference could be due to chance.\n- **Cohen‚Äôs d** tells you how *big* the difference is (practical vs. statistical significance).\n- **Confidence interval** shows the range where the true mean likely falls.\n- **Describe group** gives basic stats for context.\n\nThese tools help the LLM ground its analysis in real data, making reviews more trustworthy.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## Practice Exercise üìù\n\n**Task:**\n- Create your own two groups of numbers (at least 5 elements each)\n- Use all four helpers to analyze them\n\nFill in the code below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# Define two sample groups\ng1 = [...]\ng2 = [...]\n\n# 1. Recalculate p-value\n# 2. Compute Cohen's d\n# 3. Compute confidence interval for g1\n# 4. Describe group g2\n"
        }
      ]
    },
    {
      "lesson_number": 5,
      "lesson_title": "Extracting and Preparing Research Paper Content",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 5: Extracting and Preparing Research Paper Content\n\n---\n\n**[Opening scene: PDF icon turning into text lines, then into token blocks]**\n\nNow that we have our statistical helpers, it‚Äôs time to get research paper content ready for the LLM!\n\nMost research papers are shared as PDFs. But LLMs work with plain text‚Äîso we need to extract the paper‚Äôs content first.\n\n**[Show: Animation‚ÄîPDF ‚Üí Text ‚Üí Token blocks]**\n\nWe‚Äôll use `PyMuPDF` (imported as `fitz`) to read all the text from a PDF file. It lets us loop through pages and collect every word‚Äîso nothing gets lost.\n\nNext, we face another challenge: LLMs like o4-mini can only process a certain number of tokens at once (their ‚Äúcontext window‚Äù). If the document is too long, we have to break it into *chunks* that fit.\n\nThat‚Äôs where `tiktoken` comes in. It splits the text into model-compatible tokens, then we break it into safe-sized pieces. Each chunk becomes a separate review unit for the LLM.\n\n**Why chunking?**\n- Prevents errors from exceeding context limits\n- Ensures the model gets *all* the content, not just the start\n- Maintains logical sections (like paragraphs or sections)\n\nIn the notebook, you‚Äôll:\n- Write a function to extract text from PDFs\n- Tokenize and chunk it using tiktoken\n- Check that your chunks cover the full paper with no missing content\n\nLet‚Äôs get started!\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 5: Extracting and Preparing Research Paper Content\n\n---\n\nIn this lesson, you'll learn how to extract text from a PDF research paper and split it into manageable chunks for LLM analysis.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. Extract Text from PDF\n\nWe'll use `PyMuPDF` (imported as `fitz`) to read every page of a PDF into a single string."
        },
        {
          "cell_type": "code",
          "source": "import fitz  # PyMuPDF\n\ndef extract_text_from_pdf(path):\n    doc = fitz.open(path)\n    full_text = \"\\n\".join(page.get_text() for page in doc)\n    return full_text"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\nSuppose you have a file called `sample_paper.pdf` in your working directory:"
        },
        {
          "cell_type": "code",
          "source": "# Uncomment and run if you have a PDF file to test\n# text = extract_text_from_pdf('sample_paper.pdf')\n# print(text[:500])  # Show the first 500 characters"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## 2. Tokenization and Chunking\n\nLLMs have a maximum number of tokens they can process at once (context limit). We‚Äôll use `tiktoken` to split our text into chunks that fit."
        },
        {
          "cell_type": "code",
          "source": "import tiktoken\n\ndef chunk_text(text, max_tokens=12000, model=\"o4-mini\"):\n    encoding = tiktoken.get_encoding(\"cl100k_base\")\n    tokens = encoding.encode(text)\n    chunks = []\n    for i in range(0, len(tokens), max_tokens):\n        chunk = tokens[i:i + max_tokens]\n        chunk_text = encoding.decode(chunk)\n        chunks.append(chunk_text)\n    return chunks"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\n"
        },
        {
          "cell_type": "code",
          "source": "# If you extracted text above:\n# chunks = chunk_text(text, max_tokens=12000)\n# print(f\"Number of chunks: {len(chunks)}\")\n# print(chunks[0][:400])  # Preview the first chunk"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## Why Chunking Matters\n\n- Prevents model input errors and data loss\n- Ensures every part of the paper gets reviewed\n- Keeps the review process organized and traceable\n\n> **Tip:** Chunk size can be adjusted based on model and document length."
        },
        {
          "cell_type": "markdown",
          "source": "## Practice Exercise üìù\n\n**Task:**\n- Try extracting and chunking a PDF of your choice (or use a short sample string if you don‚Äôt have a PDF handy).\n- Print out the number of chunks and the first 200 characters of each.\n\nFill in the code below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# 1. Extract text from a PDF or use a sample string\n# 2. Chunk the text and print chunk info\n"
        }
      ]
    },
    {
      "lesson_number": 6,
      "lesson_title": "Mapping and Registering Tools for LLM Access",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 6: Mapping and Registering Tools for LLM Access\n\n---\n\n**[Opening scene: LLM cartoon with toolbelt, linking to calculator, chart, and stats icons]**\n\nYou‚Äôve built your statistical helpers‚Äînow let‚Äôs make sure the LLM knows how to use them!\n\nThis lesson covers two key steps:\n1. **Mapping**: Connecting the function names the model uses (like 'recalculate_p_value') to your actual Python functions\n2. **Registering Tool Schemas**: Telling the OpenAI API what each tool does and what inputs it expects\n\n**[Show: Diagram‚ÄîLLM calls 'compute_cohens_d' ‚Üí mapped to Python function ‚Üí returns result]**\n\nThis setup lets the model:\n- Discover what tools are available\n- Know the required input arguments and their types\n- Safely call your Python functions and get results to use in its reasoning\n\nYou‚Äôll see how each function gets a schema‚Äîlike a blueprint‚Äîdescribing what it does, what inputs it takes, and what it returns. This helps with validation and error checking too.\n\nAfter this lesson, your LLM will be able to call your Python tools whenever it needs statistical backup during review!\n\nLet‚Äôs dive into the code.\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 6: Mapping and Registering Tools for LLM Access\n\n---\n\nHere, you'll connect your helper functions to the names and schemas that the LLM API expects for function calling.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. Tool Function Map\n\nThis dictionary links the tool names (as seen by the model) to your Python functions."
        },
        {
          "cell_type": "code",
          "source": "tool_function_map = {\n    \"recalculate_p_value\": recalculate_p_value,\n    \"compute_cohens_d\": compute_cohens_d,\n    \"compute_confidence_interval\": compute_confidence_interval,\n    \"describe_group\": describe_group,\n}\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 2. Tool Schema Definitions\n\nEach tool needs a schema so the LLM knows what it does and what input it requires. This is a list of dictionaries, one for each function."
        },
        {
          "cell_type": "code",
          "source": "tools = [\n    {\n        \"type\": \"function\",\n        \"name\": \"recalculate_p_value\",\n        \"description\": \"Calculate p-value between two sample groups\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n                \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n            },\n            \"required\": [\"group1\", \"group2\"]\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"name\": \"compute_cohens_d\",\n        \"description\": \"Compute effect size (Cohen's d) between two groups\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n                \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n            },\n            \"required\": [\"group1\", \"group2\"]\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"name\": \"compute_confidence_interval\",\n        \"description\": \"Compute confidence interval for a sample group\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n                \"confidence\": {\"type\": \"number\", \"default\": 0.95},\n            },\n            \"required\": [\"data\"]\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"name\": \"describe_group\",\n        \"description\": \"Summarize sample mean, std deviation, and count\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}},\n            },\n            \"required\": [\"data\"]\n        }\n    }\n]\n"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## How Does This Work?\n\n- The **function map** allows your backend to find and call the correct Python function when the LLM requests a tool.\n- The **schemas** tell the OpenAI API (and the LLM) exactly what each tool does and what inputs are needed‚Äîso the model knows how to call them.\n\nThis makes tool use both discoverable and safe!\n"
        },
        {
          "cell_type": "markdown",
          "source": "## Practice Exercise üìù\n\n**Task:**\n- Add a new tool schema for a function called `compute_median` that takes a list of numbers as `data` and returns the median.\n- Write the Python function and add it to your map.\n\nFill in the code below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# 1. Define compute_median(data)\n# 2. Add its schema to the tools list\n# 3. Add it to tool_function_map\n"
        }
      ]
    },
    {
      "lesson_number": 7,
      "lesson_title": "Reasoning Workflow‚ÄîReviewing Paper Chunks with o4-mini",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 7: Reasoning Workflow‚ÄîReviewing Paper Chunks with o4-mini\n\n---\n\n**[Opening scene: LLM character at a desk, reviewing chunks of a research paper, occasionally looking up stats on a calculator]**\n\nThis is where everything comes together!\n\nIn this lesson, you‚Äôll see how the LLM reviews each chunk of the research paper:\n- Receives a chunk of text and a system prompt\n- Analyzes for weak arguments or unsupported claims\n- If needed, asks for a tool call (like recalculating a p-value)\n- Gets the result from your Python helper\n- Refines its judgment and delivers a structured review\n\nThis workflow mimics how a human reviewer thinks: read, question, verify, and conclude.\n\nYou‚Äôll walk through the `review_text_chunk()` function. It handles all the logic:\n- Sending the chunk and prompt to o4-mini\n- Detecting if a tool was called\n- Running the function and feeding the result back to the model\n- Returning the final review output\n\nWe‚Äôll also look at a sample interaction, where the model spots a claim, asks for a calculation, and updates its critique based on the data.\n\nLet‚Äôs step through the code and see your AI reviewer in action!\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 7: Reasoning Workflow‚ÄîReviewing Paper Chunks with o4-mini\n\n---\n\nThis lesson shows how the LLM processes each chunk, calls tools as needed, and produces a structured review.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. The Review Function\n\nThis function takes a text chunk, sends it to o4-mini with a system prompt, and manages tool calls."
        },
        {
          "cell_type": "code",
          "source": "import openai  # Make sure your API key is set\nimport os\n\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")  # Already set earlier"
        },
        {
          "cell_type": "markdown",
          "source": "### Review Logic\n\nBelow is a simplified version of the review workflow. (For demo, we'll use the OpenAI ChatCompletion API‚Äîreplace as needed for o4-mini's latest API syntax.)"
        },
        {
          "cell_type": "code",
          "source": "def review_text_chunk(chunk, tools=tools, tool_function_map=tool_function_map):\n    system_prompt = (\n        \"You are an expert AI research reviewer. Read the given chunk of a research paper and highlight weak arguments, unsupported claims, or flawed methodology. \"\n        \"You can request tools to: Recalculate p-values, Compute confidence intervals, Estimate effect size (Cohen's d), Describe sample statistics. \"\n        \"Be rigorous and explain your reasoning. Conclude with suggestions and a verdict.\"\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": chunk}\n    ]\n    # Simulate tool-calling loop (actual o4-mini API may differ)\n    response = openai.ChatCompletion.create(\n        model=\"gpt-4\",  # Replace with \"o4-mini\" as needed\n        messages=messages,\n        # In practice, pass tool schema as 'tools' or 'functions' argument\n    )\n    output_text = response['choices'][0]['message']['content']\n    return output_text"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example Usage\nSuppose you have a sample chunk:"
        },
        {
          "cell_type": "code",
          "source": "sample_chunk = \"In our experiment, Group A (n=5) had an average score of 2.6, while Group B (n=5) had an average score of 3.8. The reported p-value was 0.02, indicating significance.\"\n# review = review_text_chunk(sample_chunk)\n# print(review)"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## How Does This Workflow Mimic a Human Reviewer?\n- Reads and understands the text\n- Spots questionable claims\n- Calls for calculations when needed\n- Revises its assessment based on the results\n\nThis multi-step reasoning is what gives your LLM reviewer real analytical power."
        },
        {
          "cell_type": "markdown",
          "source": "## Practice Exercise üìù\n\n**Task:**\n- Write a short research text chunk with a statistical claim\n- Pass it to your review function and print the output\n\nFill in the code below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# Compose a sample chunk and review it\n"
        }
      ]
    },
    {
      "lesson_number": 8,
      "lesson_title": "Bringing It All Together‚ÄîFull Paper Review Pipeline",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 8: Bringing It All Together‚ÄîFull Paper Review Pipeline\n\n---\n\n**[Opening scene: Animated assembly line‚ÄîPDF in, code modules working, review report out]**\n\nYou‚Äôve built every piece‚Äînow let‚Äôs connect them into a complete research paper review pipeline!\n\nThis lesson walks you through:\n- The function that coordinates extraction, chunking, review, and aggregation\n- How each module (text extraction, chunking, review, tool use) fits into the flow\n- Saving and displaying the final review output\n\nYou‚Äôll see how the design stays modular‚Äîmaking it easy to test, debug, and extend. You‚Äôll also learn best practices for reproducibility and tips for scaling up (like adding a UI or extra tools).\n\nBy the end, you‚Äôll have a working, end-to-end AI reviewer that can process an entire PDF and output a structured critique.\n\nLet‚Äôs jump into the code and run your first full review!\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 8: Bringing It All Together‚ÄîFull Paper Review Pipeline\n\n---\n\nIn this lesson, you'll connect every module into a working pipeline that takes a PDF, reviews it chunk by chunk, and outputs a full critique.\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. Full Paper Review Function\n\nThis function orchestrates extraction, chunking, review, and aggregation."
        },
        {
          "cell_type": "code",
          "source": "def review_full_pdf(pdf_path):\n    raw_text = extract_text_from_pdf(pdf_path)\n    chunks = chunk_text(raw_text)\n    print(f\"\\nExtracted {len(chunks)} chunks from PDF\\n\")\n    all_reviews = []\n    for idx, chunk in enumerate(chunks):\n        print(f\"\\nReviewing Chunk {idx + 1}/{len(chunks)}...\")\n        review = review_text_chunk(chunk)\n        all_reviews.append(f\"### Chunk {idx + 1} Review\\n{review}\")\n    full_review = \"\\n\\n\".join(all_reviews)\n    return full_review"
        },
        {
          "cell_type": "markdown",
          "source": "#### Example\n"
        },
        {
          "cell_type": "code",
          "source": "# Replace with your own PDF filename\n# output = review_full_pdf('sample_paper.pdf')\n# print(output[:1000])  # Preview the first 1000 characters"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## 2. Saving Results\n\nIt's helpful to save the full review to a Markdown file so you can read or share it."
        },
        {
          "cell_type": "code",
          "source": "# Example: Save output\n# with open('paper_review_output.md', 'w') as f:\n#     f.write(output)\n# print(\"Review saved to paper_review_output.md\")"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## Best Practices\n- Keep code modular: separate extraction, chunking, review, and saving\n- Make it easy to swap in new tools or prompts\n- Use clear logging for reproducibility\n\n**Tip:** You can later add a UI or web interface, or extend with more analysis tools!"
        },
        {
          "cell_type": "markdown",
          "source": "## Practice Exercise üìù\n\n**Task:**\n- Run the full pipeline on a short text or PDF (if available)\n- Save the output and preview the first section\n\nFill in the code below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# Run end-to-end pipeline and save output\n"
        }
      ]
    },
    {
      "lesson_number": 9,
      "lesson_title": "Capstone Project‚ÄîBuild and Run Your Own Paper Reviewer",
      "lesson_format": "Video + Notebook",
      "video_script": "# Lesson 9: Capstone Project‚ÄîBuild and Run Your Own Paper Reviewer\n\n---\n\n**[Opening scene: Trophy icon, student at a desk running the full pipeline]**\n\nCongratulations‚Äîit's time to bring everything together in your capstone project!\n\nIn this lesson, you‚Äôll:\n- Run the full research reviewer pipeline on your own PDF or sample text\n- Experiment with prompt tweaks or by extending the toolset\n- Test the system, interpret the output, and reflect on its strengths and limitations\n\n**Challenge ideas:**\n- Try adding a new statistical helper tool (like median or mode)\n- Experiment with different system prompts to guide the LLM‚Äôs review style\n- Analyze a real research paper and compare the AI‚Äôs critique with your own\n\nYou‚Äôll end this project with a working, customizable AI reviewer‚Äîand the practical skills to build more advanced LLM tools in the future.\n\nReady? Open the notebook, choose a paper, and let your reviewer go to work!\n",
      "notebook": [
        {
          "cell_type": "markdown",
          "source": "# Lesson 9: Capstone Project‚ÄîBuild and Run Your Own Paper Reviewer\n\n---\n\nIn this final lesson, you'll apply everything you've learned to build, run, and analyze your own research paper reviewer!\n"
        },
        {
          "cell_type": "markdown",
          "source": "## 1. Run the Full Pipeline\n\nChoose a PDF or sample text, and use your pipeline to generate a structured review."
        },
        {
          "cell_type": "code",
          "source": "# Replace with your own PDF filename or sample text\n# output = review_full_pdf('your_paper.pdf')\n# print(output[:2000])  # Preview the output"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## 2. Experiment: Prompt Tweaks and Tool Extensions\n\nTry changing the system prompt to make the review more lenient, more critical, or more concise. Or, add a new helper tool (like median calculation) and update your schemas and map."
        },
        {
          "cell_type": "code",
          "source": "# Example: Add a median helper\n# def compute_median(data):\n#     import numpy as np\n#     return {'median': float(np.median(data))}\n# # Add to tool_function_map and schemas as in Lesson 6\n"
        },
        {
          "cell_type": "markdown",
          "source": "---\n\n## 3. Reflect and Analyze\n\n- How well did your reviewer spot issues and suggest improvements?\n- Did tool-calling help the LLM produce more rigorous reviews?\n- What could you add or improve for the next version?\n\n---\n\n## Practice Exercise üìù\n\n**Challenge:**\n- Run your reviewer on a new paper or section\n- Tweak a prompt or add a helper tool\n- Summarize your findings and next steps below:"
        },
        {
          "cell_type": "code",
          "source": "# Your code here\n# Run, experiment, and reflect\n"
        }
      ]
    }
  ]
}
