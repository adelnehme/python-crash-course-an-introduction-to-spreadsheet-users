{
    "cell_type": "markdown",
    "source": [
      "# Building an AI Research Paper Reviewer with OpenAI’s o4-mini: Reasoning, Prompt Engineering, and Tool Use\n",
      "\n",
      "---\n",
      "\n",
      "## Course Summary\n",
      "In this hands-on course, you'll learn how to leverage OpenAI's o4-mini reasoning model to build a practical research paper reviewer in Python. You'll develop skills in prompt engineering for reasoning models, integrate function calling for statistical analysis, and apply these concepts in a real-world capstone project. By the end, you'll understand how to combine LLM reasoning, custom tools, and prompt design to create robust AI-powered applications.\n",
      "\n",
      "---\n",
      "\n",
      "## Lesson 1: Introduction to Reasoning Models and the o4-mini API\n",
      "\n",
      "- **Lesson Format:** Video Only\n",
      "- **Lesson Goal:** Understand what reasoning models are, the unique capabilities of o4-mini, and when to choose this model for your projects.\n",
      "\n",
      "### Video Script\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 1: Introduction to Reasoning Models and the o4-mini API\")**\n",
      "\n",
      "**Host:** Hi everyone, and welcome to our course on building an AI Research Paper Reviewer with OpenAI’s o4-mini! I’m excited to guide you through this journey.\n",
      "\n",
      "**(Visual: Animated graphic showing \"AI\" transforming into a \"thinking brain\" icon.)**\n",
      "\n",
      "**Host:** In this first lesson, we're going to lay the groundwork. We'll explore what \"reasoning models\" are, dive into the specifics of OpenAI's o4-mini, and get a sneak peek at the cool project we'll be building together.\n",
      "\n",
      "**(Visual: Slide: \"What are Reasoning Models?\")**\n",
      "**(Text on slide: - Go beyond pattern matching. - Perform multi-step problem solving. - Understand context and nuance. - Examples: Math, logic puzzles, code generation, scientific analysis.)**\n",
      "\n",
      "**Host:** So, what exactly are reasoning models? Think of them as the next step up from standard language models. While many AI models are great at recognizing patterns or generating text that *sounds* human, reasoning models are designed to *think* more like humans. They can tackle problems that require multiple steps, understand deeper context, and even work through logic puzzles or complex mathematical problems.\n",
      "\n",
      "**Host:** Imagine you're trying to solve a complex puzzle. You wouldn't just guess randomly, right? You'd break it down, consider different angles, and follow a logical path. Reasoning models aim to do something similar. They can analyze information, draw inferences, and make decisions based on that analysis. This makes them incredibly powerful for tasks like scientific discovery, in-depth text analysis, and even advanced coding assistance.\n",
      "\n",
      "**(Visual: Slide: \"Introducing o4-mini\")**\n",
      "**(Text on slide: - Part of OpenAI's \"o-series\" - Strengths: Cost-effective, High-performance reasoning, Math & Coding, Low-latency - Ideal for: Analysis, Demos, Experiments, Research tools)**\n",
      "\n",
      "**Host:** Now, let's talk about o4-mini. It's one of OpenAI's latest models in their \"o-series,\" which are specifically focused on reasoning capabilities. Even though it's \"mini,\" don't let the name fool you! O4-mini packs a punch.\n",
      "\n",
      "**Host:** Its key strengths? First, it's cost-effective, making it accessible for a wider range of projects. Second, it delivers high-performance reasoning, especially in areas like math, coding, and scientific analysis. It's fast, offering low-latency outputs, which is crucial when you need quick results for high-volume tasks.\n",
      "\n",
      "**(Visual: Infographic comparing o4-mini to a \"Swiss Army Knife\" - versatile, compact, powerful.)**\n",
      "\n",
      "**Host:** Think of o4-mini as a versatile and efficient tool in your AI toolkit. It's perfect for tasks that require deep analysis but also need to be budget-friendly and quick. This could be anything from powering a sophisticated chatbot that needs to understand complex queries, to generating code explanations, or, as we'll see, reviewing research papers.\n",
      "\n",
      "**Host:** Of course, like any tool, it has its context. While powerful, it's a \"mini\" model, so for extremely large-scale, deeply nuanced tasks where computational power is no object, larger models in the o-series or GPT family might be considered. But for a vast array of practical applications, o4-mini hits a sweet spot of capability and efficiency.\n",
      "\n",
      "**(Visual: Slide: \"Project Overview: AI Research Paper Reviewer\")**\n",
      "**(Text on slide: - Goal: Build a tool to analyze research papers. - Features: Extract text from PDFs, Identify weak arguments, unsupported claims, flawed methodology, Suggest improvements, Use statistical tools for verification.)**\n",
      "\n",
      "**Host:** So, what are we building in this course? We're going to create an AI Research Paper Reviewer! This tool will take a research paper, typically in PDF format, and use o4-mini's reasoning abilities to critically analyze it.\n",
      "\n",
      "**Host:** Our reviewer will:\n",
      "1.  Extract the content from the paper.\n",
      "2.  Use o4-mini to meticulously scan for things like weak arguments, claims that aren't backed by evidence, or potential flaws in the research methodology.\n",
      "3.  And importantly, it won't just criticize; it will also suggest improvements and provide a structured conclusion.\n",
      "\n",
      "**(Visual: Mockup of the final project output: a structured review with highlighted points and suggestions.)**\n",
      "\n",
      "**Host:** What's really exciting is that we'll enhance our reviewer by giving it \"tools\" – specifically, statistical tools. This means our AI won't just take the paper's statistical claims at face value. It will be able to, for example, ask for a p-value to be recalculated or an effect size to be computed, making its review much more rigorous and data-backed.\n",
      "\n",
      "**Host:** By the end of this course, you'll have a working Python application and a much deeper understanding of how to harness reasoning models like o4-mini, engineer effective prompts, and integrate external tools to build truly intelligent applications.\n",
      "\n",
      "**(Visual: Quick montage: code snippets, OpenAI logo, gears turning, a lightbulb.)**\n",
      "\n",
      "**Host:** This project will cover everything from setting up your environment and making your first API calls, to advanced prompt engineering, function calling, and structuring a complete application.\n",
      "\n",
      "**Host:** I hope you're as excited as I am! In the next lesson, we'll get our hands dirty by setting up our development environment and getting API access. See you there!\n",
      "\n",
      "**(Outro Music with End Card: \"Next Lesson: Setting Up Your Environment\")**\n",
      "\n",
      "### Suggested Images and Visuals:\n",
      "*   **Opening:** Course title card.\n",
      "*   **Reasoning Models:**\n",
      "    *   Animated graphic: \"AI\" icon transforming into a \"thinking brain\" or \"gears turning\" icon.\n",
      "    *   Slide with bullet points: \"What are Reasoning Models?\"\n",
      "    *   Iconography for use cases: a calculator for math, a lightbulb for logic, code brackets for coding, a microscope for scientific analysis.\n",
      "*   **o4-mini:**\n",
      "    *   Slide with bullet points: \"Introducing o4-mini\" and its strengths.\n",
      "    *   Infographic: o4-mini as a \"Swiss Army Knife\" – compact, versatile, powerful.\n",
      "    *   Comparison graphic (optional): o4-mini positioned between smaller utility models and larger flagship models regarding cost/performance for reasoning tasks.\n",
      "*   **Project Overview:**\n",
      "    *   Slide with bullet points: \"Project Overview: AI Research Paper Reviewer\" and its features.\n",
      "    *   Visual flow diagram: PDF input -> Text Extraction -> o4-mini Analysis (with tool use) -> Structured Review Output.\n",
      "    *   Mockup of the final project's output (a text-based review with clear sections).\n",
      "*   **Excitement/Teaser:**\n",
      "    *   Quick montage: Python logo, OpenAI logo, code snippets scrolling, gears turning, a lightbulb icon.\n",
      "*   **Closing:** Next lesson title card.\n",
      "\n",
      "---\n",
      "\n",
      "## Lesson 2: Setting Up Your Environment and Accessing the API\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Get hands-on with obtaining OpenAI API access, installing necessary dependencies, and securely configuring your environment for development.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 2: Setting Up Your Environment and Accessing the API\")**\n",
      "\n",
      "**Host:** Welcome back! In Lesson 1, we learned about reasoning models and got an overview of o4-mini and our exciting project. Now, it's time to roll up our sleeves and get our development environment ready.\n",
      "\n",
      "**(Visual: Checklist graphic: Get API Key, Install Libraries, Set Environment Variable.)**\n",
      "\n",
      "**Host:** In this lesson, we'll cover three key steps. First, I'll show you how to get an API key from OpenAI – this is your ticket to using o4-mini. Second, we'll install all the necessary Python libraries that our project will depend on. And third, a crucial step: we'll learn how to set up an environment variable to store your API key securely.\n",
      "\n",
      "**(Visual: Screen recording mock-up of the OpenAI API key page.)**\n",
      "\n",
      "**Host:** Getting your API key is straightforward. You'll need an OpenAI account. Once you're logged in, you can navigate to the API key section and generate a new secret key. Remember, this key is like a password – keep it safe and don't share it publicly! Also, ensure you have some credits in your OpenAI account, as API usage is typically billed.\n",
      "\n",
      "**(Visual: Logos of Python, pip, PyMuPDF, Tiktoken, NumPy, SciPy.)**\n",
      "\n",
      "**Host:** Next up, libraries. We'll be using a few powerful Python packages. `openai` is, of course, for interacting with the OpenAI API. `PyMuPDF` will help us extract text from PDF files. `tiktoken` is OpenAI's library for tokenizing text, which is important for managing input to the model. And `numpy` and `scipy` will come in handy for our statistical helper functions later on. We'll install these using pip.\n",
      "\n",
      "**(Visual: Code terminal showing `pip install ...` command.)**\n",
      "\n",
      "**Host:** Finally, security. Hardcoding your API key directly into your script is a big no-no. It's risky! Instead, we'll use an environment variable. This way, your key is kept separate from your code, making your application more secure and easier to manage, especially if you share your code or deploy it.\n",
      "\n",
      "**Host:** After this video, you'll jump into the notebook where we'll walk through each of these steps with code. You'll install the libraries and practice setting and retrieving your API key. By the end, you'll have a perfectly configured environment, ready for us to start building our research paper reviewer.\n",
      "\n",
      "**Host:** Let's get started! Head over to the notebook.\n",
      "\n",
      "**(Outro Music with End Card: \"Time for the Notebook!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Introduction**\n",
      "Welcome to the practical part of Lesson 2! Here, we'll set up everything you need to start building our AI Research Paper Reviewer. We'll cover:\n",
      "1.  Obtaining your OpenAI API Key.\n",
      "2.  Installing the required Python libraries.\n",
      "3.  Securely managing your API key using environment variables.\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Obtaining Your OpenAI API Key**\n",
      "\n",
      "To interact with o4-mini (or any OpenAI model via API), you need an API key.\n",
      "\n",
      "*   **Step 1:** Go to the OpenAI platform: [https://platform.openai.com/](https://platform.openai.com/)\n",
      "*   **Step 2:** Log in with your OpenAI account. If you don't have one, you'll need to create it.\n",
      "*   **Step 3:** Navigate to the API keys section. This is usually found under your account settings or a dedicated \"API Keys\" link (e.g., [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)).\n",
      "*   **Step 4:** Click on \"Create new secret key.\" You can give it a name, like \"o4-mini-project\".\n",
      "*   **Step 5:** **Crucially, copy the key immediately and save it somewhere secure.** You won't be able to see it again after you close the dialog.\n",
      "*   **Step 6 (Billing):** Ensure you have set up billing and have credits in your OpenAI account. API usage is not free. You can check this in the [billing section](https://platform.openai.com/account/billing/overview).\n",
      "\n",
      "**Important:** Treat your API key like a password. Do not share it, and do not commit it to version control (like Git).\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. Installing Dependencies**\n",
      "\n",
      "Our project relies on several Python libraries. We'll install them using `pip`.\n",
      "\n",
      "*   `openai`: The official Python library for the OpenAI API.\n",
      "*   `PyMuPDF`: A library to extract text and images from PDF documents. We'll use it for reading research papers.\n",
      "*   `tiktoken`: OpenAI's tokenizer library. It helps us count tokens and prepare text for the model, ensuring we don't exceed context limits.\n",
      "*   `numpy`: A fundamental package for numerical computation in Python. Useful for our statistical functions.\n",
      "*   `scipy`: A library that provides many user-friendly and efficient numerical routines, such as those for statistics.\n",
      "\n",
      "You can install these by running the following command in your terminal or a code cell in your notebook environment (prefix with `!` if in a Jupyter-like notebook cell):\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: Install libraries\n",
      "# It's recommended to run this in your terminal.\n",
      "# If running in a notebook, uncomment the line below.\n",
      "# !pip install openai PyMuPDF tiktoken numpy scipy\n",
      "\n",
      "# For this course, we'll assume you've run this in your terminal.\n",
      "# If you haven't, please do so now.\n",
      "print(\"Libraries to install: openai, PyMuPDF, tiktoken, numpy, scipy\")\n",
      "print(\"Please ensure these are installed in your Python environment.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "---\n",
      "\n",
      "#### **3. Setting Environment Variables for Secure API Access**\n",
      "\n",
      "Hardcoding your API key directly into your scripts is a security risk. A much safer way is to use environment variables. Your script can then read the key from the environment.\n",
      "\n",
      "**How to set an environment variable:**\n",
      "\n",
      "*   **Linux/macOS (in terminal):**\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "export OPENAI_API_KEY=\"YOUR_KEY_HERE\"\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "    To make this permanent, add this line to your shell's configuration file (e.g., `.bashrc`, `.zshrc`).\n",
      "\n",
      "*   **Windows (in Command Prompt):**\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "set OPENAI_API_KEY=YOUR_KEY_HERE\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "    For PowerShell:\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "$Env:OPENAI_API_KEY=\"YOUR_KEY_HERE\"\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "    To set it permanently on Windows, search for \"environment variables\" in the Start menu and add it through the system properties dialog.\n",
      "\n",
      "*   **Within a Python script or notebook (for the current session only, less secure for shared environments but useful for development):**\n",
      "    We'll use the `os` module for this.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: Setting and checking the OPENAI_API_KEY\n",
      "import os\n",
      "\n",
      "# Ideally, you should set this in your system's environment variables.\n",
      "# If you haven't, you can set it here for this session.\n",
      "# IMPORTANT: Replace \"YOUR_API_KEY_HERE\" with your actual key if you use this method.\n",
      "# However, it's best practice NOT to put your actual key in a saved notebook.\n",
      "# For this exercise, we'll try to read it, assuming it's already set.\n",
      "\n",
      "# Attempt to get the API key from an environment variable\n",
      "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "\n",
      "if api_key:\n",
      "    print(\"OPENAI_API_KEY found in environment variables.\")\n",
      "    # For security, we won't print the key itself, just a part of it or its presence.\n",
      "    # print(f\"API Key starts with: {api_key[:5]}...\") # Uncomment to verify locally\n",
      "else:\n",
      "    print(\"OPENAI_API_KEY not found in environment variables.\")\n",
      "    print(\"Please set it up as described above.\")\n",
      "    print(\"For example, in your terminal: export OPENAI_API_KEY='your_key_here'\")\n",
      "    print(\"Or, for this session (less secure, do not save the key in the notebook):\")\n",
      "    print(\"# os.environ['OPENAI_API_KEY'] = 'YOUR_ACTUAL_KEY_HERE'\")\n",
      "    # os.environ['OPENAI_API_KEY'] = 'sk-...' # Replace and uncomment if you must do it this way for testing\n",
      "\n",
      "# You'll need this for subsequent lessons. Let's ensure the openai client can be initialized.\n",
      "# We'll instantiate the client in later lessons when we make API calls.\n",
      "try:\n",
      "    from openai import OpenAI\n",
      "    # This would be the typical way to initialize the client.\n",
      "    # The client automatically picks up the OPENAI_API_KEY from the environment.\n",
      "    # client = OpenAI()\n",
      "    print(\"OpenAI library is installed and client can be initialized (conceptually).\")\n",
      "except ImportError:\n",
      "    print(\"OpenAI library not found. Please install it.\")\n",
      "except Exception as e:\n",
      "    print(f\"An error occurred: {e}\")\n",
      "    print(\"This might be due to a missing API key or other configuration issues.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanations:**\n",
      "\n",
      "*   **Why secure environment variables matter:**\n",
      "    *   **Security:** Prevents accidental exposure of your secret keys if your code is shared, committed to a public repository, or accessed by unauthorized users.\n",
      "    *   **Configuration Management:** Allows different configurations for different environments (development, testing, production) without changing the code. For instance, you might use a different API key or settings for testing.\n",
      "    *   **Collaboration:** Team members can use their own API keys without modifying a shared codebase.\n",
      "\n",
      "*   **Brief intro to each library’s role in the project:**\n",
      "    *   `openai`: This is our bridge to OpenAI's models. We'll use it to send requests to o4-mini and receive its analysis.\n",
      "    *   `PyMuPDF` (imported as `fitz`): Think of this as our PDF specialist. Research papers are often in PDF format, and PyMuPDF will help us \"read\" the text content from these files.\n",
      "    *   `tiktoken`: Language models have a limit on how much text they can process at once (context window), measured in \"tokens.\" Tiktoken helps us count tokens accurately for models like o4-mini and break down large texts into manageable chunks.\n",
      "    *   `numpy`: The workhorse for numerical operations. When we implement statistical tools (like calculating means, standard deviations), NumPy will provide efficient array operations.\n",
      "    *   `scipy`: Builds on NumPy and provides more advanced scientific and technical computing capabilities, including statistical functions like t-tests, which we'll use to verify claims in papers.\n",
      "\n",
      "---\n",
      "\n",
      "**Next Steps:**\n",
      "With your environment set up, API key secured, and libraries installed, you're ready to move on to Lesson 3, where we'll dive into the art and science of prompt engineering for reasoning models!\n",
      "\n",
      "---\n",
      "## Lesson 3: Fundamentals of Prompt Engineering for Reasoning Models\n",
      "\n",
      "- **Lesson Format:** Video Only\n",
      "- **Lesson Goal:** Learn the principles of crafting effective prompts for reasoning models and how prompt design influences model behavior.\n",
      "\n",
      "### Video Script\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 3: Fundamentals of Prompt Engineering for Reasoning Models\")**\n",
      "\n",
      "**Host:** Hello and welcome back! In our last lesson, we successfully set up our development environment. Now, we're ready to dive into a crucial aspect of working with any Large Language Model, especially reasoning models like o4-mini: Prompt Engineering.\n",
      "\n",
      "**(Visual: A graphic depicting a key turning a lock, with the key labeled \"Prompt\" and the lock labeled \"LLM Output\".)**\n",
      "\n",
      "**Host:** So, what is prompt engineering? At its core, it's the art and science of crafting effective inputs – or \"prompts\" – to guide an LLM towards producing the desired output. Think of it like giving very clear and specific instructions to a brilliant but very literal assistant. The better your instructions, the better the results.\n",
      "\n",
      "**(Visual: Slide: \"What is Prompt Engineering?\")**\n",
      "**(Text on slide: - Crafting effective inputs (prompts). - Guiding the LLM's behavior and output. - More than just asking a question. - Involves: Clarity, Context, Constraints, Role-playing.)**\n",
      "\n",
      "**Host:** For reasoning models, prompt engineering is even more critical. These models can perform complex tasks, but they need precise guidance to unlock their full potential. It's not just about asking a question; it's about structuring your request in a way that helps the model \"think\" through the problem.\n",
      "\n",
      "**(Visual: Slide: \"Prompt Structure for Reasoning Tasks\")**\n",
      "**(Text on slide: 1. Role Definition, 2. Context/Input Data, 3. Task/Instruction, 4. Output Format (Optional but Recommended), 5. Constraints/Examples (Few-shot))**\n",
      "\n",
      "**Host:** Let's break down the typical structure of a good prompt for a reasoning task:\n",
      "\n",
      "**Host:** **One: Role Definition.** Tell the model *who* it should be. For example, instead of just asking it to \"review this text,\" you might say, \"You are an expert AI research reviewer.\" This sets the stage and influences its tone and focus.\n",
      "\n",
      "**(Visual: Icon of a graduation cap or a detective's magnifying glass for \"expert reviewer\".)**\n",
      "\n",
      "**Host:** **Two: Context and Input Data.** Provide all the necessary information the model needs. This could be the research paper abstract, a specific claim to verify, or, as in our project, a chunk of the paper's text.\n",
      "\n",
      "**Host:** **Three: The Task or Instruction.** This is where you clearly state what you want the model to do. Be specific. Instead of \"find issues,\" try \"Identify weak arguments, unsupported claims, or flawed methodology in the provided text.\"\n",
      "\n",
      "**(Visual: Checklist icon for \"Task/Instruction\".)**\n",
      "\n",
      "**Host:** **Four: Output Format (Optional but Recommended).** If you need the output in a particular structure (like a list, JSON, or specific headings), tell the model. For example, \"Provide your feedback as a list of critical points, followed by a summary of suggestions.\" This makes the output much easier to parse and use programmatically.\n",
      "\n",
      "**Host:** **Five: Constraints and Examples (Few-shot prompting).** Sometimes, you might want to add constraints, like \"Be concise\" or \"Focus only on statistical errors.\" You can also provide a few examples of the kind of input and desired output you expect (this is called few-shot prompting) to further guide the model. For o4-mini and its reasoning capabilities, often a clear zero-shot prompt (no examples) with good role definition and task instruction is very effective.\n",
      "\n",
      "**(Visual: Slide: \"Guiding o4-mini for Detailed, Critical Analysis\")**\n",
      "**(Text on slide: - System Prompt: \"You are an expert AI research reviewer...\" - Specific Instructions: \"Highlight weak arguments, unsupported claims, or flawed methodology.\" - Encourage Tool Use: \"You can request tools to: Recalculate p-values...\" - Reasoning Effort: `reasoning={\"effort\": \"high\"}` (API parameter))**\n",
      "\n",
      "**Host:** Now, how do we apply this to our research paper reviewer and o4-mini?\n",
      "For our project, we'll use a \"system prompt.\" This is a high-level instruction that sets the overall behavior of the model for the entire conversation. We'll tell o4-mini: \"You are an expert AI research reviewer. Read the given chunk of a research paper and highlight weak arguments, unsupported claims, or flawed methodology.\"\n",
      "\n",
      "**Host:** We'll also explicitly tell it about the tools it can use: \"You can request tools to: Recalculate p-values, Compute confidence intervals, Estimate effect size (Cohen's d), Describe sample statistics.\" This empowers the model to not just critique subjectively but to seek objective verification.\n",
      "\n",
      "**Host:** And a neat feature with some OpenAI reasoning APIs, like the one we'll be using for o4-mini, is the ability to specify reasoning effort. We'll use a setting like `reasoning={\"effort\": \"high\"}`. This encourages the model to perform more thorough, multi-step internal reasoning before giving an answer, leading to more insightful analysis.\n",
      "\n",
      "**(Visual: Analogy: A chef being told \"take your time to make this perfect\" vs \"whip something up quickly\".)**\n",
      "\n",
      "**Host:** Think of it like telling a chef, \"Take your time and use all your skills to make this dish perfect,\" versus, \"Just whip something up quickly.\" The \"high effort\" setting aims for that more considered, deeper analysis.\n",
      "\n",
      "**(Visual: Slide: \"Common Pitfalls & Best Practices\")**\n",
      "**(Text on slide: Pitfalls: - Vague prompts, - Ambiguity, - Too much/too little context, - Assuming prior knowledge. Best Practices: - Be specific & clear, - Iterate and test, - Use delimiters for input, - Define the persona/role, - Specify output structure.)**\n",
      "\n",
      "**Host:** Let's touch on some common pitfalls and best practices.\n",
      "**Pitfalls to avoid:**\n",
      "*   **Vague Prompts:** \"Analyze this paper\" is too broad. What kind of analysis?\n",
      "*   **Ambiguity:** If your prompt can be interpreted in multiple ways, the model might pick the one you didn't intend.\n",
      "*   **Too Much or Too Little Context:** Overwhelming the model with irrelevant information or not giving it enough can lead to poor results. This is why we'll be chunking the paper.\n",
      "*   **Assuming Prior Knowledge:** While models are trained on vast data, don't assume they know specific, niche details unless provided.\n",
      "\n",
      "**Host:** **Best Practices to embrace:**\n",
      "*   **Be Specific and Clear:** Leave no room for misinterpretation.\n",
      "*   **Iterate and Test:** Prompt engineering is often an iterative process. Try a prompt, see the result, refine, and try again.\n",
      "*   **Use Delimiters:** When providing text for analysis, clearly separate it from your instructions using delimiters like triple quotes (`\"\"\"text\"\"\"`) or XML tags.\n",
      "*   **Define the Persona/Role:** As we discussed, this really helps.\n",
      "*   **Specify Output Structure:** If you need structured data, ask for it.\n",
      "\n",
      "**(Visual: A scientist in a lab, carefully adjusting dials and observing results – analogy for iterative prompt testing.)**\n",
      "\n",
      "**Host:** Prompt engineering is a skill that develops with practice. As we build our project, you'll see these principles in action, especially when we define the system prompt for o4-mini and how it interacts with the paper chunks and the tools.\n",
      "\n",
      "**Host:** That's a foundational look at prompt engineering. In our next lesson, we'll start building the \"tools\" for our AI reviewer – the statistical helper functions. This is where the reasoning capabilities of o4-mini will really start to shine! See you then.\n",
      "\n",
      "**(Outro Music with End Card: \"Next Lesson: Integrating Function Calling—Building Statistical Tool Helpers\")**\n",
      "\n",
      "### Suggested Images and Visuals:\n",
      "*   **Opening:** Course title card.\n",
      "*   **What is Prompt Engineering?:**\n",
      "    *   Graphic: A key (\"Prompt\") turning a lock (\"LLM Output\").\n",
      "    *   Slide with bullet points defining prompt engineering.\n",
      "    *   Icons for clarity, context, constraints, role-playing.\n",
      "*   **Prompt Structure:**\n",
      "    *   Slide listing the 5 components of prompt structure.\n",
      "    *   Visual icons for each component (e.g., graduation cap for role, document for context, checklist for task, code brackets for output format, gears/examples for constraints).\n",
      "*   **Guiding o4-mini:**\n",
      "    *   Slide detailing specific prompts for o4-mini in the project.\n",
      "    *   Visual representation of \"System Prompt\" vs \"User Prompt\".\n",
      "    *   Graphic illustrating \"reasoning effort: high\" – perhaps a brain with more connections lighting up.\n",
      "    *   Analogy: Chef taking time vs. rushing.\n",
      "*   **Pitfalls & Best Practices:**\n",
      "    *   Slide with two columns: \"Pitfalls\" (with a warning icon) and \"Best Practices\" (with a checkmark icon).\n",
      "    *   Simple icons for each pitfall/best practice.\n",
      "*   **Iterative Process:**\n",
      "    *   Visual: A scientist in a lab, carefully adjusting settings and observing results, symbolizing iterative prompt refinement.\n",
      "*   **Closing:** Next lesson title card.\n",
      "\n",
      "---\n",
      "## Lesson 4: Integrating Function Calling—Building Statistical Tool Helpers\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Implement and understand Python functions for statistical analysis, and prepare them for function calling by the LLM.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 4: Integrating Function Calling—Building Statistical Tool Helpers\")**\n",
      "\n",
      "**Host:** Welcome to Lesson 4! We've covered the basics of o4-mini and the art of prompt engineering. Now, we're going to give our AI reviewer some real power by integrating \"function calling,\" also known as \"tool use.\"\n",
      "\n",
      "**(Visual: Graphic of an LLM brain icon with smaller tool icons (calculator, graph) connecting to it.)**\n",
      "\n",
      "**Host:** Imagine our o4-mini model is like a very smart research assistant. While it can understand and analyze text incredibly well, it doesn't inherently know how to perform complex mathematical calculations or run statistical tests on raw data *itself*. That's where function calling comes in.\n",
      "\n",
      "**(Visual: Slide: \"Why Function Calling Enhances LLM Reasoning\")**\n",
      "**(Text on slide: - Extends LLM capabilities. - Access to real-time, accurate data/calculations. - Grounded & verifiable responses. - Allows interaction with external systems/APIs.)**\n",
      "\n",
      "**Host:** Function calling allows the LLM to pause its reasoning, request a specific external function (a tool that we define) to be run with certain parameters, and then receive the result of that function to continue its reasoning process. This is huge! It means our LLM isn't limited to its pre-trained knowledge. It can:\n",
      "*   Perform precise calculations.\n",
      "*   Access up-to-date information from external APIs.\n",
      "*   Verify claims by running its own checks.\n",
      "\n",
      "**Host:** For our research paper reviewer, this is crucial. A paper might claim a certain statistical significance (a p-value) or a particular effect size. With function calling, o4-mini can say, \"Hmm, let me double-check that,\" and then use a tool we provide to recalculate the p-value based on the data presented in the paper. This makes its review much more robust and trustworthy.\n",
      "\n",
      "**(Visual: Slide: \"Overview of Our Statistical Helpers\")**\n",
      "**(Text on slide: - `recalculate_p_value`: Validates statistical significance. - `compute_cohens_d`: Measures effect size. - `compute_confidence_interval`: Assesses reliability of estimates. - `describe_group`: Provides basic sample statistics (mean, std dev, n).)**\n",
      "\n",
      "**Host:** We'll be building a few key statistical helper functions in Python:\n",
      "1.  `recalculate_p_value`: This will perform a t-test to check if the difference between two groups is statistically significant.\n",
      "2.  `compute_cohens_d`: This calculates Cohen's d, a measure of effect size, telling us the magnitude of a difference, not just if it's statistically significant.\n",
      "3.  `compute_confidence_interval`: This helps understand the reliability of an estimated mean.\n",
      "4.  `describe_group`: A simpler function to get basic stats like mean, standard deviation, and sample size for a group of data.\n",
      "\n",
      "**(Visual: A diagram showing the LLM encountering a claim in a paper (e.g., \"p < 0.05\"). The LLM then makes a \"tool call\" to our `recalculate_p_value` function. The function (Python code) runs, returns the result, and the LLM incorporates this into its review.)**\n",
      "\n",
      "**Host:** How does the LLM interact with these tools? When we send a chunk of the paper to o4-mini, along with its prompt to be a critical reviewer, we'll also tell it about these available tools – what they're called, what they do, and what inputs they expect.\n",
      "\n",
      "**Host:** If o4-mini, during its analysis, encounters a statistical claim it wants to verify, or needs more information about some data, it will generate a special kind of message saying, \"I need to use the 'recalculate_p_value' tool with this data.\" Our Python code will detect this, run the actual `recalculate_p_value` function, and then send the result back to o4-mini. The model then takes this new information and continues its review, now better informed.\n",
      "\n",
      "**Host:** It’s like a dialogue: the LLM analyzes, asks for a tool to be used, gets the result, and then refines its analysis. This makes the AI much more than just a text summarizer; it becomes an active investigator.\n",
      "\n",
      "**Host:** In the notebook that follows, we'll dive into the Python code for each of these statistical helper functions. We'll use libraries like `SciPy` and `NumPy` to do the heavy lifting for the calculations. You'll see how they're implemented and understand the role each one plays in making our AI reviewer smarter and more reliable.\n",
      "\n",
      "**Host:** Ready to build some tools? Let's head over to the notebook!\n",
      "\n",
      "**(Outro Music with End Card: \"Time for the Notebook!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Introduction**\n",
      "In this lesson, we'll implement the Python functions that will serve as \"tools\" for our o4-mini model. These functions will perform statistical calculations, allowing the LLM to verify claims and gain deeper insights from the research paper's data. We'll be using `numpy` for numerical operations and `scipy.stats` for statistical tests.\n",
      "\n",
      "**Reminder:** Make sure you have `numpy` and `scipy` installed (`pip install numpy scipy`).\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Imports for Statistical Helpers**\n",
      "First, let's import the necessary modules from `scipy.stats` and `numpy`.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: Imports for statistical functions\n",
      "from scipy.stats import ttest_ind, sem, t\n",
      "import numpy as np\n",
      "\n",
      "print(\"NumPy and SciPy modules imported successfully.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanations of Imports:**\n",
      "*   `ttest_ind` from `scipy.stats`: This function is used to perform an independent two-sample t-test. It tests the null hypothesis that two independent samples have identical average (expected) values. We'll use Welch's t-test, which doesn't assume equal population variance.\n",
      "*   `sem` from `scipy.stats`: Calculates the standard error of the mean (SEM), which is a measure of the precision of the sample mean as an estimate of the population mean.\n",
      "*   `t` from `scipy.stats`: Represents the t-distribution. We'll use its `ppf` (percent point function, or inverse CDF) method to find the critical t-value for calculating confidence intervals.\n",
      "*   `numpy` (as `np`): Essential for numerical operations, especially array manipulation, mean, and standard deviation calculations.\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. Implementing Statistical Helper Functions**\n",
      "\n",
      "We will now define the four helper functions as described in the original tutorial. These functions will be called by our LLM when it needs to perform a specific statistical check.\n",
      "\n",
      "##### **2.1 `recalculate_p_value`**\n",
      "This function performs Welch’s t-test between two independent sample groups to determine if their means are statistically different.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: recalculate_p_value function\n",
      "def recalculate_p_value(group1, group2):\n",
      "    \"\"\"\n",
      "    Performs Welch’s t-test between two independent sample groups.\n",
      "    Tests the hypothesis that the means of the two groups are statistically different.\n",
      "\n",
      "    Args:\n",
      "        group1 (list or np.array): Data for the first group.\n",
      "        group2 (list or np.array): Data for the second group.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing the calculated p-value, rounded to 4 decimal places.\n",
      "    \"\"\"\n",
      "    # Ensure inputs are numpy arrays for robust calculations\n",
      "    group1 = np.array(group1)\n",
      "    group2 = np.array(group2)\n",
      "\n",
      "    # Perform Welch's t-test (equal_var=False)\n",
      "    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
      "    return {\"p_value\": round(p_value, 4)}\n",
      "\n",
      "# Example Usage:\n",
      "group_a = [2.5, 3.0, 2.8, 3.2, 2.9]\n",
      "group_b = [3.5, 3.8, 3.4, 3.9, 3.6]\n",
      "p_value_result = recalculate_p_value(group_a, group_b)\n",
      "print(f\"Example p-value calculation: {p_value_result}\")\n",
      "\n",
      "group_c = [2.5, 3.0, 2.8, 3.2, 2.9]\n",
      "group_d = [2.6, 3.1, 2.7, 3.3, 2.8] # More similar to group_c\n",
      "p_value_result_similar = recalculate_p_value(group_c, group_d)\n",
      "print(f\"Example p-value (similar groups): {p_value_result_similar}\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "**Conceptual Explanation:**\n",
      "*   **P-value:** The p-value helps determine the significance of results. A small p-value (typically ≤ 0.05) indicates strong evidence against the null hypothesis (which often states there's no difference or no effect). Our LLM can use this function to verify if a claimed difference between groups in a paper is statistically significant based on the provided data.\n",
      "*   **Welch's t-test:** Chosen because it doesn't assume that the two groups have equal variances, making it more robust for real-world data.\n",
      "\n",
      "---\n",
      "\n",
      "##### **2.2 `compute_cohens_d`**\n",
      "Cohen’s d is a standardized measure of effect size, indicating the magnitude of the difference between two group means.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 3: compute_cohens_d function\n",
      "def compute_cohens_d(group1, group2):\n",
      "    \"\"\"\n",
      "    Computes Cohen's d as a measure of effect size between two groups.\n",
      "\n",
      "    Args:\n",
      "        group1 (list or np.array): Data for the first group.\n",
      "        group2 (list or np.array): Data for the second group.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing the calculated Cohen's d value, rounded to 4 decimal places.\n",
      "    \"\"\"\n",
      "    group1 = np.array(group1)\n",
      "    group2 = np.array(group2)\n",
      "\n",
      "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
      "    # ddof=1 for sample standard deviation\n",
      "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
      "\n",
      "    # Calculate pooled standard deviation\n",
      "    # (n1-1)*std1**2 + (n2-1)*std2**2 / (n1+n2-2) is for pooled std when variances are assumed equal.\n",
      "    # For Cohen's d, a common approach when sample sizes are equal or nearly so,\n",
      "    # or when not pooling variances in the t-test (like Welch's), is to use sqrt((std1^2 + std2^2) / 2).\n",
      "    # This is the version used in the original tutorial.\n",
      "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
      "\n",
      "    # Handle case where pooled_std is zero to avoid division by zero\n",
      "    if pooled_std == 0:\n",
      "        return {\"cohens_d\": 0.0} # Or perhaps indicate an issue, like np.nan\n",
      "\n",
      "    d = (mean1 - mean2) / pooled_std\n",
      "    return {\"cohens_d\": round(d, 4)}\n",
      "\n",
      "# Example Usage:\n",
      "cohens_d_result = compute_cohens_d(group_a, group_b)\n",
      "print(f\"Example Cohen's d calculation: {cohens_d_result}\")\n",
      "\n",
      "cohens_d_result_similar = compute_cohens_d(group_c, group_d)\n",
      "print(f\"Example Cohen's d (similar groups): {cohens_d_result_similar}\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "**Conceptual Explanation:**\n",
      "*   **Effect Size (Cohen's d):** While a p-value tells you if an effect is statistically significant (i.e., unlikely due to chance), Cohen's d tells you the *size* of the effect.\n",
      "    *   Small effect: d ≈ 0.2\n",
      "    *   Medium effect: d ≈ 0.5\n",
      "    *   Large effect: d ≈ 0.8\n",
      "*   This is important because with very large sample sizes, even tiny, practically meaningless differences can become statistically significant. Cohen's d helps the LLM assess the *practical significance* or importance of a finding.\n",
      "\n",
      "---\n",
      "\n",
      "##### **2.3 `compute_confidence_interval`**\n",
      "This function computes the confidence interval for the mean of a single sample group, indicating the range within which the true population mean likely lies.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 4: compute_confidence_interval function\n",
      "def compute_confidence_interval(data, confidence=0.95):\n",
      "    \"\"\"\n",
      "    Computes the confidence interval for the mean of a single sample group.\n",
      "\n",
      "    Args:\n",
      "        data (list or np.array): The sample data.\n",
      "        confidence (float, optional): The desired confidence level (e.g., 0.95 for 95%). Defaults to 0.95.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing the sample mean, confidence interval [lower, upper], and confidence level.\n",
      "    \"\"\"\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n < 2: # Need at least 2 data points to calculate sem and CI meaningfully\n",
      "        return {\n",
      "            \"mean\": round(np.mean(data), 4) if n == 1 else np.nan,\n",
      "            \"confidence_interval\": [np.nan, np.nan],\n",
      "            \"confidence\": confidence,\n",
      "            \"note\": \"Not enough data points for CI calculation.\"\n",
      "        }\n",
      "\n",
      "    mean = np.mean(data)\n",
      "    standard_error_mean = sem(data)\n",
      "\n",
      "    # Calculate margin of error using the t-distribution\n",
      "    # t.ppf takes (1 + confidence) / 2 for a two-sided interval\n",
      "    # df (degrees of freedom) is n-1\n",
      "    margin_of_error = standard_error_mean * t.ppf((1 + confidence) / 2., n-1)\n",
      "\n",
      "    lower_bound = mean - margin_of_error\n",
      "    upper_bound = mean + margin_of_error\n",
      "\n",
      "    return {\n",
      "        \"mean\": round(mean, 4),\n",
      "        \"confidence_interval\": [round(lower_bound, 4), round(upper_bound, 4)],\n",
      "        \"confidence\": confidence\n",
      "    }\n",
      "\n",
      "# Example Usage:\n",
      "sample_data = [10, 12, 11.5, 13, 10.5, 12.5, 11]\n",
      "ci_result = compute_confidence_interval(sample_data)\n",
      "print(f\"Example 95% Confidence Interval calculation: {ci_result}\")\n",
      "\n",
      "ci_result_99 = compute_confidence_interval(sample_data, confidence=0.99)\n",
      "print(f\"Example 99% Confidence Interval calculation: {ci_result_99}\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "**Conceptual Explanation:**\n",
      "*   **Confidence Interval (CI):** A CI provides a range of plausible values for an unknown population parameter (like the mean) based on sample data. For example, a 95% CI for a mean suggests that if we were to repeat the experiment many times, 95% of the CIs calculated would contain the true population mean.\n",
      "*   It helps the LLM assess the precision and reliability of sample estimates reported in a paper. A very wide CI might indicate less certainty about the true value.\n",
      "\n",
      "---\n",
      "\n",
      "##### **2.4 `describe_group`**\n",
      "A simple helper to summarize a sample group with its mean, standard deviation, and sample size.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 5: describe_group function\n",
      "def describe_group(data):\n",
      "    \"\"\"\n",
      "    Summarizes a sample group with its mean, standard deviation, and sample size (n).\n",
      "\n",
      "    Args:\n",
      "        data (list or np.array): The sample data.\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing the mean, standard deviation (sample), and n.\n",
      "    \"\"\"\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "\n",
      "    if n == 0:\n",
      "        return {\n",
      "            \"mean\": np.nan,\n",
      "            \"std_dev\": np.nan,\n",
      "            \"n\": 0,\n",
      "            \"note\": \"Empty data array.\"\n",
      "        }\n",
      "    \n",
      "    mean = np.mean(data)\n",
      "    # ddof=1 for sample standard deviation (divides by N-1)\n",
      "    std_dev = np.std(data, ddof=1) if n > 1 else 0 # std dev of single point is 0 or undefined\n",
      "\n",
      "    return {\n",
      "        \"mean\": round(mean, 4),\n",
      "        \"std_dev\": round(std_dev, 4),\n",
      "        \"n\": n\n",
      "    }\n",
      "\n",
      "# Example Usage:\n",
      "group_stats = describe_group(sample_data)\n",
      "print(f\"Example group description: {group_stats}\")\n",
      "\n",
      "empty_stats = describe_group([])\n",
      "print(f\"Example empty group description: {empty_stats}\")\n",
      "\n",
      "single_point_stats = describe_group([5])\n",
      "print(f\"Example single point group description: {single_point_stats}\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "**Conceptual Explanation:**\n",
      "*   **Descriptive Statistics:** Mean (average), standard deviation (measure of data spread), and n (sample size) are fundamental descriptors of a dataset.\n",
      "*   This function allows the LLM to quickly get these basic stats for any group of numbers mentioned in the paper, which can be crucial context for interpreting other statistical results or claims. For example, a very small 'n' might make findings less reliable.\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. How These Tools Support Robust Paper Review**\n",
      "\n",
      "By providing these functions to o4-mini, we empower it to:\n",
      "*   **Verify Claims:** If a paper states \"Group A was significantly different from Group B (p < 0.01),\" the LLM can request the raw data (if available or inferable from text/tables) and use `recalculate_p_value` to check.\n",
      "*   **Assess Practical Significance:** Even if a result is statistically significant, the LLM can use `compute_cohens_d` to determine if the effect is large enough to be meaningful.\n",
      "*   **Evaluate Precision:** Using `compute_confidence_interval`, the LLM can assess the reliability of reported means or other estimates.\n",
      "*   **Gather Context:** `describe_group` provides immediate context about data distributions.\n",
      "\n",
      "This transforms the LLM from a passive reader into an active, analytical tool that can perform its own checks, leading to more insightful and trustworthy reviews.\n",
      "\n",
      "---\n",
      "\n",
      "**Saving these functions:**\n",
      "In a real project, you would save these functions in a separate Python file, for example, `statistics_helper.py`. Then, in your main script, you would import them like this:\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# from statistics_helper import (\n",
      "#     recalculate_p_value,\n",
      "#     compute_cohens_d,\n",
      "#     compute_confidence_interval,\n",
      "#     describe_group\n",
      "# )\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "For the subsequent lessons in this course, we will assume these functions are available as if imported from such a file.\n",
      "\n",
      "---\n",
      "**Next Steps:**\n",
      "Now that we have our statistical toolkit, the next step is to learn how to extract text from research papers (which are often PDFs) and prepare it for our LLM. That's what we'll cover in Lesson 5!\n",
      "\n",
      "---\n",
      "## Lesson 5: Extracting and Preparing Research Paper Content\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Learn to extract, tokenize, and chunk PDF content for LLM processing, handling real-world document limitations.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 5: Extracting and Preparing Research Paper Content\")**\n",
      "\n",
      "**Host:** Hi there! In Lesson 4, we built a set of powerful statistical tools for our AI reviewer. Now, before our o4-mini model can use these tools or even analyze a paper, it needs the paper's content! That's what this lesson is all about: getting the text out of research papers and preparing it for the LLM.\n",
      "\n",
      "**(Visual: Animation showing a PDF document icon transforming into clean text, then being cut into smaller pieces.)**\n",
      "\n",
      "**Host:** Research papers usually come as PDF files. PDFs are great for preserving layout, but extracting clean, usable text from them can sometimes be tricky. We'll be using a Python library called `PyMuPDF` (which you might import as `fitz`) to handle this. It's quite effective at pulling text out page by page.\n",
      "\n",
      "**(Visual: Slide: \"Challenge 1: Extracting Text from PDFs\")**\n",
      "**(Text on slide: - PDFs preserve layout, not always clean text flow. - `PyMuPDF` (fitz) library to the rescue! - Iterate through pages, extract text, combine.)**\n",
      "\n",
      "**Host:** Our first step will be to write a function that takes a PDF file path, opens the PDF, goes through each page, extracts the text content, and then combines all that text into a single string. This gives us the raw material for our AI to work with.\n",
      "\n",
      "**(Visual: Slide: \"Challenge 2: LLM Context Limits & Tokenization\")**\n",
      "**(Text on slide: - LLMs have input size limits (context window). - Measured in \"tokens,\" not characters or words. - `tiktoken` library for OpenAI tokenization. - Need to break large text into smaller, coherent chunks.)**\n",
      "\n",
      "**Host:** Once we have the raw text, we face another challenge: LLMs, including o4-mini, have a limit on how much text they can process at once. This is called the \"context window,\" and it's measured in \"tokens.\" Tokens are pieces of words; for example, \"eating\" might be one token, but \"unforgettable\" might be \"un-forget-able,\" three tokens.\n",
      "\n",
      "**Host:** A typical research paper is far too long to fit into the model's context window in one go. If we try to send it all, the API will give us an error, or the text will be truncated. So, we need to be smart about it. This is where \"tokenization\" and \"chunking\" come in.\n",
      "\n",
      "**Host:** We'll use OpenAI's `tiktoken` library. This library knows how models like o4-mini \"see\" text in terms of tokens. We'll write a function that first encodes the entire paper's text into tokens. Then, it will divide this long sequence of tokens into smaller \"chunks,\" each fitting within a specified maximum token limit (say, 12,000 tokens, which is a generous size for o4-mini but still requires chunking for full papers).\n",
      "\n",
      "**(Visual: Diagram showing a long scroll of text being tokenized, then cut into fixed-size token chunks. Each chunk is then decoded back to text.)**\n",
      "\n",
      "**Host:** Importantly, after creating these token chunks, we'll decode each chunk back into readable text. This ensures that our model receives coherent segments of the paper, maintaining as much of the natural language structure as possible. We don't want to just chop words in half randomly.\n",
      "\n",
      "**Host:** By chunking the document, we can feed the paper to o4-mini piece by piece. The model can review each chunk, and we can then aggregate these reviews later. This approach allows us to handle documents of virtually any length.\n",
      "\n",
      "**Host:** In the notebook, you'll implement two key functions: one to extract all text from a PDF, and another to take that text, tokenize it, and split it into manageable chunks suitable for o4-mini. This is a critical preprocessing step for many LLM applications that deal with long documents.\n",
      "\n",
      "**Host:** Ready to process some PDFs? Let's jump into the notebook!\n",
      "\n",
      "**(Outro Music with End Card: \"Time for the Notebook!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Introduction**\n",
      "Before our o4-mini model can review a research paper, we need to extract its textual content and prepare it in a format the model can process. This involves two main steps:\n",
      "1.  **Extracting Text from PDF:** Using `PyMuPDF` (imported as `fitz`) to get raw text from PDF files.\n",
      "2.  **Tokenizing and Chunking Text:** Using `tiktoken` to break down the extracted text into smaller, manageable pieces that fit within the LLM's context window.\n",
      "\n",
      "**Reminder:** Make sure you have `PyMuPDF` and `tiktoken` installed (`pip install PyMuPDF tiktoken`).\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Imports for Content Processing**\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: Imports for PDF extraction and tokenization\n",
      "import fitz  # PyMuPDF\n",
      "import tiktoken\n",
      "import os # For creating a dummy PDF later\n",
      "\n",
      "print(\"PyMuPDF (fitz) and tiktoken imported successfully.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "---\n",
      "\n",
      "#### **2. Extracting Text from PDF (`extract_text_from_pdf`)**\n",
      "\n",
      "We'll define a function that takes the path to a PDF file, opens it using `fitz`, iterates through each page, extracts the text, and concatenates it into a single string.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: Function to extract text from PDF\n",
      "def extract_text_from_pdf(pdf_path):\n",
      "    \"\"\"\n",
      "    Extracts all text content from a PDF file.\n",
      "\n",
      "    Args:\n",
      "        pdf_path (str): The file path to the PDF.\n",
      "\n",
      "    Returns:\n",
      "        str: A single string containing all extracted text from the PDF.\n",
      "             Returns an empty string if the PDF cannot be opened or is empty.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        doc = fitz.open(pdf_path)  # Open the PDF file\n",
      "    except Exception as e:\n",
      "        print(f\"Error opening PDF {pdf_path}: {e}\")\n",
      "        return \"\"\n",
      "\n",
      "    full_text = []\n",
      "    for page_num in range(len(doc)):\n",
      "        page = doc.load_page(page_num)  # Load one page\n",
      "        full_text.append(page.get_text(\"text\"))  # Extract text from page\n",
      "    \n",
      "    doc.close()\n",
      "    return \"\\n\".join(full_text) # Join text from all pages\n",
      "\n",
      "# --- Example Usage ---\n",
      "# For this example, let's create a dummy PDF to test the function.\n",
      "# In a real scenario, you would provide a path to an actual research paper PDF.\n",
      "\n",
      "# Create a dummy PDF file for testing (if it doesn't exist)\n",
      "dummy_pdf_path = \"dummy_paper.pdf\"\n",
      "if not os.path.exists(dummy_pdf_path):\n",
      "    try:\n",
      "        doc = fitz.open()  # Create a new empty PDF\n",
      "        page = doc.new_page()\n",
      "        page.insert_text((72, 72), \"This is the first page of our dummy research paper.\")\n",
      "        page.insert_text((72, 144), \"It discusses important findings about AI and LLMs.\")\n",
      "        page = doc.new_page()\n",
      "        page.insert_text((72, 72), \"The second page continues with more details and a conclusion.\")\n",
      "        page.insert_text((72, 100), \"The study found that LLMs are very useful.\")\n",
      "        doc.save(dummy_pdf_path)\n",
      "        doc.close()\n",
      "        print(f\"Dummy PDF '{dummy_pdf_path}' created for testing.\")\n",
      "    except Exception as e:\n",
      "        print(f\"Could not create dummy PDF: {e}\")\n",
      "        print(\"Please provide your own PDF for testing if this fails.\")\n",
      "\n",
      "if os.path.exists(dummy_pdf_path):\n",
      "    print(f\"\\nAttempting to extract text from '{dummy_pdf_path}'...\")\n",
      "    extracted_text = extract_text_from_pdf(dummy_pdf_path)\n",
      "    if extracted_text:\n",
      "        print(\"\\n--- Extracted Text (First 300 chars) ---\")\n",
      "        print(extracted_text[:300])\n",
      "        print(\"--- End of Snippet ---\")\n",
      "    else:\n",
      "        print(\"No text extracted or PDF was empty/invalid.\")\n",
      "else:\n",
      "    print(f\"\\n'{dummy_pdf_path}' not found. Skipping extraction example.\")\n",
      "    print(\"Please create a 'dummy_paper.pdf' or use another PDF for testing.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanation:**\n",
      "*   `fitz.open(pdf_path)`: Opens the specified PDF document.\n",
      "*   `doc.load_page(page_num)`: Loads a specific page from the document.\n",
      "*   `page.get_text(\"text\")`: Extracts plain text from the loaded page. `PyMuPDF` can also extract text with formatting information, but for LLM input, plain text is usually preferred.\n",
      "*   The text from each page is appended to a list, and finally, `\"\\n\".join(full_text)` combines them into one large string, with newlines separating the content from different pages.\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. Chunking Text (`chunk_text`)**\n",
      "\n",
      "LLMs have a context window limit (maximum number of tokens they can process at once). For o4-mini, while it has a relatively large context window compared to older models, research papers can easily exceed this. We need to break the text into manageable chunks.\n",
      "\n",
      "We'll use the `tiktoken` library to:\n",
      "1.  Get an encoding appropriate for o4-mini (e.g., `cl100k_base`, which is used by GPT-4 and related models).\n",
      "2.  Encode the full text into tokens.\n",
      "3.  Split the list of tokens into chunks of a `max_tokens` size.\n",
      "4.  Decode each token chunk back into text.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 3: Function to chunk text using tiktoken\n",
      "def chunk_text(text, max_tokens=12000, model_encoding=\"cl100k_base\"):\n",
      "    \"\"\"\n",
      "    Splits a long text into smaller chunks based on a maximum token limit.\n",
      "\n",
      "    Args:\n",
      "        text (str): The input text to be chunked.\n",
      "        max_tokens (int): The maximum number of tokens allowed per chunk.\n",
      "                          The tutorial mentions 12000 for o4-mini.\n",
      "        model_encoding (str): The encoding name used by the target model (e.g., \"cl100k_base\").\n",
      "\n",
      "    Returns:\n",
      "        list[str]: A list of text chunks.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        encoding = tiktoken.get_encoding(model_encoding)\n",
      "    except Exception as e:\n",
      "        print(f\"Error getting encoding '{model_encoding}': {e}\")\n",
      "        # Fallback or raise error\n",
      "        # For simplicity, let's try a common one if the specified fails, though cl100k_base should be standard.\n",
      "        try:\n",
      "            encoding = tiktoken.get_encoding(\"gpt2\") # A very common encoding\n",
      "            print(\"Fell back to 'gpt2' encoding. This might affect token counts for o4-mini.\")\n",
      "        except:\n",
      "            print(\"Could not load any tiktoken encoding. Chunking will be approximate based on characters.\")\n",
      "            # Simple character-based chunking as a last resort (not token-aware)\n",
      "            # This is NOT ideal for LLMs but better than nothing if tiktoken fails catastrophically.\n",
      "            avg_chars_per_token = 3 # Rough estimate\n",
      "            max_chars = max_tokens * avg_chars_per_token\n",
      "            return [text[i:i + max_chars] for i in range(0, len(text), max_chars)]\n",
      "\n",
      "\n",
      "    tokens = encoding.encode(text)\n",
      "    # print(f\"Total tokens in document: {len(tokens)}\") # For debugging\n",
      "\n",
      "    chunks = []\n",
      "    for i in range(0, len(tokens), max_tokens):\n",
      "        chunk_tokens = tokens[i:i + max_tokens]\n",
      "        chunk_text_content = encoding.decode(chunk_tokens)\n",
      "        chunks.append(chunk_text_content)\n",
      "    \n",
      "    return chunks\n",
      "\n",
      "# --- Example Usage ---\n",
      "# Use the extracted_text from the dummy PDF if available\n",
      "if 'extracted_text' in globals() and extracted_text:\n",
      "    print(f\"\\nOriginal text length (chars): {len(extracted_text)}\")\n",
      "    \n",
      "    # Let's use a smaller max_tokens for this example to ensure we get multiple chunks from the dummy text\n",
      "    example_max_tokens = 30 \n",
      "    text_chunks = chunk_text(extracted_text, max_tokens=example_max_tokens)\n",
      "    \n",
      "    print(f\"\\nNumber of chunks created (with max_tokens={example_max_tokens}): {len(text_chunks)}\")\n",
      "    for i, chunk in enumerate(text_chunks):\n",
      "        print(f\"\\n--- Chunk {i+1} (Tokens: ~{len(tiktoken.get_encoding('cl100k_base').encode(chunk))}) ---\")\n",
      "        print(chunk)\n",
      "        print(\"--- End of Chunk ---\")\n",
      "else:\n",
      "    print(\"\\nSkipping chunking example as extracted_text is not available.\")\n",
      "\n",
      "# Example with a longer generic text\n",
      "long_text_example = \"This is a very long string designed to test the chunking functionality. \" * 20\n",
      "long_text_example += \"It repeats over and over again to simulate a much larger document that would \" * 20\n",
      "long_text_example += \"definitely exceed the token limit of many models if not chunked properly. \" * 20\n",
      "long_text_example += \"The process involves tokenizing, splitting tokens, and then decoding back to text.\" *20\n",
      "\n",
      "print(f\"\\nOriginal long_text_example length (chars): {len(long_text_example)}\")\n",
      "example_max_tokens_long = 100 # A bit larger for this longer text\n",
      "text_chunks_long = chunk_text(long_text_example, max_tokens=example_max_tokens_long)\n",
      "print(f\"\\nNumber of chunks for long_text_example (max_tokens={example_max_tokens_long}): {len(text_chunks_long)}\")\n",
      "# print(f\"First chunk of long text: {text_chunks_long[0]}\") # Uncomment to see the first chunk\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanations:**\n",
      "*   **Why Chunking Matters for LLMs:**\n",
      "    *   **Context Window Limits:** LLMs can only process a finite amount of information at once. Sending too much text results in errors or truncation.\n",
      "    *   **Performance:** Processing extremely long texts can be slow and computationally expensive, even if they theoretically fit.\n",
      "    *   **Focus:** Smaller chunks can sometimes help the model focus its attention on specific parts of a document, though this depends on the task. For a comprehensive review, we need to ensure all content is processed.\n",
      "*   **Ensuring No Loss of Context or Text:**\n",
      "    *   Our `chunk_text` function processes the *entire* token list. It iterates from the beginning to the end, taking `max_tokens` at a time. This ensures all parts of the document are included in at least one chunk.\n",
      "    *   `encoding.decode(chunk_tokens)` is crucial. It converts the token IDs back into human-readable text. This preserves natural language structure within each chunk, making it easier for the LLM to understand compared to, say, just sending raw token IDs or arbitrarily splitting strings by character count.\n",
      "    *   The main challenge with chunking is maintaining context *across* chunks. For example, a sentence might be split between two chunks. While our current method doesn't implement sophisticated context-aware chunking (like overlapping chunks or sentence-boundary detection), it's a common and effective first step. For very nuanced tasks, more advanced chunking strategies can be employed, but for a general review, this approach is often sufficient.\n",
      "\n",
      "---\n",
      "\n",
      "**Next Steps:**\n",
      "We now know how to get text out of PDFs and prepare it in LLM-friendly chunks. In Lesson 6, we'll learn how to define the \"tools\" (our statistical functions from Lesson 4) in a way that o4-mini can understand and request them. This involves setting up tool mappings and defining tool schemas for the OpenAI API.\n",
      "\n",
      "---\n",
      "## Lesson 6: Mapping and Registering Tools for LLM Access\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Set up the mapping between LLM tool calls and Python functions, and define tool schemas for function calling.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 6: Mapping and Registering Tools for LLM Access\")**\n",
      "\n",
      "**Host:** Hello and welcome to Lesson 6! We've built our statistical helper functions and learned how to prepare research paper content. Now, how do we actually connect these two pieces? How does o4-mini know about our Python functions and how to ask for them to be run? That's where tool mapping and tool registration come in.\n",
      "\n",
      "**(Visual: A diagram showing the LLM on one side, Python functions on the other, and a \"Tool Schema / Map\" acting as a bridge in between.)**\n",
      "\n",
      "**Host:** Think of it like this: our LLM, o4-mini, speaks its own language of intent. When it decides it needs a calculation done, it will say something like, \"I need to use a tool called 'recalculate_p_value' with these specific inputs.\" Our Python code needs to understand this request and call the *actual* Python function `recalculate_p_value` that we wrote in Lesson 4.\n",
      "\n",
      "**(Visual: Slide: \"What is Tool Mapping?\")**\n",
      "**(Text on slide: - Problem: LLM refers to tools by name (e.g., \"recalculate_p_value\"). - Solution: A dictionary mapping tool names (strings) to actual Python functions. - `tool_function_map = { \"recalculate_p_value\": recalculate_p_value_function }`)**\n",
      "\n",
      "**Host:** The first part of this is **Tool Mapping**. It's quite simple: we'll create a Python dictionary. This dictionary will map the function names that the LLM will use (which are strings) to the actual Python function objects themselves. For example, the string `\"recalculate_p_value\"` will map to our Python function `recalculate_p_value`. This map acts as a directory, allowing our main program to look up and execute the correct Python code when the LLM requests a tool.\n",
      "\n",
      "**(Visual: Slide: \"Defining Tool Schemas for OpenAI API\")**\n",
      "**(Text on slide: - How the LLM \"knows\" about tools. - JSON Schema structure: `type`, `name`, `description`, `parameters`. - `description`: Tells LLM *what the tool does*. - `parameters`: Tells LLM *what inputs the tool needs* (name, type, required).)**\n",
      "\n",
      "**Host:** The second, and very important, part is **Defining Tool Schemas**. This is how we tell the LLM about the tools *before* it even starts its analysis. We need to provide a structured description for each tool, almost like a mini-API documentation for the LLM.\n",
      "\n",
      "**Host:** This schema is typically defined in a specific JSON-like structure that the OpenAI API understands. For each tool, we'll specify:\n",
      "*   `type`: Usually \"function\".\n",
      "*   `name`: The name of the function, like `\"recalculate_p_value\"`. This *must* match the name the LLM will use and the key in our `tool_function_map`.\n",
      "*   `description`: A clear, concise explanation of what the tool does. For example, \"Calculates the p-value between two sample groups.\" This is crucial because the LLM uses this description to decide *when* to use the tool.\n",
      "*   `parameters`: This defines the inputs the function expects. We'll specify the properties (arguments), their types (e.g., array of numbers, string, boolean), and which ones are required. For example, for `recalculate_p_value`, we'd specify `group1` and `group2` as required parameters, both being arrays of numbers.\n",
      "\n",
      "**(Visual: An example of a tool schema for `recalculate_p_value`, highlighting the `name`, `description`, and `parameters` sections.)**\n",
      "\n",
      "**Host:** By providing these schemas, the LLM doesn't just guess. It understands what tools are at its disposal, what they're for, and how to ask for them correctly with the right arguments. When the LLM decides to use a tool, it will format its request (a \"function call\") according to this schema.\n",
      "\n",
      "**Host:** So, the workflow is:\n",
      "1.  We define our Python helper functions (Lesson 4).\n",
      "2.  We create a `tool_function_map` to link string names to these functions.\n",
      "3.  We create a list of `tool schemas` to describe these functions to the LLM.\n",
      "4.  When we make an API call to o4-mini, we pass this list of tool schemas.\n",
      "5.  If o4-mini decides to use a tool, it sends back a structured request.\n",
      "6.  Our code uses the `tool_function_map` to find and run the actual Python function with the arguments provided by the LLM.\n",
      "7.  We then send the result back to the LLM to continue its work.\n",
      "\n",
      "**Host:** This mechanism is what enables powerful, reliable interactions where the LLM can leverage external code to augment its reasoning. In the notebook, we'll create the `tool_function_map` and then carefully define the JSON schemas for each of our statistical helper functions. This is a key step in bringing our AI reviewer to life!\n",
      "\n",
      "**Host:** Let's get to it!\n",
      "\n",
      "**(Outro Music with End Card: \"Time for the Notebook!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Introduction**\n",
      "In this lesson, we'll bridge the gap between our Python statistical helper functions (from Lesson 4) and the o4-mini LLM. We need to:\n",
      "1.  Create a **Tool Function Map**: A Python dictionary that maps the string names of functions (as the LLM will know them) to our actual Python function objects.\n",
      "2.  Define **Tool Schemas**: A list of structured descriptions (like JSON objects) for each tool. This tells the LLM what tools are available, what they do, and what parameters they expect.\n",
      "\n",
      "This setup is essential for the OpenAI API's function calling/tool use feature.\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Re-defining/Importing Helper Functions (Conceptual)**\n",
      "\n",
      "For this notebook, let's quickly redefine our helper functions or imagine we're importing them from `statistics_helper.py`. In a full application, these would be in a separate file.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: Statistical Helper Functions (from Lesson 4)\n",
      "# For brevity, we're including them directly here.\n",
      "# In a real project, you'd import them:\n",
      "# from statistics_helper import (\n",
      "#     recalculate_p_value, compute_cohens_d,\n",
      "#     compute_confidence_interval, describe_group\n",
      "# )\n",
      "\n",
      "from scipy.stats import ttest_ind, sem, t\n",
      "import numpy as np\n",
      "\n",
      "def recalculate_p_value(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) < 2 or len(group2) < 2: return {\"p_value\": np.nan, \"error\": \"Not enough data\"}\n",
      "    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
      "    return {\"p_value\": round(p_value, 4)}\n",
      "\n",
      "def compute_cohens_d(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) == 0 or len(group2) == 0: return {\"cohens_d\": np.nan, \"error\": \"Empty group data\"}\n",
      "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
      "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
      "    if std1 == 0 and std2 == 0 and mean1 == mean2 : return {\"cohens_d\": 0.0} # Identical constant groups\n",
      "    if std1 == 0 and std2 == 0 and mean1 != mean2 : return {\"cohens_d\": np.inf if mean1 > mean2 else -np.inf, \"warning\": \"Zero standard deviation in both groups with different means.\"}\n",
      "\n",
      "\n",
      "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
      "    if pooled_std == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else (np.inf if mean1 > mean2 else -np.inf), \"warning\":\"Pooled standard deviation is zero.\"}\n",
      "    d = (mean1 - mean2) / pooled_std\n",
      "    return {\"cohens_d\": round(d, 4)}\n",
      "\n",
      "def compute_confidence_interval(data, confidence=0.95):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n < 2: return {\"mean\": round(np.mean(data), 4) if n == 1 else np.nan, \"confidence_interval\": [np.nan, np.nan], \"confidence\": confidence, \"note\":\"Not enough data\"}\n",
      "    mean = np.mean(data)\n",
      "    standard_error_mean = sem(data)\n",
      "    if standard_error_mean == 0: # All data points are the same\n",
      "         return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean,4), round(mean,4)], \"confidence\": confidence, \"note\": \"Data points are identical.\"}\n",
      "    margin_of_error = standard_error_mean * t.ppf((1 + confidence) / 2., n-1)\n",
      "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence\": confidence}\n",
      "\n",
      "def describe_group(data):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n == 0: return {\"mean\": np.nan, \"std_dev\": np.nan, \"n\": 0}\n",
      "    mean = np.mean(data)\n",
      "    std_dev = np.std(data, ddof=1) if n > 1 else 0\n",
      "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4), \"n\": n}\n",
      "\n",
      "print(\"Statistical helper functions defined/imported.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "---\n",
      "\n",
      "#### **2. Tool Function Map (`tool_function_map`)**\n",
      "\n",
      "This dictionary maps the string name of a tool (which the LLM will use) to the actual Python function object.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: Create the tool_function_map\n",
      "tool_function_map = {\n",
      "    \"recalculate_p_value\": recalculate_p_value,\n",
      "    \"compute_cohens_d\": compute_cohens_d,\n",
      "    \"compute_confidence_interval\": compute_confidence_interval,\n",
      "    \"describe_group\": describe_group,\n",
      "}\n",
      "\n",
      "print(\"Tool function map created:\")\n",
      "for name, func in tool_function_map.items():\n",
      "    print(f\"- \\\"{name}\\\" maps to {func.__name__}\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "**Conceptual Explanation:**\n",
      "*   When the LLM returns a \"function call\" request, it will specify a `name` (e.g., `\"recalculate_p_value\"`).\n",
      "*   Our code will use this `name` as a key in `tool_function_map` to retrieve the corresponding Python function (`recalculate_p_value`).\n",
      "*   Then, it will execute this retrieved function with the arguments provided by the LLM.\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. Defining Tool Schemas (`tools`)**\n",
      "\n",
      "This is a list of dictionaries, where each dictionary describes one tool. The LLM uses these descriptions to understand what tools are available, what they do, and how to call them. The structure must follow OpenAI's specifications for tool definitions.\n",
      "\n",
      "Key fields for each tool's schema:\n",
      "*   `type`: Must be `\"function\"`.\n",
      "*   `name`: The name of the function. This is how the LLM will refer to it. **It must match a key in `tool_function_map`**.\n",
      "*   `description`: A natural language description of what the function does. The LLM uses this to decide *when* to call the function.\n",
      "*   `parameters`: An object describing the parameters the function accepts.\n",
      "    *   `type`: Must be `\"object\"`.\n",
      "    *   `properties`: An object where each key is a parameter name, and its value describes that parameter (e.g., its `type` like `\"array\"`, `\"number\"`, `\"string\"`, and an `items` field if it's an array to describe array elements).\n",
      "    *   `required`: A list of parameter names that are mandatory.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 3: Define the tool schemas\n",
      "tools = [\n",
      "    {\n",
      "        \"type\": \"function\",\n",
      "        \"name\": \"recalculate_p_value\",\n",
      "        \"description\": \"Calculate p-value between two independent sample groups using Welch's t-test. Use this to verify claims of statistical significance between two groups.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"group1\": {\n",
      "                    \"type\": \"array\",\n",
      "                    \"items\": {\"type\": \"number\"},\n",
      "                    \"description\": \"An array of numerical data for the first sample group.\"\n",
      "                },\n",
      "                \"group2\": {\n",
      "                    \"type\": \"array\",\n",
      "                    \"items\": {\"type\": \"number\"},\n",
      "                    \"description\": \"An array of numerical data for the second sample group.\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\"group1\", \"group2\"]\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\",\n",
      "        \"name\": \"compute_cohens_d\",\n",
      "        \"description\": \"Compute effect size (Cohen's d) between two groups. Use this to understand the magnitude of the difference between two group means.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"group1\": {\n",
      "                    \"type\": \"array\",\n",
      "                    \"items\": {\"type\": \"number\"},\n",
      "                    \"description\": \"An array of numerical data for the first sample group.\"\n",
      "                },\n",
      "                \"group2\": {\n",
      "                    \"type\": \"array\",\n",
      "                    \"items\": {\"type\": \"number\"},\n",
      "                    \"description\": \"An array of numerical data for the second sample group.\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\"group1\", \"group2\"]\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\",\n",
      "        \"name\": \"compute_confidence_interval\",\n",
      "        \"description\": \"Compute the confidence interval for the mean of a single sample group. Use this to assess the reliability of a sample mean.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"data\": {\n",
      "                    \"type\": \"array\",\n",
      "                    \"items\": {\"type\": \"number\"},\n",
      "                    \"description\": \"An array of numerical data for the sample group.\"\n",
      "                },\n",
      "                \"confidence\": {\n",
      "                    \"type\": \"number\",\n",
      "                    \"default\": 0.95,\n",
      "                    \"description\": \"The desired confidence level (e.g., 0.95 for 95%, 0.99 for 99%).\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\"data\"]\n",
      "        }\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\",\n",
      "        \"name\": \"describe_group\",\n",
      "        \"description\": \"Summarize a sample group with its mean, standard deviation, and sample size (n). Use this to get basic descriptive statistics for a dataset.\",\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {\n",
      "                \"data\": {\n",
      "                    \"type\": \"array\",\n",
      "                    \"items\": {\"type\": \"number\"},\n",
      "                    \"description\": \"An array of numerical data for the sample group.\"\n",
      "                }\n",
      "            },\n",
      "            \"required\": [\"data\"]\n",
      "        }\n",
      "    }\n",
      "]\n",
      "\n",
      "import json # For pretty printing\n",
      "print(\"Tool schemas defined:\")\n",
      "print(json.dumps(tools, indent=2))\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanations:**\n",
      "*   **How the LLM \"Knows\" What Tools Are Available:** When we make an API call to o4-mini (which we'll do in the next lesson), we will pass this `tools` list as part of the request. The LLM then parses these schemas to understand its available toolkit.\n",
      "*   **The `description` field is critical.** The LLM relies heavily on these natural language descriptions to determine *which tool is appropriate for a given situation* based on the text it's analyzing and the task it's been given. Clear, accurate, and informative descriptions are key to effective tool use.\n",
      "*   **Input Validation (Basic):** The `parameters` schema also helps the LLM formulate its function call requests correctly. It knows which arguments are `required` and their expected `type` (e.g., an array of numbers). While the LLM tries to adhere to this, your Python functions should still ideally include robust error handling for unexpected inputs, as the LLM might occasionally make mistakes, especially with complex data extraction from text. Our helper functions have some basic checks (e.g., for empty lists or insufficient data).\n",
      "\n",
      "---\n",
      "\n",
      "**Next Steps:**\n",
      "We've now defined our tools and made them \"visible\" to the LLM through schemas and a mapping. In Lesson 7, we'll finally bring it all together by implementing the core reasoning workflow: sending text chunks to o4-mini, instructing it to act as a reviewer, handling its tool call requests, and getting its analysis. This is where the magic happens!\n",
      "\n",
      "---\n",
      "## Lesson 7: Reasoning Workflow—Reviewing Paper Chunks with o4-mini\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Implement the LLM reasoning loop: sending text chunks, handling tool calls, and aggregating responses.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 7: Reasoning Workflow—Reviewing Paper Chunks with o4-mini\")**\n",
      "\n",
      "**Host:** Welcome to Lesson 7! This is where all our previous work comes together. We have our statistical tools, we know how to prepare paper content, and we've defined how o4-mini can access these tools. Now, let's build the core engine that drives the review process: the reasoning workflow.\n",
      "\n",
      "**(Visual: A flowchart: [Chunk of Paper Text] -> [o4-mini + System Prompt + Tools] -> [Analysis / Tool Call?] -> If Tool Call: [Execute Python Function] -> [Return Result to o4-mini] -> [Final Chunk Review].)**\n",
      "\n",
      "**Host:** The heart of our application will be a function, let's call it `review_text_chunk`. This function will take a single chunk of text from the research paper and orchestrate the interaction with o4-mini to get a critical review for that specific chunk.\n",
      "\n",
      "**(Visual: Slide: \"The System Prompt: Setting the Stage\")**\n",
      "**(Text on slide: `role: \"system\"`, `content: \"You are an expert AI research reviewer...\"` - Instructs o4-mini on its persona and task. - Mentions available tools.)**\n",
      "\n",
      "**Host:** First, we need to tell o4-mini how to behave. We'll use a **system prompt**. This is a powerful instruction that sets the context for the entire interaction with the model for that chunk. We'll tell it something like: \"You are an expert AI research reviewer. Read the given chunk of a research paper and highlight weak arguments, unsupported claims, or flawed methodology. You can request tools to: Recalculate p-values, Compute confidence intervals, Estimate effect size (Cohen's d), Describe sample statistics. Be rigorous and explain your reasoning. Conclude with suggestions and a verdict.\" This prompt is key to getting high-quality, critical analysis.\n",
      "\n",
      "**(Visual: Slide: \"Sending Input & Interpreting Output\")**\n",
      "**(Text on slide: - API Call: `client.responses.create(...)` - Input: System prompt, user message (text chunk), tool schemas. - `reasoning={\"effort\": \"high\"}` for deeper analysis. - Output: Can be text OR a function call request.)**\n",
      "\n",
      "**Host:** Next, we'll make an API call to o4-mini using the OpenAI client. We'll send:\n",
      "1.  Our carefully crafted system prompt.\n",
      "2.  The actual text chunk from the paper as a user message.\n",
      "3.  The list of tool schemas we defined in the last lesson, so the model knows what tools it can use.\n",
      "We'll also use the `reasoning={\"effort\": \"high\"}` parameter, encouraging o4-mini to \"think harder\" and provide a more thorough analysis.\n",
      "\n",
      "**Host:** The model's response can be one of two things:\n",
      "1.  A direct textual analysis of the chunk.\n",
      "2.  A \"function call\" request, meaning it wants to use one of our statistical tools.\n",
      "\n",
      "**(Visual: A decision diamond: \"Model Output: Text or Tool Call?\")**\n",
      "**(If Tool Call branch: - Extract tool name & arguments. - Use `tool_function_map` to find Python function. - Execute function. - Send result back to o4-mini in a new API call. - Get final analysis.)**\n",
      "\n",
      "**Host:** This is the crucial part: **Managing tool invocation and result handoff.** If o4-mini requests a tool, its response will be structured, telling us the tool's `name` and the `arguments` it wants to use.\n",
      "Our Python code will then:\n",
      "1.  Extract this name and arguments.\n",
      "2.  Use our `tool_function_map` (from Lesson 6) to find the actual Python function corresponding to that name.\n",
      "3.  Execute that Python function with the arguments provided by the LLM.\n",
      "4.  Take the result from our Python function and send it *back* to o4-mini in a *new* API call. This new call includes the original conversation history plus this new \"tool result\" message.\n",
      "5.  O4-mini will then use this tool's output to continue and finalize its reasoning for that chunk, providing a more informed textual analysis.\n",
      "\n",
      "**Host:** This loop—model analyzes, model requests tool, we run tool, we give result back to model, model finalizes analysis—is what makes the system so powerful. It's like a human researcher who stops to do a quick calculation or look something up before drawing a conclusion.\n",
      "\n",
      "**Host:** In the notebook, we'll implement this `review_text_chunk` function. You'll see how to construct the API call, how to check for tool calls in the response, how to execute the local functions, and how to send the results back for the model to complete its review. This is the engine room of our AI reviewer!\n",
      "\n",
      "**Host:** Let's get this reasoning loop running! Head over to the notebook.\n",
      "\n",
      "**(Outro Music with End Card: \"Time for the Notebook!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Introduction**\n",
      "This is a pivotal lesson where we implement the core logic for reviewing a single chunk of text using o4-mini, including handling its requests to use our statistical tools. We'll build the `review_text_chunk` function which will:\n",
      "1.  Send the text chunk to o4-mini with a system prompt and tool definitions.\n",
      "2.  Check if o4-mini wants to call a function.\n",
      "3.  If a function call is requested:\n",
      "    a.  Execute the corresponding local Python function.\n",
      "    b.  Send the function's result back to o4-mini.\n",
      "4.  Return the final textual review from the model.\n",
      "\n",
      "**Prerequisites:**\n",
      "*   OpenAI API key set as an environment variable (`OPENAI_API_KEY`).\n",
      "*   The `openai` library installed.\n",
      "*   The helper functions (`recalculate_p_value`, etc.), `tool_function_map`, and `tools` list from previous lessons available.\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Setup: Imports, API Client, and Previous Components**\n",
      "\n",
      "Let's bring in everything we need.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: Imports and Setup\n",
      "import os\n",
      "import json # For parsing arguments if they are strings, and pretty printing\n",
      "import openai\n",
      "\n",
      "# Initialize the OpenAI Client\n",
      "# The client automatically picks up the OPENAI_API_KEY from the environment.\n",
      "try:\n",
      "    client = openai.OpenAI()\n",
      "    print(\"OpenAI client initialized successfully.\")\n",
      "except Exception as e:\n",
      "    print(f\"Error initializing OpenAI client: {e}\")\n",
      "    print(\"Please ensure your OPENAI_API_KEY environment variable is set correctly.\")\n",
      "    client = None # Set to None if initialization fails\n",
      "\n",
      "# --- Bring in components from previous lessons ---\n",
      "# Statistical Helper Functions (already defined in Lesson 6's notebook context)\n",
      "# For clarity, let's ensure they are here or loaded.\n",
      "from scipy.stats import ttest_ind, sem, t\n",
      "import numpy as np\n",
      "\n",
      "def recalculate_p_value(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) < 2 or len(group2) < 2: return {\"p_value\": np.nan, \"error\": \"Not enough data\"}\n",
      "    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
      "    return {\"p_value\": round(p_value, 4)}\n",
      "\n",
      "def compute_cohens_d(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) == 0 or len(group2) == 0: return {\"cohens_d\": np.nan, \"error\": \"Empty group data\"}\n",
      "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
      "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
      "    if std1 == 0 and std2 == 0 and mean1 == mean2 : return {\"cohens_d\": 0.0}\n",
      "    if std1 == 0 and std2 == 0 and mean1 != mean2 : return {\"cohens_d\": np.inf if mean1 > mean2 else -np.inf, \"warning\": \"Zero standard deviation in both groups with different means.\"}\n",
      "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
      "    if pooled_std == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else (np.inf if mean1 > mean2 else -np.inf), \"warning\":\"Pooled standard deviation is zero.\"}\n",
      "    d = (mean1 - mean2) / pooled_std\n",
      "    return {\"cohens_d\": round(d, 4)}\n",
      "\n",
      "def compute_confidence_interval(data, confidence=0.95):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n < 2: return {\"mean\": round(np.mean(data), 4) if n == 1 else np.nan, \"confidence_interval\": [np.nan, np.nan], \"confidence\": confidence, \"note\":\"Not enough data\"}\n",
      "    mean = np.mean(data)\n",
      "    standard_error_mean = sem(data)\n",
      "    if standard_error_mean == 0:\n",
      "         return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean,4), round(mean,4)], \"confidence\": confidence, \"note\": \"Data points are identical.\"}\n",
      "    margin_of_error = standard_error_mean * t.ppf((1 + confidence) / 2., n-1)\n",
      "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence\": confidence}\n",
      "\n",
      "def describe_group(data):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n == 0: return {\"mean\": np.nan, \"std_dev\": np.nan, \"n\": 0}\n",
      "    mean = np.mean(data)\n",
      "    std_dev = np.std(data, ddof=1) if n > 1 else 0\n",
      "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4), \"n\": n}\n",
      "\n",
      "# Tool Function Map\n",
      "tool_function_map = {\n",
      "    \"recalculate_p_value\": recalculate_p_value,\n",
      "    \"compute_cohens_d\": compute_cohens_d,\n",
      "    \"compute_confidence_interval\": compute_confidence_interval,\n",
      "    \"describe_group\": describe_group,\n",
      "}\n",
      "\n",
      "# Tool Schemas\n",
      "tools = [\n",
      "    {\"type\": \"function\", \"name\": \"recalculate_p_value\", \"description\": \"Calculate p-value between two independent sample groups...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}},\n",
      "    {\"type\": \"function\", \"name\": \"compute_cohens_d\", \"description\": \"Compute effect size (Cohen's d) between two groups...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}},\n",
      "    {\"type\": \"function\", \"name\": \"compute_confidence_interval\", \"description\": \"Compute the confidence interval for the mean of a single sample group...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"confidence\": {\"type\": \"number\", \"default\": 0.95}}, \"required\": [\"data\"]}},\n",
      "    {\"type\": \"function\", \"name\": \"describe_group\", \"description\": \"Summarize a sample group with its mean, standard deviation, and sample size (n)...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"data\"]}}\n",
      "]\n",
      "# (Using abbreviated descriptions for brevity in this cell, full versions from Lesson 6 are assumed)\n",
      "\n",
      "print(\"Helper functions, tool_function_map, and tools schemas are ready.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "---\n",
      "\n",
      "#### **2. The `review_text_chunk` Function**\n",
      "\n",
      "This is the core function. It takes a text chunk, interacts with o4-mini, and handles any tool calls.\n",
      "The original tutorial uses `client.responses.create` with specific parameters like `reasoning={\"effort\": \"high\"}` and an `input` list for messages, and `response.output` for the list of response messages, `response.output_text` for the direct text. We will follow this structure.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: review_text_chunk function\n",
      "def review_text_chunk(chunk_text_content, attempt_count=0, max_attempts=3):\n",
      "    \"\"\"\n",
      "    Reviews a single chunk of text using o4-mini, handling tool calls.\n",
      "    Includes basic retry logic for robustness.\n",
      "\n",
      "    Args:\n",
      "        chunk_text_content (str): The text chunk to review.\n",
      "        attempt_count (int): Current attempt number for retries.\n",
      "        max_attempts (int): Maximum number of retries for API calls.\n",
      "\n",
      "    Returns:\n",
      "        str: The textual review from o4-mini, or an error message.\n",
      "    \"\"\"\n",
      "    if not client:\n",
      "        return \"Error: OpenAI client not initialized.\"\n",
      "    if attempt_count >= max_attempts:\n",
      "        return \"Error: Maximum retry attempts reached for this chunk.\"\n",
      "\n",
      "    # Define the initial conversation messages for the API call\n",
      "    # The system prompt guides the LLM's behavior.\n",
      "    # The user message contains the text chunk to be reviewed.\n",
      "    current_messages = [\n",
      "        {\n",
      "            \"role\": \"system\",\n",
      "            \"content\": (\n",
      "                \"You are an expert AI research reviewer. Read the given chunk of a research paper and \"\n",
      "                \"highlight weak arguments, unsupported claims, or flawed methodology. \"\n",
      "                \"You can request tools to: Recalculate p-values, Compute confidence intervals, \"\n",
      "                \"Estimate effect size (Cohen's d), Describe sample statistics. \"\n",
      "                \"Be rigorous and explain your reasoning. Conclude with suggestions and a verdict on the chunk's claims.\"\n",
      "            )\n",
      "        },\n",
      "        {\n",
      "            \"role\": \"user\",\n",
      "            \"content\": chunk_text_content\n",
      "        }\n",
      "    ]\n",
      "\n",
      "    try:\n",
      "        # Initial API call to o4-mini\n",
      "        print(f\"Attempt {attempt_count + 1}: Sending chunk to o4-mini for review...\")\n",
      "        response = client.responses.create( # Using .responses.create as per tutorial\n",
      "            model=\"o4-mini\",\n",
      "            reasoning={\"effort\": \"high\"},  # Encourages detailed reasoning\n",
      "            input=current_messages,       # Use 'input' for messages as per tutorial for this API\n",
      "            tools=tools                   # Provide the defined tool schemas\n",
      "            # No explicit tool_choice=\"auto\" needed, Responses API handles it\n",
      "        )\n",
      "        \n",
      "        # The response.output is a list of messages or actions from the model\n",
      "        # We need to iterate through it to find text responses or tool calls.\n",
      "        \n",
      "        # Check for tool calls in the response output\n",
      "        tool_calls_to_process = []\n",
      "        if response.output: # response.output is a list\n",
      "            for item in response.output:\n",
      "                # Check if the item is a function call, using getattr for safety\n",
      "                if getattr(item, \"type\", None) == \"function_call\":\n",
      "                    function_call_details = getattr(item, \"function_call\", None)\n",
      "                    if function_call_details:\n",
      "                        tool_calls_to_process.append({\n",
      "                            \"id\": getattr(item, \"id\", None), # Tool call ID, if provided by API\n",
      "                            \"name\": getattr(function_call_details, \"name\", \"\"),\n",
      "                            \"arguments\": getattr(function_call_details, \"arguments\", {}) # Arguments might be a dict or str\n",
      "                        })\n",
      "        \n",
      "        if tool_calls_to_process:\n",
      "            print(f\"Tool call(s) requested by o4-mini: {[tc['name'] for tc in tool_calls_to_process]}\")\n",
      "            \n",
      "            # Append the assistant's message (containing the tool calls) to current_messages\n",
      "            # The tutorial implies that response.output itself can be part of the *next* input\n",
      "            # This means the assistant's turn which includes the tool_call directive should be added.\n",
      "            # Let's assume response.output contains the assistant's turn that has the tool_call.\n",
      "            current_messages.extend(response.output) # Add assistant's turn with tool calls\n",
      "\n",
      "            for tool_call in tool_calls_to_process:\n",
      "                fn_name = tool_call[\"name\"]\n",
      "                fn_args_raw = tool_call[\"arguments\"]\n",
      "\n",
      "                if fn_name in tool_function_map:\n",
      "                    actual_function = tool_function_map[fn_name]\n",
      "                    \n",
      "                    # Arguments from the LLM might be a string requiring JSON parsing, or already a dict.\n",
      "                    # The tutorial example `tool_function_map[fn_name](**args)` implies args is a dict.\n",
      "                    # Let's assume fn_args_raw is already a dictionary of arguments.\n",
      "                    # If it were a JSON string: fn_args = json.loads(fn_args_raw)\n",
      "                    fn_args = fn_args_raw if isinstance(fn_args_raw, dict) else {} # Basic safety\n",
      "\n",
      "                    try:\n",
      "                        print(f\"Executing tool: {fn_name} with args: {fn_args}\")\n",
      "                        tool_result = actual_function(**fn_args)\n",
      "                        print(f\"Tool {fn_name} result: {tool_result}\")\n",
      "                    except Exception as e:\n",
      "                        print(f\"Error executing tool {fn_name}: {e}\")\n",
      "                        tool_result = {\"error\": f\"Failed to execute tool {fn_name}: {str(e)}\"}\n",
      "\n",
      "                    # Append the tool result message to send back to the model\n",
      "                    current_messages.append({\n",
      "                        \"role\": \"tool\",\n",
      "                        # \"tool_call_id\": tool_call[\"id\"], # If tool_call_id is available and required\n",
      "                        \"name\": fn_name, # Name of the function that was called\n",
      "                        \"content\": str(tool_result)  # Result must be a string\n",
      "                    })\n",
      "                else:\n",
      "                    print(f\"Warning: Model requested unknown tool '{fn_name}'\")\n",
      "                    # Append an error message for the unknown tool\n",
      "                    current_messages.append({\n",
      "                        \"role\": \"tool\",\n",
      "                        \"name\": fn_name,\n",
      "                        \"content\": f'{{\"error\": \"Tool named {fn_name} not found.\"}}'\n",
      "                    })\n",
      "\n",
      "            # Make a second API call with the tool results included\n",
      "            print(\"Sending tool results back to o4-mini...\")\n",
      "            tool_response = client.responses.create(\n",
      "                model=\"o4-mini\",\n",
      "                reasoning={\"effort\": \"high\"},\n",
      "                input=current_messages, # Send the whole conversation history including tool results\n",
      "                max_output_tokens=3000 # As per tutorial example\n",
      "                # Do not send 'tools' again if we only want a text response now\n",
      "            )\n",
      "\n",
      "            # Process the response after tool execution\n",
      "            if hasattr(tool_response, \"output_text\") and tool_response.output_text:\n",
      "                return tool_response.output_text.strip()\n",
      "            elif tool_response.status == \"incomplete\":\n",
      "                 reason = getattr(tool_response.incomplete_details, \"reason\", \"unknown\")\n",
      "                 return f\"Incomplete response after tool call: {reason}\"\n",
      "            else: # Fallback if output_text is not present but output list is\n",
      "                final_text_parts = [item.text for item in tool_response.output if hasattr(item, 'text')]\n",
      "                if final_text_parts: return \" \".join(final_text_parts).strip()\n",
      "                return \"No valid text output after tool call.\"\n",
      "\n",
      "        # If no tool was called, return the original response's text\n",
      "        elif hasattr(response, \"output_text\") and response.output_text:\n",
      "            return response.output_text.strip()\n",
      "        # Fallback if output_text is not present but output list is\n",
      "        elif response.output and any(hasattr(item, 'text') for item in response.output):\n",
      "            text_parts = [item.text for item in response.output if hasattr(item, 'text')]\n",
      "            return \" \".join(text_parts).strip()\n",
      "        elif response.status == \"incomplete\":\n",
      "            reason = getattr(response.incomplete_details, \"reason\", \"unknown\")\n",
      "            return f\"Incomplete response (no tool call): {reason}\"\n",
      "        else:\n",
      "            return \"No valid output or tool call returned by the model.\"\n",
      "\n",
      "    except openai.APIError as e: # More specific OpenAI errors\n",
      "        print(f\"OpenAI API Error during chunk review (attempt {attempt_count + 1}): {e}\")\n",
      "        # Implement retry logic for certain types of API errors (e.g., rate limits, server errors)\n",
      "        if isinstance(e, (openai.RateLimitError, openai.APIConnectionError, openai.InternalServerError)):\n",
      "            if attempt_count < max_attempts -1 :\n",
      "                 print(f\"Retrying chunk review...\")\n",
      "                 import time\n",
      "                 time.sleep(2**(attempt_count)) # Exponential backoff\n",
      "                 return review_text_chunk(chunk_text_content, attempt_count + 1, max_attempts)\n",
      "        return f\"OpenAI API Error: {e}\"\n",
      "    except Exception as e:\n",
      "        print(f\"General Error during chunk review (attempt {attempt_count + 1}): {e}\")\n",
      "        # Basic retry for general errors, could be refined\n",
      "        if attempt_count < max_attempts -1 :\n",
      "            print(f\"Retrying chunk review due to general error...\")\n",
      "            import time\n",
      "            time.sleep(2**(attempt_count)) # Exponential backoff\n",
      "            return review_text_chunk(chunk_text_content, attempt_count + 1, max_attempts)\n",
      "        return f\"Error during chunk review: {e}\"\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "---\n",
      "\n",
      "#### **3. Example Interaction Walkthrough**\n",
      "\n",
      "Let's test `review_text_chunk` with a sample text that might trigger a tool call.\n",
      "**Note:** Running this live requires a valid OpenAI API key with credits and might take some time.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 3: Test review_text_chunk\n",
      "if client: # Only run if client was initialized\n",
      "    # Sample text designed to potentially trigger a statistical tool\n",
      "    # This text implies data that the LLM might want to verify.\n",
      "    sample_chunk_for_review = \"\"\"\n",
      "    In our study, Group A (mean=25, n=30, data: [23,24,25,26,27,...]) showed significantly \n",
      "    higher scores than Group B (mean=22, n=30, data: [20,21,22,23,24,...]). \n",
      "    We found p < 0.01. The effect size was substantial.\n",
      "    The control group (n=10, values: [10,11,10,12,10,11,9,10,11,10]) had a mean of 10.5.\n",
      "    \"\"\"\n",
      "    # To make this runnable without full data, the LLM would need to ask for data.\n",
      "    # Let's craft a simpler one where data is directly provided for tools.\n",
      "    \n",
      "    sample_chunk_for_review_tool_friendly = \"\"\"\n",
      "    The treatment group consisted of values [10, 12, 11, 13, 14].\n",
      "    The control group had values [8, 9, 7, 8, 9].\n",
      "    We claim a significant difference (p < 0.05) and a medium effect size.\n",
      "    Please verify these claims.\n",
      "    Also, describe the treatment group's statistics and its 95% confidence interval.\n",
      "    \"\"\"\n",
      "\n",
      "    print(\"\\n--- Reviewing Sample Chunk (Tool Friendly) ---\")\n",
      "    review_output = review_text_chunk(sample_chunk_for_review_tool_friendly)\n",
      "    print(\"\\n--- O4-Mini Review Output ---\")\n",
      "    print(review_output)\n",
      "    print(\"--- End of Review ---\")\n",
      "    \n",
      "    # Example of text that might not trigger a tool call\n",
      "    sample_chunk_no_tool = \"\"\"\n",
      "    The theoretical framework of this paper is based on established principles of cognitive load theory.\n",
      "    We argue that current educational methods often overlook these principles, leading to suboptimal learning outcomes.\n",
      "    Our proposed intervention aims to address this gap by introducing adaptive learning modules.\n",
      "    \"\"\"\n",
      "    print(\"\\n--- Reviewing Sample Chunk (No Tool Expected) ---\")\n",
      "    review_output_no_tool = review_text_chunk(sample_chunk_no_tool)\n",
      "    print(\"\\n--- O4-Mini Review Output (No Tool) ---\")\n",
      "    print(review_output_no_tool)\n",
      "    print(\"--- End of Review ---\")\n",
      "\n",
      "else:\n",
      "    print(\"OpenAI client not initialized. Skipping review_text_chunk test.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanations:**\n",
      "*   **High-level view of the reasoning and verification loop:**\n",
      "    1.  **Prompting for Critical Analysis:** The system prompt sets o4-mini in \"expert reviewer\" mode.\n",
      "    2.  **Initial Analysis by LLM:** The model reads the chunk and identifies areas needing scrutiny, especially statistical claims or data summaries.\n",
      "    3.  **Tool Identification:** Based on its understanding and the tool descriptions, the LLM decides if a tool can help (e.g., \"This paper claims a p-value; I should use `recalculate_p_value`\").\n",
      "    4.  **Structured Tool Call:** The LLM formulates a request for the specific tool with necessary arguments (which it tries to extract or infer from the text).\n",
      "    5.  **Local Execution:** Our Python environment runs the *actual* trusted statistical function. This is key – the LLM doesn't *do* the math; it *asks* for it to be done by our code.\n",
      "    6.  **Informed Refinement:** The LLM receives the tool's output (e.g., the recalculated p-value) and incorporates this objective data into its final review of the chunk. If the tool's result contradicts the paper, the LLM can point this out.\n",
      "*   **How the model’s multi-step process mimics a human reviewer:**\n",
      "    *   A human reviewer reads a section, might find a claim interesting or questionable.\n",
      "    *   They might then pause, grab a calculator (or statistical software), input some numbers from a table in the paper, and perform a quick check.\n",
      "    *   Based on this check, they refine their critique or confirm the paper's findings.\n",
      "    *   Our `review_text_chunk` function, with its tool-use loop, automates a similar process, allowing the LLM to \"pause and verify.\"\n",
      "\n",
      "---\n",
      "\n",
      "**Next Steps:**\n",
      "We've successfully built the engine for reviewing individual chunks! In Lesson 8, we'll create the wrapper functions to process an entire PDF: extract text, chunk it, review each chunk using the function we just built, and then aggregate all the reviews into a final, comprehensive report.\n",
      "\n",
      "---\n",
      "## Lesson 8: Bringing It All Together—Full Paper Review Pipeline\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Orchestrate all components into a working research paper reviewer, from PDF input to structured review output.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 8: Bringing It All Together—Full Paper Review Pipeline\")**\n",
      "\n",
      "**Host:** Welcome to Lesson 8! We've built all the individual components: PDF text extraction, chunking, statistical tools, tool registration, and the core chunk review logic with o4-mini. Now it's time to assemble everything into a complete, end-to-end research paper review pipeline!\n",
      "\n",
      "**(Visual: An animation showing puzzle pieces labeled \"PDF Extraction,\" \"Chunking,\" \"Tool Helpers,\" \"o4-mini Review,\" \"Tool Mapping\" all coming together to form a complete \"Review Pipeline\" graphic.)**\n",
      "\n",
      "**Host:** Think of what we've done so far as building different specialized stations on an assembly line. Now, we're going to connect that assembly line so a research paper (our raw material) can go in one end, and a full, structured review comes out the other.\n",
      "\n",
      "**(Visual: Slide: \"Overview of the End-to-End Flow\")**\n",
      "**(Text on slide: 1. Input: PDF file path. 2. Extract Text: Use `extract_text_from_pdf`. 3. Chunk Text: Use `chunk_text`. 4. Review Chunks: Loop through chunks, call `review_text_chunk` for each. 5. Aggregate Reviews: Combine individual chunk reviews. 6. Output: Formatted full review (e.g., Markdown).)**\n",
      "\n",
      "**Host:** Here’s how our full pipeline will work:\n",
      "1.  It starts with the **path to a PDF research paper** as input.\n",
      "2.  First, we'll use our `extract_text_from_pdf` function (from Lesson 5) to get all the raw text content from the PDF.\n",
      "3.  Then, this raw text will be fed into our `chunk_text` function (also from Lesson 5) to break it down into manageable, token-limited chunks for o4-mini.\n",
      "4.  Next, the core part: we'll loop through each of these chunks. For every chunk, we'll call our `review_text_chunk` function (from Lesson 7). This is where o4-mini analyzes the content, potentially uses our statistical tools, and generates a review for that specific piece of the paper.\n",
      "5.  As we get the review for each chunk, we'll collect them.\n",
      "6.  Finally, we'll combine all these individual chunk reviews into a single, coherent document. We can format this nicely, perhaps in Markdown, with clear headings for each chunk's review.\n",
      "\n",
      "**(Visual: A simplified code structure or flowchart showing how `review_full_pdf` calls `extract_text_from_pdf`, then `chunk_text`, then loops calling `review_text_chunk`.)**\n",
      "\n",
      "**Host:** We'll encapsulate this entire process in a main function, let's call it `review_full_pdf`. This function will be the conductor of our orchestra, ensuring each part plays its role at the right time.\n",
      "\n",
      "**Host:** We'll also discuss how to make this runnable as a script from your command line, allowing you to easily point it at any PDF and get a review. This involves using Python's `argparse` module to handle command-line arguments.\n",
      "\n",
      "**(Visual: Mockup of a terminal command: `python pdf_reviewer.py Fake_paper.pdf` and then a snippet of the Markdown output.)**\n",
      "\n",
      "**Host:** And of course, saving the output is important. We'll write the final aggregated review to a Markdown file. Markdown is great because it's easy to read and can be converted to HTML or other formats if needed.\n",
      "\n",
      "**Host:** By the end of this lesson's notebook, you'll have a complete Python script that can take a research paper PDF, process it through o4-mini with tool support, and produce a detailed review. This is the culmination of all the concepts we've learned!\n",
      "\n",
      "**Host:** Let's put it all together! Head over to the notebook.\n",
      "\n",
      "**(Outro Music with End Card: \"Time for the Notebook!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Introduction**\n",
      "In this lesson, we assemble all the pieces we've built into a cohesive pipeline. We'll create a main function `review_full_pdf` that orchestrates the entire process:\n",
      "1.  Extracting text from a given PDF path.\n",
      "2.  Chunking the extracted text.\n",
      "3.  Iterating through chunks and calling `review_text_chunk` (our powerful function from Lesson 7) for each.\n",
      "4.  Aggregating the reviews.\n",
      "5.  A `main` block to run the script with a PDF path from the command line (simulated here).\n",
      "\n",
      "This will result in a functional AI research paper reviewer.\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. Consolidating All Necessary Imports and Functions**\n",
      "\n",
      "First, let's ensure all our building blocks are in place. This includes imports and all the functions we've defined in previous lessons.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: All Imports and Function Definitions (Consolidated)\n",
      "\n",
      "# Standard library imports\n",
      "import os\n",
      "import json\n",
      "import argparse # For command-line argument parsing\n",
      "import time\n",
      "\n",
      "# Third-party library imports\n",
      "import fitz  # PyMuPDF\n",
      "import tiktoken\n",
      "import numpy as np\n",
      "from scipy.stats import ttest_ind, sem, t\n",
      "import openai\n",
      "\n",
      "# --- OpenAI Client Initialization ---\n",
      "try:\n",
      "    client = openai.OpenAI()\n",
      "    print(\"OpenAI client initialized.\")\n",
      "except Exception as e:\n",
      "    print(f\"Failed to initialize OpenAI client: {e}. Ensure OPENAI_API_KEY is set.\")\n",
      "    client = None\n",
      "\n",
      "# --- Function: extract_text_from_pdf (Lesson 5) ---\n",
      "def extract_text_from_pdf(pdf_path):\n",
      "    try:\n",
      "        doc = fitz.open(pdf_path)\n",
      "    except Exception as e:\n",
      "        print(f\"Error opening PDF {pdf_path}: {e}\")\n",
      "        return \"\"\n",
      "    full_text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
      "    doc.close()\n",
      "    return full_text\n",
      "\n",
      "# --- Function: chunk_text (Lesson 5) ---\n",
      "def chunk_text(text, max_tokens=12000, model_encoding=\"cl100k_base\"):\n",
      "    try:\n",
      "        encoding = tiktoken.get_encoding(model_encoding)\n",
      "    except:\n",
      "        encoding = tiktoken.get_encoding(\"gpt2\") # Fallback\n",
      "    tokens = encoding.encode(text)\n",
      "    chunks = []\n",
      "    for i in range(0, len(tokens), max_tokens):\n",
      "        chunk_tokens = tokens[i:i + max_tokens]\n",
      "        chunks.append(encoding.decode(chunk_tokens))\n",
      "    return chunks\n",
      "\n",
      "# --- Statistical Helper Functions (Lesson 4 & 6) ---\n",
      "def recalculate_p_value(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) < 2 or len(group2) < 2: return {\"p_value\": np.nan, \"error\": \"Not enough data\"}\n",
      "    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
      "    return {\"p_value\": round(p_value, 4)}\n",
      "\n",
      "def compute_cohens_d(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) == 0 or len(group2) == 0: return {\"cohens_d\": np.nan, \"error\": \"Empty group data\"}\n",
      "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
      "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
      "    if std1 == 0 and std2 == 0 and mean1 == mean2 : return {\"cohens_d\": 0.0}\n",
      "    if std1 == 0 and std2 == 0 and mean1 != mean2 : return {\"cohens_d\": np.inf if mean1 > mean2 else -np.inf, \"warning\": \"Zero standard deviation in both groups with different means.\"}\n",
      "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
      "    if pooled_std == 0: return {\"cohens_d\": 0.0 if mean1 == mean2 else (np.inf if mean1 > mean2 else -np.inf), \"warning\":\"Pooled standard deviation is zero.\"}\n",
      "    d = (mean1 - mean2) / pooled_std\n",
      "    return {\"cohens_d\": round(d, 4)}\n",
      "\n",
      "def compute_confidence_interval(data, confidence=0.95):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n < 2: return {\"mean\": round(np.mean(data), 4) if n == 1 else np.nan, \"confidence_interval\": [np.nan, np.nan], \"confidence\": confidence, \"note\":\"Not enough data\"}\n",
      "    mean = np.mean(data)\n",
      "    standard_error_mean = sem(data)\n",
      "    if standard_error_mean == 0:\n",
      "         return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean,4), round(mean,4)], \"confidence\": confidence, \"note\": \"Data points are identical.\"}\n",
      "    margin_of_error = standard_error_mean * t.ppf((1 + confidence) / 2., n-1)\n",
      "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence\": confidence}\n",
      "\n",
      "def describe_group(data):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n == 0: return {\"mean\": np.nan, \"std_dev\": np.nan, \"n\": 0}\n",
      "    mean = np.mean(data)\n",
      "    std_dev = np.std(data, ddof=1) if n > 1 else 0\n",
      "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4), \"n\": n}\n",
      "\n",
      "# --- Tool Definitions (Lesson 6) ---\n",
      "tool_function_map = {\n",
      "    \"recalculate_p_value\": recalculate_p_value, \"compute_cohens_d\": compute_cohens_d,\n",
      "    \"compute_confidence_interval\": compute_confidence_interval, \"describe_group\": describe_group,\n",
      "}\n",
      "tools = [\n",
      "    {\"type\": \"function\", \"name\": \"recalculate_p_value\", \"description\": \"Calculate p-value between two independent sample groups...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}},\n",
      "    {\"type\": \"function\", \"name\": \"compute_cohens_d\", \"description\": \"Compute effect size (Cohen's d) between two groups...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"group1\", \"group2\"]}},\n",
      "    {\"type\": \"function\", \"name\": \"compute_confidence_interval\", \"description\": \"Compute the confidence interval for the mean of a single sample group...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}, \"confidence\": {\"type\": \"number\", \"default\": 0.95}}, \"required\": [\"data\"]}},\n",
      "    {\"type\": \"function\", \"name\": \"describe_group\", \"description\": \"Summarize a sample group with its mean, standard deviation, and sample size (n)...\", \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}}}, \"required\": [\"data\"]}}\n",
      "]\n",
      "# (Using abbreviated descriptions for brevity in this cell)\n",
      "\n",
      "# --- Function: review_text_chunk (Lesson 7) ---\n",
      "def review_text_chunk(chunk_text_content, attempt_count=0, max_attempts=3):\n",
      "    if not client: return \"Error: OpenAI client not initialized.\"\n",
      "    if attempt_count >= max_attempts: return f\"Error: Max retry ({max_attempts}) reached.\"\n",
      "\n",
      "    current_messages = [\n",
      "        {\"role\": \"system\", \"content\": \"You are an expert AI research reviewer... Be rigorous... Conclude with suggestions and a verdict on the chunk's claims.\"}, # Abbreviated system prompt\n",
      "        {\"role\": \"user\", \"content\": chunk_text_content}\n",
      "    ]\n",
      "    try:\n",
      "        # print(f\"Attempt {attempt_count + 1}: Sending chunk to o4-mini...\")\n",
      "        response = client.responses.create(\n",
      "            model=\"o4-mini\", reasoning={\"effort\": \"high\"},\n",
      "            input=current_messages, tools=tools\n",
      "        )\n",
      "        tool_calls_to_process = []\n",
      "        if response.output:\n",
      "            for item in response.output:\n",
      "                if getattr(item, \"type\", None) == \"function_call\":\n",
      "                    function_call_details = getattr(item, \"function_call\", None)\n",
      "                    if function_call_details:\n",
      "                        tool_calls_to_process.append({\n",
      "                            \"id\": getattr(item, \"id\", None),\n",
      "                            \"name\": getattr(function_call_details, \"name\", \"\"),\n",
      "                            \"arguments\": getattr(function_call_details, \"arguments\", {})\n",
      "                        })\n",
      "        \n",
      "        if tool_calls_to_process:\n",
      "            # print(f\"Tool call(s) requested: {[tc['name'] for tc in tool_calls_to_process]}\")\n",
      "            current_messages.extend(response.output) # Add assistant's turn with tool calls\n",
      "            for tool_call in tool_calls_to_process:\n",
      "                fn_name, fn_args = tool_call[\"name\"], tool_call[\"arguments\"]\n",
      "                if fn_name in tool_function_map:\n",
      "                    try:\n",
      "                        tool_result = tool_function_map[fn_name](**fn_args)\n",
      "                    except Exception as e:\n",
      "                        tool_result = {\"error\": f\"Tool {fn_name} execution failed: {e}\"}\n",
      "                    current_messages.append({\"role\": \"tool\", \"name\": fn_name, \"content\": str(tool_result)})\n",
      "                else:\n",
      "                     current_messages.append({\"role\": \"tool\", \"name\": fn_name, \"content\": f'{{\"error\": \"Tool {fn_name} not found.\"}}'})\n",
      "            \n",
      "            # print(\"Sending tool results back to o4-mini...\")\n",
      "            tool_response = client.responses.create(\n",
      "                model=\"o4-mini\", reasoning={\"effort\": \"high\"},\n",
      "                input=current_messages, max_output_tokens=3000\n",
      "            )\n",
      "            if hasattr(tool_response, \"output_text\") and tool_response.output_text: return tool_response.output_text.strip()\n",
      "            if tool_response.output and any(hasattr(item, 'text') for item in tool_response.output): return \" \".join([item.text for item in tool_response.output if hasattr(item, 'text')]).strip()\n",
      "            if tool_response.status == \"incomplete\": return f\"Incomplete response after tool call: {getattr(tool_response.incomplete_details, 'reason', 'unknown')}\"\n",
      "            return \"No valid text output after tool call.\"\n",
      "\n",
      "        if hasattr(response, \"output_text\") and response.output_text: return response.output_text.strip()\n",
      "        if response.output and any(hasattr(item, 'text') for item in response.output): return \" \".join([item.text for item in response.output if hasattr(item, 'text')]).strip()\n",
      "        if response.status == \"incomplete\": return f\"Incomplete response (no tool call): {getattr(response.incomplete_details, 'reason', 'unknown')}\"\n",
      "        return \"No valid output or tool call by model.\"\n",
      "\n",
      "    except openai.APIError as e:\n",
      "        print(f\"OpenAI API Error (attempt {attempt_count + 1}): {e}\")\n",
      "        if isinstance(e, (openai.RateLimitError, openai.APIConnectionError, openai.InternalServerError)) and attempt_count < max_attempts - 1:\n",
      "            time.sleep(2**(attempt_count)); return review_text_chunk(chunk_text_content, attempt_count + 1, max_attempts)\n",
      "        return f\"OpenAI API Error: {e}\"\n",
      "    except Exception as e:\n",
      "        print(f\"General Error (attempt {attempt_count + 1}): {e}\")\n",
      "        if attempt_count < max_attempts - 1:\n",
      "            time.sleep(2**(attempt_count)); return review_text_chunk(chunk_text_content, attempt_count + 1, max_attempts)\n",
      "        return f\"Error during chunk review: {e}\"\n",
      "\n",
      "print(\"All components (imports, client, functions, tools) are ready.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "**Note:** The system prompt in `review_text_chunk` and some descriptions in `tools` have been abbreviated for this consolidated cell. Assume full versions are used.\n",
      "\n",
      "---\n",
      "\n",
      "#### **2. The `review_full_pdf` Function**\n",
      "\n",
      "This function ties everything together.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: review_full_pdf function\n",
      "def review_full_pdf(pdf_path):\n",
      "    \"\"\"\n",
      "    Orchestrates the full review of a research paper PDF.\n",
      "    Extracts text, chunks it, reviews each chunk, and aggregates reviews.\n",
      "\n",
      "    Args:\n",
      "        pdf_path (str): Path to the PDF file.\n",
      "\n",
      "    Returns:\n",
      "        str: A string containing the full aggregated review in Markdown format.\n",
      "    \"\"\"\n",
      "    if not client:\n",
      "        return \"Error: OpenAI client not initialized. Cannot review PDF.\"\n",
      "\n",
      "    print(f\"Starting review for PDF: {pdf_path}\")\n",
      "\n",
      "    # Step 1: Extract text from PDF\n",
      "    raw_text = extract_text_from_pdf(pdf_path)\n",
      "    if not raw_text:\n",
      "        return f\"Could not extract text from {pdf_path}. Aborting review.\"\n",
      "    print(f\"Successfully extracted text. Total characters: {len(raw_text)}\")\n",
      "\n",
      "    # Step 2: Chunk the text\n",
      "    # Using a smaller max_tokens for quicker testing if needed, adjust for production\n",
      "    # The tutorial uses 12000, which is reasonable for o4-mini.\n",
      "    text_chunks = chunk_text(raw_text, max_tokens=12000) \n",
      "    print(f\"Text divided into {len(text_chunks)} chunks.\")\n",
      "\n",
      "    all_reviews = []\n",
      "    for idx, chunk in enumerate(text_chunks):\n",
      "        print(f\"\\nReviewing Chunk {idx + 1}/{len(text_chunks)}...\")\n",
      "        # Add a small delay to be kind to the API, especially if many chunks\n",
      "        if idx > 0 : time.sleep(1) # Simple delay, consider more robust rate limiting for many chunks\n",
      "        \n",
      "        chunk_review = review_text_chunk(chunk)\n",
      "        all_reviews.append(f\"## Review of Chunk {idx + 1}\\n\\n{chunk_review}\")\n",
      "        print(f\"Finished reviewing Chunk {idx + 1}.\")\n",
      "        # print(f\"Partial review for chunk {idx+1}:\\n{chunk_review[:200]}...\") # For debugging\n",
      "\n",
      "    full_review_md = f\"# AI-Powered Review of '{os.path.basename(pdf_path)}'\\n\\n\"\n",
      "    full_review_md += \"\\n\\n---\\n\\n\".join(all_reviews)\n",
      "    \n",
      "    print(\"\\nFull review aggregated.\")\n",
      "    return full_review_md\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "---\n",
      "\n",
      "#### **3. Main Execution Block (`if __name__ == \"__main__\":`)**\n",
      "\n",
      "This block allows the script to be run from the command line, taking the PDF path as an argument. We'll simulate its behavior here.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 3: Main execution logic (simulated for notebook)\n",
      "\n",
      "def main():\n",
      "    # In a .py script, you'd use argparse:\n",
      "    # parser = argparse.ArgumentParser(description=\"Review an academic paper PDF using o4-mini.\")\n",
      "    # parser.add_argument(\"pdf_path\", type=str, help=\"Path to the research paper PDF.\")\n",
      "    # args = parser.parse_args()\n",
      "    # pdf_to_review_path = args.pdf_path\n",
      "    \n",
      "    # For notebook simulation, we'll define the path directly.\n",
      "    # Use the dummy_paper.pdf created in Lesson 5, or provide your own.\n",
      "    pdf_to_review_path = \"dummy_paper.pdf\" # Make sure this file exists\n",
      "                                           # Or use the \"Fake_paper.pdf\" if you downloaded from tutorial repo\n",
      "    \n",
      "    # Create a dummy PDF if it doesn't exist, for testing purposes\n",
      "    if not os.path.exists(pdf_to_review_path) and pdf_to_review_path == \"dummy_paper.pdf\":\n",
      "        try:\n",
      "            doc_dummy = fitz.open()\n",
      "            page_dummy = doc_dummy.new_page()\n",
      "            page_dummy.insert_text((72, 72), \"This is a dummy paper. Section 1: Introduction. Our study aims to explore the effects of LLMs on productivity.\")\n",
      "            page_dummy.insert_text((72, 100), \"Section 2: Methods. We used a control group (data: [1,2,3,2,1]) and a treatment group (data: [5,6,7,6,5]). We claim p < 0.05.\")\n",
      "            doc_dummy.save(pdf_to_review_path)\n",
      "            doc_dummy.close()\n",
      "            print(f\"Created '{pdf_to_review_path}' for testing.\")\n",
      "        except Exception as e:\n",
      "            print(f\"Could not create dummy PDF '{pdf_to_review_path}': {e}\")\n",
      "            return\n",
      "\n",
      "    if not os.path.exists(pdf_to_review_path):\n",
      "        print(f\"Error: PDF file not found at '{pdf_to_review_path}'. Please provide a valid path.\")\n",
      "        return\n",
      "\n",
      "    if not client:\n",
      "        print(\"OpenAI client not initialized. Cannot proceed with review.\")\n",
      "        return\n",
      "\n",
      "    print(f\"Preparing to review: {pdf_to_review_path}\")\n",
      "    final_review = review_full_pdf(pdf_to_review_path)\n",
      "    \n",
      "    print(\"\\n\\n--- FINAL AGGREGATED REVIEW ---\")\n",
      "    # Print first 1000 characters of the review to keep notebook output manageable\n",
      "    print(final_review[:1000] + \"...\" if len(final_review) > 1000 else final_review)\n",
      "    \n",
      "    output_filename = \"paper_review_output.md\"\n",
      "    try:\n",
      "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
      "            f.write(final_review)\n",
      "        print(f\"\\nReview successfully saved to '{output_filename}'\")\n",
      "    except Exception as e:\n",
      "        print(f\"\\nError saving review to file: {e}\")\n",
      "\n",
      "# To run the main function (simulating command-line execution):\n",
      "if __name__ == \"__main__\" or __name__ == \"__builtin__\": # __builtin__ for notebook context\n",
      "    # Check if client is available before running\n",
      "    if client:\n",
      "        main()\n",
      "    else:\n",
      "        print(\"Skipping main execution as OpenAI client is not available.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "**Conceptual Explanations:**\n",
      "*   **Best practices for modular code and reproducibility:**\n",
      "    *   **Functions for specific tasks:** We've broken down the problem into manageable functions (`extract_text_from_pdf`, `chunk_text`, `review_text_chunk`, `review_full_pdf`, and the statistical helpers). This makes the code easier to understand, test, and maintain.\n",
      "    *   **Configuration (Implicit):** API keys are handled via environment variables, not hardcoded. Model names, token limits, etc., could also be made configurable.\n",
      "    *   **Clear Dependencies:** Using `requirements.txt` (implicitly, by listing `pip install` commands) helps others set up the same environment.\n",
      "    *   **Saving Outputs:** Saving the review to a file allows for persistence and later use. Using standard formats like Markdown aids reproducibility and sharing.\n",
      "*   **Tips for further extensions:**\n",
      "    *   **UI Integration:** Wrap this logic in a web application (e.g., using Flask or FastAPI) where users can upload PDFs.\n",
      "    *   **More Sophisticated Chunking:** Implement overlapping chunks or sentence-boundary-aware chunking to potentially improve context preservation between chunks.\n",
      "    *   **Additional Tools:** Add more tools, like a citation checker, a plagiarism detector (via API), or tools to summarize tables.\n",
      "    *   **Error Handling and Logging:** Implement more robust error handling, retries with exponential backoff for API calls, and logging for easier debugging.\n",
      "    *   **Caching:** Cache results for chunks that haven't changed to save API calls and time if re-reviewing.\n",
      "    *   **User Feedback Loop:** Allow users to rate the quality of the review or specific points, which could be used to refine prompts or even fine-tune models in the long run.\n",
      "\n",
      "---\n",
      "\n",
      "**Next Steps:**\n",
      "Congratulations! You've now assembled a full AI research paper reviewer. The final lesson is the Capstone Project, where you'll get to run this pipeline on your own chosen paper, experiment with it, and potentially extend its capabilities. This is your chance to truly make it your own!\n",
      "\n",
      "---\n",
      "## Lesson 9: Capstone Project—Build and Run Your Own Paper Reviewer\n",
      "\n",
      "- **Lesson Format:** Video + Notebook\n",
      "- **Lesson Goal:** Apply all skills to build, test, and analyze your own research paper reviewer using o4-mini and custom tools.\n",
      "\n",
      "### Narrated Script (Video Introduction)\n",
      "\n",
      "**(Intro Music with Title Card: \"Lesson 9: Capstone Project—Build and Run Your Own Paper Reviewer\")**\n",
      "\n",
      "**Host:** Welcome to the final lesson and our capstone project! You've journeyed through understanding reasoning models, prompt engineering, tool integration, PDF processing, and building a full review pipeline. Now it's time to take the driver's seat and apply everything you've learned.\n",
      "\n",
      "**(Visual: A montage of previous lesson highlights: o4-mini logo, prompt engineering graphic, tool icons, PDF processing animation, pipeline diagram.)**\n",
      "\n",
      "**Host:** In this capstone, you'll be working with the complete code for the AI Research Paper Reviewer that we finalized in Lesson 8. Your main task will be to run it, test it with different papers, and really see it in action.\n",
      "\n",
      "**(Visual: Slide: \"Capstone Project: Your Mission\")**\n",
      "**(Text on slide: 1. Run the Full Pipeline: Use the complete script from Lesson 8. 2. Test with PDFs: Your own or a provided demo paper. 3. Analyze the Output: Examine the review quality, tool usage. 4. Experiment & Extend (Optional): Tweak prompts, add new tools.)**\n",
      "\n",
      "**Host:** Here’s a step-by-step walkthrough of what you'll do in the notebook:\n",
      "1.  **Prepare your PDF:** You can use the `Fake_paper.pdf` from the original tutorial's GitHub repository (if you have access to it or a similar one designed with flaws), or find a research paper of your own. A shorter paper, or one you're familiar with, might be good for initial testing.\n",
      "2.  **Run the Full Pipeline:** You'll execute the main script, providing the path to your chosen PDF. The script will then go through all the steps: extracting text, chunking, and having o4-mini review each chunk with potential tool use.\n",
      "3.  **Interpret Model Output:** Once the review is generated (it will be saved to a Markdown file), your job is to analyze it.\n",
      "    *   Does the review make sense?\n",
      "    *   Did o4-mini identify genuine strengths or weaknesses?\n",
      "    *   If you used a paper with known statistical claims, did the model attempt to use its tools? Were the tool outputs (as reflected in the review) accurate?\n",
      "    *   How was the overall quality of the critique?\n",
      "\n",
      "**(Visual: A person looking thoughtfully at a generated review document, perhaps with a checklist or magnifying glass.)**\n",
      "\n",
      "**Host:** We'll also discuss **troubleshooting tips**. What if the output isn't what you expect?\n",
      "*   Maybe the PDF extraction had issues (common with complex PDF layouts).\n",
      "*   Perhaps the system prompt needs further refinement for the specific type of paper or review style you want.\n",
      "*   Are the tool descriptions clear enough for the LLM to use them effectively?\n",
      "\n",
      "**(Visual: Slide: \"Experimentation & Extension Ideas\")**\n",
      "**(Text on slide: - Tweak the System Prompt: Change persona, focus, output format. - Modify Tool Descriptions: Make them clearer or more specific. - Add a New Tool: E.g., a tool to check for acronym definitions. - Adjust Chunking Strategy: Experiment with `max_tokens`.)**\n",
      "\n",
      "**Host:** The notebook will also provide **challenge exercises** for those who want to dive deeper. This is where you can really make the project your own:\n",
      "*   Try **tweaking the system prompt**. What happens if you ask the reviewer to be more concise? Or to focus only on methodology?\n",
      "*   Could you **add a new, simple helper tool**? For example, a tool that checks if all acronyms are defined on their first use within a chunk.\n",
      "*   Experiment with the `max_tokens` for chunking. How does it affect the review?\n",
      "\n",
      "**Host:** This capstone is your opportunity to consolidate your learning, experiment, and see the practical power of combining LLMs like o4-mini with custom code and thoughtful engineering. It's about understanding not just *how* to build it, but also evaluating its performance and thinking about how to improve it.\n",
      "\n",
      "**Host:** This is the culmination of the course. I'm excited to see what you do with it. Let's head to the final notebook and bring your AI Research Paper Reviewer to life!\n",
      "\n",
      "**(Outro Music with End Card: \"Let's Build! Happy Reviewing!\")**\n",
      "\n",
      "---\n",
      "\n",
      "### Notebook Content\n",
      "\n",
      "#### **Capstone Project: Build and Run Your Own AI Research Paper Reviewer**\n",
      "\n",
      "**Welcome to your capstone project!** In this final notebook, you will use the complete code for the AI Research Paper Reviewer. Your goal is to run it on a research paper, analyze its output, and optionally, experiment with modifications.\n",
      "\n",
      "---\n",
      "\n",
      "#### **1. The Complete Code**\n",
      "\n",
      "Below is the consolidated code for the AI Research Paper Reviewer from Lesson 8. Ensure all necessary libraries are installed (`openai`, `PyMuPDF`, `tiktoken`, `numpy`, `scipy`) and your `OPENAI_API_KEY` environment variable is set.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 1: Full AI Research Paper Reviewer Code\n",
      "# (This cell contains the same consolidated code from Lesson 8, Cell 1)\n",
      "\n",
      "# Standard library imports\n",
      "import os\n",
      "import json\n",
      "import argparse\n",
      "import time\n",
      "\n",
      "# Third-party library imports\n",
      "import fitz  # PyMuPDF\n",
      "import tiktoken\n",
      "import numpy as np\n",
      "from scipy.stats import ttest_ind, sem, t\n",
      "import openai\n",
      "\n",
      "# --- OpenAI Client Initialization ---\n",
      "try:\n",
      "    client = openai.OpenAI()\n",
      "    print(\"OpenAI client initialized for the Capstone Project.\")\n",
      "except Exception as e:\n",
      "    print(f\"Failed to initialize OpenAI client: {e}. Ensure OPENAI_API_KEY is set.\")\n",
      "    client = None\n",
      "\n",
      "# --- Function: extract_text_from_pdf ---\n",
      "def extract_text_from_pdf(pdf_path):\n",
      "    try:\n",
      "        doc = fitz.open(pdf_path)\n",
      "    except Exception as e:\n",
      "        print(f\"Error opening PDF {pdf_path}: {e}\")\n",
      "        return \"\"\n",
      "    full_text = \"\\n\".join(page.get_text(\"text\") for page in doc)\n",
      "    doc.close()\n",
      "    return full_text\n",
      "\n",
      "# --- Function: chunk_text ---\n",
      "def chunk_text(text, max_tokens=12000, model_encoding=\"cl100k_base\"): # Using 12000 as per tutorial\n",
      "    try:\n",
      "        encoding = tiktoken.get_encoding(model_encoding)\n",
      "    except:\n",
      "        encoding = tiktoken.get_encoding(\"gpt2\") \n",
      "    tokens = encoding.encode(text)\n",
      "    chunks = []\n",
      "    for i in range(0, len(tokens), max_tokens):\n",
      "        chunk_tokens = tokens[i:i + max_tokens]\n",
      "        chunks.append(encoding.decode(chunk_tokens))\n",
      "    return chunks\n",
      "\n",
      "# --- Statistical Helper Functions ---\n",
      "def recalculate_p_value(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) < 2 or len(group2) < 2: return {\"p_value\": np.nan, \"error\": \"Not enough data points in one or both groups for t-test.\"}\n",
      "    t_stat, p_value = ttest_ind(group1, group2, equal_var=False)\n",
      "    return {\"p_value\": round(p_value, 4)}\n",
      "\n",
      "def compute_cohens_d(group1, group2):\n",
      "    group1 = np.array(group1); group2 = np.array(group2)\n",
      "    if len(group1) == 0 or len(group2) == 0: return {\"cohens_d\": np.nan, \"error\": \"Empty group data provided for Cohen's d.\"}\n",
      "    mean1, mean2 = np.mean(group1), np.mean(group2)\n",
      "    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n",
      "    # Handle cases where std is 0 to avoid division by zero or misleading results\n",
      "    if std1 == 0 and std2 == 0: # Both groups have no variance\n",
      "        return {\"cohens_d\": 0.0 if mean1 == mean2 else (np.inf if mean1 > mean2 else -np.inf), \"note\": \"Both groups have zero standard deviation.\"}\n",
      "    pooled_std = np.sqrt((std1**2 + std2**2) / 2)\n",
      "    if pooled_std == 0: # Should be caught by above if both std1 and std2 are 0\n",
      "        return {\"cohens_d\": 0.0 if mean1 == mean2 else (np.inf if mean1 > mean2 else -np.inf), \"note\":\"Pooled standard deviation is zero.\"}\n",
      "    d = (mean1 - mean2) / pooled_std\n",
      "    return {\"cohens_d\": round(d, 4)}\n",
      "\n",
      "def compute_confidence_interval(data, confidence=0.95):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n < 2: return {\"mean\": round(np.mean(data), 4) if n == 1 else np.nan, \"confidence_interval\": [np.nan, np.nan], \"confidence\": confidence, \"note\":\"Not enough data points for confidence interval.\"}\n",
      "    mean = np.mean(data)\n",
      "    standard_error_mean = sem(data)\n",
      "    if standard_error_mean == 0: # All data points are identical\n",
      "         return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean,4), round(mean,4)], \"confidence\": confidence, \"note\": \"All data points are identical; CI is the mean itself.\"}\n",
      "    margin_of_error = standard_error_mean * t.ppf((1 + confidence) / 2., n-1)\n",
      "    return {\"mean\": round(mean, 4), \"confidence_interval\": [round(mean - margin_of_error, 4), round(mean + margin_of_error, 4)], \"confidence\": confidence}\n",
      "\n",
      "def describe_group(data):\n",
      "    data = np.array(data)\n",
      "    n = len(data)\n",
      "    if n == 0: return {\"mean\": np.nan, \"std_dev\": np.nan, \"n\": 0, \"note\": \"Empty data array.\"}\n",
      "    mean = np.mean(data)\n",
      "    std_dev = np.std(data, ddof=1) if n > 1 else 0.0 # Std dev of a single point is undefined or 0 by convention\n",
      "    return {\"mean\": round(mean, 4), \"std_dev\": round(std_dev, 4), \"n\": n}\n",
      "\n",
      "# --- Tool Definitions ---\n",
      "tool_function_map = {\n",
      "    \"recalculate_p_value\": recalculate_p_value, \"compute_cohens_d\": compute_cohens_d,\n",
      "    \"compute_confidence_interval\": compute_confidence_interval, \"describe_group\": describe_group,\n",
      "}\n",
      "# Ensure full descriptions for tools for better LLM understanding\n",
      "tools = [\n",
      "    {\n",
      "        \"type\": \"function\", \"name\": \"recalculate_p_value\",\n",
      "        \"description\": \"Calculate p-value between two independent sample groups using Welch's t-test. Use to verify claims of statistical significance (e.g. p < 0.05) when group data is available.\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Data for first group\"}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Data for second group\"}}, \"required\": [\"group1\", \"group2\"]}\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\", \"name\": \"compute_cohens_d\",\n",
      "        \"description\": \"Compute effect size (Cohen's d) between two groups. Use to quantify the magnitude of difference between group means.\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"group1\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Data for first group\"}, \"group2\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Data for second group\"}}, \"required\": [\"group1\", \"group2\"]}\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\", \"name\": \"compute_confidence_interval\",\n",
      "        \"description\": \"Compute the confidence interval for a sample mean. Use to assess the reliability of a mean estimate.\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Sample data\"}, \"confidence\": {\"type\": \"number\", \"default\": 0.95, \"description\": \"Confidence level (e.g., 0.95)\"}}, \"required\": [\"data\"]}\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"function\", \"name\": \"describe_group\",\n",
      "        \"description\": \"Summarize sample data with mean, standard deviation, and count (n). Use to get basic stats of a dataset.\",\n",
      "        \"parameters\": {\"type\": \"object\", \"properties\": {\"data\": {\"type\": \"array\", \"items\": {\"type\": \"number\"}, \"description\": \"Sample data\"}}, \"required\": [\"data\"]}\n",
      "    }\n",
      "]\n",
      "\n",
      "# --- Function: review_text_chunk ---\n",
      "# Using the robust version from Lesson 8\n",
      "def review_text_chunk(chunk_text_content, attempt_count=0, max_attempts=3):\n",
      "    if not client: return \"Error: OpenAI client not initialized.\"\n",
      "    if attempt_count >= max_attempts: return f\"Error: Max retry ({max_attempts}) reached for chunk.\"\n",
      "\n",
      "    # Ensure system prompt is detailed for good performance\n",
      "    system_prompt_content = (\n",
      "        \"You are an expert AI research reviewer. Your task is to critically analyze the provided chunk of a research paper. \"\n",
      "        \"Focus on identifying weak arguments, unsupported claims, logical fallacies, or flawed methodology. \"\n",
      "        \"If statistical data or claims are present, use the available tools to verify them if possible. \"\n",
      "        \"Specifically, you can use tools to: Recalculate p-values, Compute confidence intervals, \"\n",
      "        \"Estimate effect size (Cohen's d), or Describe sample statistics. \"\n",
      "        \"When using a tool, clearly state why you are using it and what data you are inputting. \"\n",
      "        \"After receiving the tool's output, incorporate it into your analysis. \"\n",
      "        \"Your review for this chunk should be rigorous, well-explained, and constructive. \"\n",
      "        \"Conclude with specific suggestions for improvement related to the content of this chunk, and a brief verdict on the claims made within it.\"\n",
      "    )\n",
      "    current_messages = [\n",
      "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
      "        {\"role\": \"user\", \"content\": chunk_text_content}\n",
      "    ]\n",
      "    try:\n",
      "        # print(f\"Attempt {attempt_count + 1} for chunk: Sending to o4-mini...\")\n",
      "        response = client.responses.create(\n",
      "            model=\"o4-mini\", reasoning={\"effort\": \"high\"},\n",
      "            input=current_messages, tools=tools\n",
      "        )\n",
      "        tool_calls_to_process = []\n",
      "        # Logic for parsing response.output and identifying tool calls (as in Lesson 7/8)\n",
      "        if response.output:\n",
      "            for item in response.output:\n",
      "                if getattr(item, \"type\", None) == \"function_call\":\n",
      "                    function_call_details = getattr(item, \"function_call\", None)\n",
      "                    if function_call_details:\n",
      "                        tool_calls_to_process.append({\n",
      "                            \"id\": getattr(item, \"id\", None), # Assuming an ID might be present\n",
      "                            \"name\": getattr(function_call_details, \"name\", \"\"),\n",
      "                            \"arguments\": getattr(function_call_details, \"arguments\", {})\n",
      "                        })\n",
      "        \n",
      "        if tool_calls_to_process:\n",
      "            # print(f\"Tool call(s) requested by o4-mini: {[tc['name'] for tc in tool_calls_to_process]}\")\n",
      "            current_messages.extend(response.output) # Add assistant's turn containing tool_call directives\n",
      "            for tool_call in tool_calls_to_process:\n",
      "                fn_name = tool_call[\"name\"]\n",
      "                fn_args_raw = tool_call[\"arguments\"]\n",
      "                # Ensure args are dict, not string (OpenAI sometimes returns stringified JSON for args)\n",
      "                fn_args = {}\n",
      "                if isinstance(fn_args_raw, str):\n",
      "                    try:\n",
      "                        fn_args = json.loads(fn_args_raw)\n",
      "                    except json.JSONDecodeError:\n",
      "                        print(f\"Warning: Could not parse arguments for tool {fn_name}: {fn_args_raw}\")\n",
      "                        tool_result = {\"error\": f\"Invalid arguments format for tool {fn_name}.\"}\n",
      "                elif isinstance(fn_args_raw, dict):\n",
      "                    fn_args = fn_args_raw\n",
      "                else: # Should not happen with current OpenAI API spec\n",
      "                    print(f\"Warning: Unexpected argument type for tool {fn_name}: {type(fn_args_raw)}\")\n",
      "                    tool_result = {\"error\": f\"Unexpected arguments type for tool {fn_name}.\"}\n",
      "\n",
      "\n",
      "                if fn_name in tool_function_map:\n",
      "                    actual_function = tool_function_map[fn_name]\n",
      "                    try:\n",
      "                        # print(f\"Executing tool: {fn_name} with args: {fn_args}\")\n",
      "                        tool_result = actual_function(**fn_args)\n",
      "                        # print(f\"Tool {fn_name} result: {tool_result}\")\n",
      "                    except Exception as e:\n",
      "                        print(f\"Error executing tool {fn_name} with args {fn_args}: {e}\")\n",
      "                        tool_result = {\"error\": f\"Failed to execute tool {fn_name}. Error: {str(e)}\"}\n",
      "                else:\n",
      "                    print(f\"Warning: Model requested unknown tool '{fn_name}'\")\n",
      "                    tool_result = {\"error\": f\"Tool named {fn_name} not found.\"}\n",
      "                \n",
      "                # Append tool result message\n",
      "                current_messages.append({\n",
      "                    \"role\": \"tool\",\n",
      "                    # \"tool_call_id\": tool_call[\"id\"], # Include if API provides and requires it\n",
      "                    \"name\": fn_name,\n",
      "                    \"content\": str(tool_result) # Result must be a string\n",
      "                })\n",
      "            \n",
      "            # print(\"Sending tool results back to o4-mini...\")\n",
      "            tool_response = client.responses.create(\n",
      "                model=\"o4-mini\", reasoning={\"effort\": \"high\"},\n",
      "                input=current_messages, max_output_tokens=3000\n",
      "            )\n",
      "            # Extract text from tool_response (as in Lesson 7/8)\n",
      "            if hasattr(tool_response, \"output_text\") and tool_response.output_text: return tool_response.output_text.strip()\n",
      "            if tool_response.output and any(hasattr(item, 'text') for item in tool_response.output): return \" \".join([item.text for item in tool_response.output if hasattr(item, 'text')]).strip()\n",
      "            if tool_response.status == \"incomplete\": return f\"Incomplete response after tool call: {getattr(tool_response.incomplete_details, 'reason', 'unknown')}\"\n",
      "            return \"No valid text output after tool call.\"\n",
      "\n",
      "        # No tool call, process initial response (as in Lesson 7/8)\n",
      "        if hasattr(response, \"output_text\") and response.output_text: return response.output_text.strip()\n",
      "        if response.output and any(hasattr(item, 'text') for item in response.output): return \" \".join([item.text for item in response.output if hasattr(item, 'text')]).strip()\n",
      "        if response.status == \"incomplete\": return f\"Incomplete response (no tool call): {getattr(response.incomplete_details, 'reason', 'unknown')}\"\n",
      "        return \"No valid output or tool call returned by the model.\"\n",
      "\n",
      "    except openai.APIError as e:\n",
      "        print(f\"OpenAI API Error (attempt {attempt_count + 1}): {e}\")\n",
      "        if isinstance(e, (openai.RateLimitError, openai.APIConnectionError, openai.InternalServerError)) and attempt_count < max_attempts - 1:\n",
      "            time.sleep( (2**(attempt_count)) * 2 ) # Exponential backoff, slightly longer\n",
      "            return review_text_chunk(chunk_text_content, attempt_count + 1, max_attempts)\n",
      "        return f\"OpenAI API Error after retries: {e}\"\n",
      "    except Exception as e:\n",
      "        print(f\"General Error during chunk review (attempt {attempt_count + 1}): {e}\")\n",
      "        if attempt_count < max_attempts - 1: # Basic retry for general errors\n",
      "            time.sleep( (2**(attempt_count)) * 2 ) # Exponential backoff\n",
      "            return review_text_chunk(chunk_text_content, attempt_count + 1, max_attempts)\n",
      "        return f\"Error during chunk review after retries: {e}\"\n",
      "\n",
      "\n",
      "# --- Function: review_full_pdf ---\n",
      "def review_full_pdf(pdf_path):\n",
      "    if not client: return \"Error: OpenAI client not initialized.\"\n",
      "    print(f\"Starting full review for PDF: {pdf_path}\")\n",
      "    raw_text = extract_text_from_pdf(pdf_path)\n",
      "    if not raw_text: return f\"Could not extract text from {pdf_path}.\"\n",
      "    print(f\"Extracted {len(raw_text)} characters.\")\n",
      "    text_chunks = chunk_text(raw_text) # Uses default max_tokens=12000\n",
      "    print(f\"Divided into {len(text_chunks)} chunks.\")\n",
      "    all_reviews = []\n",
      "    for idx, chunk in enumerate(text_chunks):\n",
      "        print(f\"\\nReviewing Chunk {idx + 1}/{len(text_chunks)}...\")\n",
      "        if idx > 0: time.sleep(max(1, 5 - len(text_chunks)*0.5)) # Dynamic delay, min 1s. Be kind to API.\n",
      "        chunk_review = review_text_chunk(chunk)\n",
      "        all_reviews.append(f\"## Review of Chunk {idx + 1}\\n\\n{chunk_review}\")\n",
      "        print(f\"Finished reviewing Chunk {idx + 1}.\")\n",
      "    full_review_md = f\"# AI-Powered Review of '{os.path.basename(pdf_path)}'\\n\\n\"\n",
      "    full_review_md += \"\\n\\n---\\n\\n\".join(all_reviews)\n",
      "    print(\"\\nFull review aggregated.\")\n",
      "    return full_review_md\n",
      "\n",
      "# --- Main execution function ---\n",
      "def run_reviewer(pdf_file_path):\n",
      "    if not os.path.exists(pdf_file_path):\n",
      "        print(f\"Error: PDF file not found at '{pdf_file_path}'.\")\n",
      "        # Try to create the dummy PDF from Lesson 8 if this is the target\n",
      "        if pdf_file_path == \"dummy_paper.pdf\":\n",
      "             try:\n",
      "                doc_dummy = fitz.open()\n",
      "                page_dummy = doc_dummy.new_page(); page_dummy.insert_text((72, 72), \"This is a dummy paper. Section 1: Intro. Our study explores LLMs.\")\n",
      "                page_dummy.insert_text((72, 100), \"Section 2: Methods. Control group (data: [1,2,3,2,1]), treatment group (data: [5,6,7,6,5]). Claim p < 0.05.\")\n",
      "                doc_dummy.save(pdf_file_path); doc_dummy.close()\n",
      "                print(f\"Created '{pdf_file_path}' for testing.\")\n",
      "             except Exception as e_create:\n",
      "                print(f\"Could not create dummy PDF '{pdf_file_path}': {e_create}\")\n",
      "                return\n",
      "        else:\n",
      "            return\n",
      "\n",
      "    if not client:\n",
      "        print(\"OpenAI client not initialized. Cannot proceed.\")\n",
      "        return\n",
      "\n",
      "    print(f\"Preparing to review: {pdf_file_path}\")\n",
      "    final_review = review_full_pdf(pdf_file_path)\n",
      "    \n",
      "    # print(\"\\n\\n--- FINAL AGGREGATED REVIEW (Snippet) ---\")\n",
      "    # print(final_review[:1500] + \"...\" if len(final_review) > 1500 else final_review) # Print more for capstone\n",
      "    \n",
      "    output_filename = f\"review_output_{os.path.basename(pdf_file_path).replace('.pdf', '')}.md\"\n",
      "    try:\n",
      "        with open(output_filename, \"w\", encoding=\"utf-8\") as f:\n",
      "            f.write(final_review)\n",
      "        print(f\"\\nReview successfully saved to '{output_filename}'\")\n",
      "        print(f\"You can open this Markdown file to see the full review.\")\n",
      "    except Exception as e:\n",
      "        print(f\"\\nError saving review to file: {e}\")\n",
      "\n",
      "print(\"Capstone project code is ready.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "---\n",
      "\n",
      "#### **2. Your Capstone Tasks**\n",
      "\n",
      "##### **Task 1: Prepare Your PDF**\n",
      "*   **Option A (Recommended for first run):** Use the `dummy_paper.pdf` that the script can auto-generate. It's simple and contains text designed to potentially trigger tool use.\n",
      "*   **Option B:** Download the `Fake_paper.pdf` mentioned in the original tutorial if you can find its source (e.g., the tutorial's GitHub repository: [https://github.com/AashiDutt/Research-Paper-Reviewer-using-OpenAI-O4-mini/blob/main/Fake_paper.pdf](https://github.com/AashiDutt/Research-Paper-Reviewer-using-OpenAI-O4-mini/blob/main/Fake_paper.pdf)). This paper is intentionally flawed.\n",
      "*   **Option C:** Choose a research paper PDF of your own.\n",
      "    *   **Tip:** Start with a shorter paper (e.g., 2-5 pages) to manage review time and API costs.\n",
      "    *   Make sure it's a text-based PDF (not scanned images of text). Most modern PDFs are.\n",
      "\n",
      "**Place your chosen PDF in the same directory as this notebook, or provide the full path to it.**\n",
      "\n",
      "##### **Task 2: Run the Full Pipeline**\n",
      "Execute the `run_reviewer` function with the path to your PDF.\n"
    ],
    "metadata": {}
  },
  {
    "cell_type": "code",
    "source": [
      "# Code Cell 2: Run the Reviewer\n",
      "# Replace 'your_paper.pdf' with the actual path to your PDF file.\n",
      "# If using the auto-generated dummy paper, use \"dummy_paper.pdf\".\n",
      "# If you downloaded Fake_paper.pdf, use \"Fake_paper.pdf\".\n",
      "\n",
      "# pdf_to_review = \"dummy_paper.pdf\" \n",
      "# pdf_to_review = \"Fake_paper.pdf\" # Uncomment if you have this file\n",
      "pdf_to_review = \"YOUR_CHOSEN_PAPER.pdf\" # <--- !!! REPLACE THIS WITH YOUR PDF FILENAME !!!\n",
      "\n",
      "# --- IMPORTANT ---\n",
      "# Before running, ensure the PDF file specified in `pdf_to_review` exists in the same directory,\n",
      "# or provide the full absolute path to the PDF.\n",
      "# For example:\n",
      "# pdf_to_review = \"/path/to/your/research_paper.pdf\" \n",
      "#\n",
      "# If you want to use the auto-generated dummy paper for a first test, set:\n",
      "pdf_to_review = \"dummy_paper.pdf\" \n",
      "# And the script will try to create it if it doesn't exist.\n",
      "\n",
      "\n",
      "if client:\n",
      "    if pdf_to_review == \"YOUR_CHOSEN_PAPER.pdf\":\n",
      "        print(\"Please replace 'YOUR_CHOSEN_PAPER.pdf' with an actual PDF file path in Code Cell 2.\")\n",
      "    else:\n",
      "        print(f\"Capstone Task: Running reviewer for '{pdf_to_review}'\")\n",
      "        run_reviewer(pdf_file_path=pdf_to_review)\n",
      "else:\n",
      "    print(\"OpenAI client not initialized. Cannot run capstone task.\")\n"
    ],
    "metadata": {},
    "outputs": [],
    "execution_count": null
  },
  {
    "cell_type": "markdown",
    "source": [
      "\n",
      "##### **Task 3: Interpret Model Output and Analyze**\n",
      "1.  **Locate the Output File:** The review will be saved in a Markdown file (e.g., `review_output_dummy_paper.md`). Open and read it.\n",
      "2.  **Analyze the Review:**\n",
      "    *   **Overall Quality:** Is the review insightful? Does it seem to understand the paper's content? Is the tone appropriate for an \"expert AI research reviewer\"?\n",
      "    *   **Identification of Flaws/Strengths:** Did it correctly identify any weak arguments, unsupported claims, or methodological issues? (This is easier to judge if you're familiar with the paper or use one designed with flaws).\n",
      "    *   **Tool Usage (Critical):**\n",
      "        *   Were any tools called? Look at the console output during the run (it prints when tools are called and their results).\n",
      "        *   Does the final review reflect the outcome of these tool calls? For instance, if a p-value was recalculated, is this mentioned or used in the critique?\n",
      "        *   If you expected a tool to be called for a certain claim but it wasn't, why might that be? (Perhaps the data wasn't clear enough in the text, or the tool description wasn't a perfect match for the LLM's interpretation).\n",
      "    *   **Suggestions:** Are the suggestions for improvement relevant and constructive?\n",
      "    *   **Chunking Impact:** Does the review flow reasonably well despite being assembled from chunks? Are there any obvious disconnects due to chunk boundaries?\n",
      "\n",
      "**Write down your observations (you can add a Markdown cell below this one in your notebook):**\n",
      "*   *Paper Used:* [e.g., dummy_paper.pdf / Fake_paper.pdf / Title of your chosen paper]\n",
      "*   *Overall Impression:*\n",
      "*   *Tool Usage Observed (and was it appropriate?):*\n",
      "*   *Key Strengths of the Review:*\n",
      "*   *Key Weaknesses/Areas for Improvement in the Review:*\n",
      "*   *Any Surprises or Interesting Behaviors:*\n",
      "\n",
      "---\n",
      "*(Example Markdown Cell for Your Analysis - Double click to edit)*\n",
      "\n",
      "**My Capstone Analysis**\n",
      "\n",
      "*   **Paper Used:** `dummy_paper.pdf`\n",
      "*   **Overall Impression:** The reviewer attempted to analyze the content. For the dummy paper, it correctly identified the sections and tried to make sense of the fabricated claims.\n",
      "*   **Tool Usage Observed (and was it appropriate?):**\n",
      "    *   Yes, for `dummy_paper.pdf`, it called `recalculate_p_value` and `describe_group` for the data `[1,2,3,2,1]` and `[5,6,7,6,5]`.\n",
      "    *   The arguments passed to the tools seemed correct based on the text.\n",
      "    *   The review mentioned the outcome of these calculations (e.g., \"The p-value was recalculated to be X, which supports/contradicts the paper's claim...\").\n",
      "*   **Key Strengths of the Review:** It followed instructions to be critical and structured its output per chunk.\n",
      "*   **Key Weaknesses/Areas for Improvement in the Review:** Sometimes the language can be a bit generic if the chunk is purely descriptive. The transition between chunks in the final document is abrupt.\n",
      "*   **Any Surprises or Interesting Behaviors:** It was interesting to see how it decided which numbers constituted \"group1\" vs \"group2\" from the sentence.\n",
      "\n",
      "---\n",
      "\n",
      "##### **Task 4: Troubleshooting (If Needed)**\n",
      "*   **No Output / Errors:**\n",
      "    *   Check your `OPENAI_API_KEY`.\n",
      "    *   Ensure all libraries are installed.\n",
      "    *   Look at console error messages – they often point to the issue (e.g., PDF not found, API error).\n",
      "    *   The `client.responses.create` API and its response structure (`response.output`, `response.output_text`, `function_call` object structure) are based on the tutorial's usage for o4-mini. If OpenAI's API for o4-mini or tool calling has evolved, these might need slight adjustments. The provided code attempts to follow the tutorial's described structure.\n",
      "*   **Poor Quality Review:**\n",
      "    *   **PDF Extraction Issues:** If the PDF has a complex layout, columns, or many figures, `PyMuPDF` might struggle to extract clean text. Try with a simpler PDF.\n",
      "    *   **Prompt Engineering:** The system prompt is key. Is it clear enough? Too restrictive? Too vague?\n",
      "    *   **Tool Descriptions:** If tools aren't being used as expected, their descriptions in the `tools` schema might need to be clearer or more targeted.\n",
      "*   **Tools Not Called:**\n",
      "    *   Does the text chunk actually contain data or claims that *should* trigger a tool?\n",
      "    *   Are the tool descriptions compelling enough for the LLM to choose them?\n",
      "    *   Is the data presented in a way the LLM can easily parse into arguments for the tool?\n",
      "\n",
      "---\n",
      "\n",
      "#### **3. Challenge Exercises (Optional)**\n",
      "\n",
      "If you're feeling adventurous, try one or more of these:\n",
      "\n",
      "1.  **Prompt Modification:**\n",
      "    *   Modify the `system_prompt_content` in `review_text_chunk`.\n",
      "        *   Try asking for a different tone (e.g., \"more supportive,\" \"extremely critical\").\n",
      "        *   Ask for a specific output format for each chunk (e.g., \"1. Summary of claims. 2. Critique. 3. Suggestions.\").\n",
      "    *   Run the reviewer again and compare the output.\n",
      "\n",
      "2.  **Tool Description Tweak:**\n",
      "    *   Identify a tool that wasn't used when you expected it, or was used inappropriately.\n",
      "    *   Modify its `description` in the `tools` schema to be clearer or more specific.\n",
      "    *   Test again. Did it improve the tool usage?\n",
      "\n",
      "3.  **Add a New Simple Tool:**\n",
      "    *   **Idea:** A tool to check for the presence of specific keywords (e.g., \"ethics,\" \"limitations,\" \"future work\").\n",
      "    *   **Steps:**\n",
      "        1.  Write a simple Python function: `def check_keywords(text_chunk, keywords_list): ... return {\"keywords_found\": [...]}`.\n",
      "        2.  Add it to `tool_function_map`.\n",
      "        3.  Define its schema in the `tools` list (name, description, parameters: `text_chunk` (string), `keywords_list` (array of strings)).\n",
      "        4.  Update the system prompt to inform the LLM about this new tool and when it might use it.\n",
      "        5.  Test it!\n",
      "\n",
      "4.  **Experiment with Chunking:**\n",
      "    *   In `review_full_pdf`, change the `max_tokens` value in the `chunk_text` call (e.g., to `6000` or `15000`).\n",
      "    *   How does this affect the number of chunks, the review time, API costs (more/fewer calls), and the coherence of the final review?\n",
      "\n",
      "---\n",
      "\n",
      "#### **4. Conceptual Explanations: Evaluating and Next Steps**\n",
      "\n",
      "*   **Evaluating the Strengths/Limits of the Deployed System:**\n",
      "    *   **Strengths:**\n",
      "        *   Automates a time-consuming part of literature review.\n",
      "        *   Leverages LLM's broad knowledge and analytical capabilities.\n",
      "        *   Tool use provides a degree of objective verification for statistical claims.\n",
      "        *   Modular design allows for easier updates and extensions.\n",
      "    *   **Limitations:**\n",
      "        *   **Dependent on PDF Text Quality:** Poor OCR or complex layouts can lead to garbage-in, garbage-out.\n",
      "        *   **LLM Hallucinations/Misinterpretations:** While o4-mini is a reasoning model, it can still misinterpret text or make errors, especially with ambiguous phrasing. Tool use mitigates this for specific, verifiable claims but not for general textual interpretation.\n",
      "        *   **Context Window per Chunk:** Analysis is chunk-by-chunk. It might miss connections or arguments that span across chunks.\n",
      "        *   **Tool Limitations:** Tools are only as good as their implementation and the data they can access from the text. They can't, for example, verify external data sources not mentioned in the paper.\n",
      "        *   **Cost:** API calls, especially for long papers with many chunks and tool uses, can incur costs.\n",
      "        *   **Not a Replacement for Human Expertise:** This tool is best seen as an assistant to a human researcher, highlighting areas for deeper inspection, rather than a definitive final reviewer.\n",
      "\n",
      "*   **Next Steps for Productionizing or Scaling Up:**\n",
      "    *   **Robust Error Handling & Logging:** Implement comprehensive logging and more sophisticated retry mechanisms.\n",
      "    *   **Asynchronous Processing:** For multiple papers or very long ones, process chunks or papers asynchronously to improve throughput (e.g., using `asyncio` or a task queue like Celery).\n",
      "    *   **User Interface (UI):** Build a web interface (e.g., with Flask, Django, or Streamlit) for easier PDF uploads and review display.\n",
      "    *   **Database Integration:** Store reviews, PDF metadata, and perhaps even user feedback.\n",
      "    *   **Advanced Chunking Strategies:** Explore overlapping chunks, semantic chunking, or methods that try to respect sentence/paragraph boundaries.\n",
      "    *   **More Sophisticated Tools:** Integrate with external APIs (e.g., for citation checking, finding related papers).\n",
      "    *   **Cost Management:** Implement budget alerts, optimize token usage (e.g., by summarizing parts of chunks if appropriate before sending to the most expensive reasoning step).\n",
      "    *   **Security & Scalability:** If deploying as a service, consider authentication, authorization, rate limiting per user, and infrastructure that can scale.\n",
      "\n",
      "---\n",
      "\n",
      "**Congratulations on completing the capstone project and the course!** You've built a sophisticated AI application from the ground up, learning about reasoning models, prompt engineering, and tool integration with OpenAI's o4-mini. We hope this project serves as a strong foundation for your future explorations in building AI-powered solutions.\n"
    ],
    "metadata": {}
}
